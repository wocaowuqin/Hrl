#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Low-Level Policy (é‡æ„ç‰ˆ)
èŒè´£ï¼šç»™å®šsubgoalï¼Œé€‰æ‹©ä¸‹ä¸€è·³èŠ‚ç‚¹æ‰§è¡Œè·¯å¾„è§„åˆ’
"""

import torch
import torch.nn as nn
import logging
from typing import Optional

logger = logging.getLogger(__name__)


class GoalConditionedLowLevelPolicy(nn.Module):
    """
    Goal-Conditioned Low-Level Policy (é‡æ„ç‰ˆ)

    èŒè´£ï¼š
    1. æ¥æ”¶å½“å‰çŠ¶æ€ + subgoal embedding
    2. è¾“å‡ºä¸‹ä¸€è·³èŠ‚ç‚¹çš„Qå€¼

    å…³é”®ç‰¹ç‚¹ï¼š
    - Goal-Conditionedï¼ˆç›®æ ‡æ¡ä»¶åŒ–ï¼‰
    - å­¦ä¹ å¦‚ä½•åˆ°è¾¾subgoal
    - æ¥æ”¶å†…åœ¨å¥–åŠ±ï¼ˆæ¥è¿‘/è¿œç¦»subgoalï¼‰
    """

    def __init__(self, config):
        super().__init__()

        # è®¾å¤‡é…ç½®
        use_cuda = config.get('use_cuda', False)
        self.device = torch.device(
            "cuda" if torch.cuda.is_available() and use_cuda else "cpu"
        )

        # ç»´åº¦é…ç½®
        self.state_dim = config.get('state_dim', 128)  # GNN encoderè¾“å‡º
        self.goal_dim = config.get('goal_dim', 64)  # Goal embeddingç»´åº¦
        self.hidden_dim = config.get('hidden_dim', 128)

        # ä»ç¯å¢ƒé…ç½®è¯»å–åŠ¨ä½œæ•°é‡
        env_cfg = config.get('environment', {})
        self.action_dim = env_cfg.get('nb_low_level_actions', 28)

        dropout = config.get('dropout', 0.1)

        logger.info(f"GoalConditionedLowLevelPolicyé…ç½®:")
        logger.info(f"  State dim: {self.state_dim}")
        logger.info(f"  Goal dim: {self.goal_dim}")
        logger.info(f"  Action dim: {self.action_dim}")

        # ============================================
        # 1. State Projectionï¼ˆçŠ¶æ€æŠ•å½±ï¼‰
        # ============================================
        self.state_projection = nn.Sequential(
            nn.Linear(self.state_dim, self.hidden_dim),
            nn.LayerNorm(self.hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )

        # ============================================
        # 2. Goal Projectionï¼ˆç›®æ ‡æŠ•å½±ï¼‰
        # ============================================
        self.goal_projection = nn.Sequential(
            nn.Linear(self.goal_dim, self.hidden_dim),
            nn.LayerNorm(self.hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )

        # ============================================
        # 3. Fusionï¼ˆçŠ¶æ€-ç›®æ ‡èåˆï¼‰
        # ============================================
        self.fusion = nn.Sequential(
            nn.Linear(self.hidden_dim * 2, self.hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )

        # ============================================
        # 4. Actorï¼ˆQ Network - åŠ¨ä½œé€‰æ‹©ï¼‰
        # ============================================
        self.actor = nn.Sequential(
            nn.Linear(self.hidden_dim, self.hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(self.hidden_dim, self.action_dim)
        )

        # ============================================
        # 5. Criticï¼ˆValue Network - çŠ¶æ€è¯„ä¼°ï¼‰
        # ============================================
        self.critic = nn.Sequential(
            nn.Linear(self.hidden_dim, self.hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(self.hidden_dim, 1)
        )

        self.to(self.device)

        logger.info(f"âœ… GoalConditionedLowLevelPolicyåˆå§‹åŒ–å®Œæˆï¼Œè®¾å¤‡: {self.device}")

    def forward(
            self,
            state_emb: torch.Tensor,
            goal_emb: Optional[torch.Tensor] = None,
            action_mask: Optional[torch.Tensor] = None
    ) -> tuple:
        """
        å‰å‘ä¼ æ’­

        Args:
            state_emb: çŠ¶æ€åµŒå…¥ [batch, state_dim]
            goal_emb: Goal embedding [batch, goal_dim] (å¯é€‰)
            action_mask: åŠ¨ä½œmask [batch, action_dim] (å¯é€‰)

        Returns:
            logits: åŠ¨ä½œlogits [batch, action_dim]
            value: çŠ¶æ€ä»·å€¼ [batch, 1]
        """
        # æŠ•å½±çŠ¶æ€
        state_proj = self.state_projection(state_emb)

        # å¦‚æœæä¾›äº†goalï¼Œè¿›è¡Œèåˆ
        if goal_emb is not None:
            # æŠ•å½±goal
            goal_proj = self.goal_projection(goal_emb)

            # èåˆstateå’Œgoal
            fused = torch.cat([state_proj, goal_proj], dim=1)
            fused = self.fusion(fused)
        else:
            # æ²¡æœ‰goalæ—¶ï¼Œç›´æ¥ä½¿ç”¨state
            fused = state_proj

        # Actorè¾“å‡ºï¼ˆQå€¼ï¼‰
        logits = self.actor(fused)

        # åº”ç”¨mask
        if action_mask is not None:
            logits = logits.masked_fill(action_mask == 0, float('-inf'))

        # Criticè¾“å‡ºï¼ˆçŠ¶æ€ä»·å€¼ï¼‰
        value = self.critic(fused)

        return logits, value

    def select_action(self, state_emb, goal_emb, action_mask=None, epsilon=0.1):
        """
        ğŸ”¥ [ä¿®å¤ç‰ˆ] é€‰æ‹©åŠ¨ä½œï¼ˆä½å±‚åŠ¨ä½œï¼‰

        Args:
            state_emb: (batch, state_dim) çŠ¶æ€åµŒå…¥
            goal_emb: (batch, goal_dim) ç›®æ ‡åµŒå…¥
            action_mask: (batch, n_actions) åŠ¨ä½œmaskï¼Œ1=å¯é€‰ï¼Œ0=ç¦æ­¢
            epsilon: æ¢ç´¢ç‡

        Returns:
            action: int, é€‰æ‹©çš„åŠ¨ä½œ
            q_value: float, å¯¹åº”çš„Qå€¼
        """
        import torch
        import numpy as np

        # 1. å‰å‘ä¼ æ’­è·å–Qå€¼
        with torch.no_grad():
            policy_output = self.forward(state_emb, goal_emb)

            # å…¼å®¹è¿”å›tupleçš„æƒ…å†µ
            if isinstance(policy_output, tuple):
                q_values = policy_output[0]  # (batch, n_actions)
            else:
                q_values = policy_output

        # 2. ğŸ”¥ å…³é”®ä¿®å¤ï¼šåº”ç”¨Maskï¼ˆåœ¨argmaxä¹‹å‰ï¼‰
        if action_mask is not None:
            # ç¡®ä¿maskåœ¨æ­£ç¡®è®¾å¤‡ä¸Š
            if isinstance(action_mask, np.ndarray):
                action_mask = torch.FloatTensor(action_mask).to(q_values.device)

            # ç¡®ä¿maskç»´åº¦åŒ¹é…
            if action_mask.dim() == 1:
                action_mask = action_mask.unsqueeze(0)

            if action_mask.size(1) != q_values.size(1):
                # ç»´åº¦ä¸åŒ¹é…ï¼Œæˆªæ–­æˆ–å¡«å……
                if action_mask.size(1) < q_values.size(1):
                    padding = torch.zeros(
                        action_mask.size(0),
                        q_values.size(1) - action_mask.size(1),
                        device=action_mask.device
                    )
                    action_mask = torch.cat([action_mask, padding], dim=1)
                else:
                    action_mask = action_mask[:, :q_values.size(1)]

            # ğŸ”¥ Qå€¼å±è”½æ³•ï¼šmask==0çš„ä½ç½®è®¾ä¸º-1e9
            masked_q_values = q_values.clone()
            masked_q_values[action_mask == 0] = -1e9
        else:
            masked_q_values = q_values

        # 3. åŠ¨ä½œé€‰æ‹©ï¼ˆepsilon-greedyï¼‰
        if np.random.rand() < epsilon:
            # æ¢ç´¢ï¼šä»æœ‰æ•ˆåŠ¨ä½œä¸­éšæœºé€‰æ‹©
            if action_mask is not None:
                valid_indices = torch.nonzero(action_mask[0] > 0, as_tuple=False).squeeze()
                if valid_indices.numel() == 0:
                    action = torch.tensor(0)
                elif valid_indices.numel() == 1:
                    action = valid_indices
                else:
                    action = valid_indices[torch.randint(len(valid_indices), (1,))]
            else:
                action = torch.randint(0, q_values.size(1), (1,))
        else:
            # åˆ©ç”¨ï¼šé€‰æ‹©Qå€¼æœ€å¤§çš„
            action = torch.argmax(masked_q_values[0])

        # è·å–å¯¹åº”çš„Qå€¼
        q_value = masked_q_values[0, action].item()

        return action, q_value


# ============================================
# å‘åå…¼å®¹ï¼šä¿ç•™æ—§æ¥å£
# ============================================

class LowLevelPolicy(nn.Module):
    """
    ä½å±‚ç­–ç•¥ç½‘ç»œ (å‘åå…¼å®¹ç‰ˆ)

    è¿™æ˜¯ä¸ºäº†ä¸ç ´åç°æœ‰ä»£ç è€Œä¿ç•™çš„æ¥å£
    å®é™…ä¸Šä¼šè°ƒç”¨é‡æ„åçš„GoalConditionedLowLevelPolicy
    ä½†å¯ä»¥åœ¨ä¸æä¾›goalçš„æƒ…å†µä¸‹ä½¿ç”¨
    """

    def __init__(self, input_dim, action_dim, hidden_dim=128):
        super().__init__()

        logger.warning("âš ï¸ LowLevelPolicyä½¿ç”¨æ—§æ¥å£ï¼Œå»ºè®®è¿ç§»åˆ°GoalConditionedLowLevelPolicy")

        # åˆ›å»ºé…ç½®
        config = {
            'state_dim': input_dim,
            'goal_dim': 64,  # é»˜è®¤goalç»´åº¦
            'hidden_dim': hidden_dim,
            'environment': {
                'nb_low_level_actions': action_dim
            },
            'use_cuda': False,
            'dropout': 0.1
        }

        # ä½¿ç”¨æ–°çš„GoalConditionedLowLevelPolicy
        self.policy = GoalConditionedLowLevelPolicy(config)

        # ç®€å•çš„Actorå’ŒCriticæ¥å£ï¼ˆå‘åå…¼å®¹ï¼‰
        # è¿™äº›å®é™…ä¸Šä¼šè°ƒç”¨policyçš„å¯¹åº”éƒ¨åˆ†
        self.actor = self.policy.actor
        self.critic = self.policy.critic

    def forward(self, state_emb):
        """
        å…¼å®¹æ—§æ¥å£ï¼ˆä¸ä½¿ç”¨goalï¼‰

        Args:
            state_emb: çŠ¶æ€åµŒå…¥ [batch, input_dim]

        Returns:
            logits: åŠ¨ä½œlogits [batch, action_dim]
            value: çŠ¶æ€ä»·å€¼ [batch, 1]
        """
        # è°ƒç”¨æ–°æ¥å£ï¼Œä½†ä¸æä¾›goal
        return self.policy.forward(state_emb, goal_emb=None, action_mask=None)