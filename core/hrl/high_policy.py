#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
High-Level Policy (é‡æ„ç‰ˆ)
èŒè´£ï¼šä»æœªè¿æ¥çš„ç›®æ ‡èŠ‚ç‚¹ä¸­é€‰æ‹©ä¸‹ä¸€ä¸ªè¦è¿æ¥çš„èŠ‚ç‚¹ï¼ˆsubgoal selectionï¼‰
"""

import torch
import torch.nn as nn
import logging
from typing import Optional

logger = logging.getLogger(__name__)


def deep_get(cfg, keys, default=None):
    """ä» config çš„å¤šä¸ªå¯èƒ½è·¯å¾„ä¸­è¯»å–å€¼"""
    if cfg is None:
        return default

    if isinstance(cfg, dict):
        for k in keys:
            if k in cfg and cfg[k] is not None:
                return cfg[k]
        for v in cfg.values():
            if isinstance(v, dict):
                found = deep_get(v, keys, None)
                if found is not None:
                    return found
        return default

    for k in keys:
        if hasattr(cfg, k):
            val = getattr(cfg, k)
            if val is not None:
                return val

    return default


class HighLevelPolicy(nn.Module):
    """
    High-Level Policy Network (é‡æ„ç‰ˆ)

    èŒè´£ï¼š
    1. ä»æœªè¿æ¥çš„ç›®æ ‡èŠ‚ç‚¹ä¸­é€‰æ‹©ä¸‹ä¸€ä¸ªè¦è¿æ¥çš„èŠ‚ç‚¹
    2. ç”Ÿæˆsubgoal embeddingä¾›Low-Levelä½¿ç”¨

    è¾“å…¥ï¼šGNNç¼–ç åçš„å›¾åµŒå…¥
    è¾“å‡ºï¼š
        - Qå€¼ï¼ˆç”¨äºç›®æ ‡é€‰æ‹©ï¼‰
        - Subgoal embeddingï¼ˆç»™Low-Levelï¼‰
    """

    def __init__(self, config):
        super().__init__()

        # è®¾å¤‡é…ç½®
        use_cuda = deep_get(config, ["use_cuda"], False)
        self.device = torch.device(
            "cuda" if torch.cuda.is_available() and use_cuda else "cpu"
        )

        # ç»´åº¦é…ç½®
        self.hidden_dim = deep_get(config, ["hidden_dim"], 128)
        self.goal_dim = deep_get(config, ["goal_dim"], 64)

        # ä»ç¯å¢ƒé…ç½®è¯»å–ç›®æ ‡æ•°é‡
        env_cfg = config.get('environment', {})
        self.num_goals = env_cfg.get('nb_high_level_goals', 10)

        # GNNè¾“å‡ºç»´åº¦
        self.gnn_output_dim = deep_get(config, ["gnn_output_dim"], self.hidden_dim)

        dropout = deep_get(config, ["dropout"], 0.1)

        logger.info(f"HighLevelPolicyé…ç½®:")
        logger.info(f"  GNNè¾“å‡ºç»´åº¦: {self.gnn_output_dim}")
        logger.info(f"  Goalç»´åº¦: {self.goal_dim}")
        logger.info(f"  ç›®æ ‡æ•°é‡: {self.num_goals}")

        # ============================================
        # 1. State Projectionï¼ˆçŠ¶æ€æŠ•å½±ï¼‰
        # ============================================
        self.state_projection = nn.Sequential(
            nn.Linear(self.gnn_output_dim, self.hidden_dim),
            nn.LayerNorm(self.hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )

        # ============================================
        # 2. Q Networkï¼ˆç›®æ ‡é€‰æ‹©ï¼‰
        # ============================================
        self.q_network = nn.Sequential(
            nn.Linear(self.hidden_dim, self.hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(self.hidden_dim, self.num_goals)
        )

        # ============================================
        # 3. Subgoal Generatorï¼ˆç”Ÿæˆgoal embeddingï¼‰
        # ============================================
        self.subgoal_generator = nn.Sequential(
            nn.Linear(self.hidden_dim, self.hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(self.hidden_dim, self.goal_dim),
            nn.Tanh()  # å½’ä¸€åŒ–åˆ°[-1, 1]
        )

        # ============================================
        # 4. Value Networkï¼ˆçŠ¶æ€ä»·å€¼è¯„ä¼°ï¼‰
        # ============================================
        self.value_network = nn.Sequential(
            nn.Linear(self.hidden_dim, self.hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(self.hidden_dim, 1)
        )

        self.to(self.device)

        logger.info(f"âœ… HighLevelPolicyåˆå§‹åŒ–å®Œæˆï¼Œè®¾å¤‡: {self.device}")

    def forward(
            self,
            graph_emb: torch.Tensor,
            return_subgoal: bool = True,
            return_value: bool = False
    ) -> tuple:
        """
        å‰å‘ä¼ æ’­

        Args:
            graph_emb: å›¾åµŒå…¥ [batch, gnn_output_dim]
            return_subgoal: æ˜¯å¦è¿”å›subgoal embedding
            return_value: æ˜¯å¦è¿”å›value

        Returns:
            q_values: ç›®æ ‡Qå€¼ [batch, num_goals]
            subgoal_emb: å­ç›®æ ‡åµŒå…¥ [batch, goal_dim] (å¯é€‰)
            value: çŠ¶æ€ä»·å€¼ [batch, 1] (å¯é€‰)
        """
        # æŠ•å½±çŠ¶æ€
        z = self.state_projection(graph_emb)

        # Qå€¼ï¼ˆç”¨äºé€‰æ‹©ç›®æ ‡ï¼‰
        q_values = self.q_network(z)

        # å¯é€‰è¿”å›
        subgoal_emb = None
        value = None

        if return_subgoal:
            subgoal_emb = self.subgoal_generator(z)

            # å¤„ç†NaN
            if torch.isnan(subgoal_emb).any():
                logger.warning("âš ï¸ SubgoalåŒ…å«NaNï¼Œä½¿ç”¨é›¶å‘é‡")
                subgoal_emb = torch.zeros_like(subgoal_emb)

        if return_value:
            value = self.value_network(z)

        return q_values, subgoal_emb, value

    def select_goal(self, state_emb, valid_goals_mask, epsilon=0.1):
        """
        ğŸ”¥ [ä¿®å¤ç‰ˆ] é€‰æ‹©ç›®æ ‡ï¼ˆé«˜å±‚åŠ¨ä½œï¼‰

        Args:
            state_emb: (batch, state_dim) çŠ¶æ€åµŒå…¥
            valid_goals_mask: (batch, n_goals) æœ‰æ•ˆç›®æ ‡maskï¼Œ1=å¯é€‰ï¼Œ0=ç¦æ­¢
            epsilon: æ¢ç´¢ç‡

        Returns:
            goal_idx: int, é€‰æ‹©çš„ç›®æ ‡ç´¢å¼•
            goal_emb: (1, goal_dim), ç›®æ ‡åµŒå…¥
        """
        import torch
        import numpy as np

        # 1. å‰å‘ä¼ æ’­è·å–Qå€¼
        with torch.no_grad():
            q_values, goal_emb, _ = self.forward(state_emb, return_subgoal=True)
            # q_values: (batch, n_goals)

        # 2. ğŸ”¥ å…³é”®ä¿®å¤ï¼šåº”ç”¨Maskï¼ˆåœ¨argmaxä¹‹å‰ï¼‰
        if valid_goals_mask is not None:
            # ç¡®ä¿maskåœ¨æ­£ç¡®è®¾å¤‡ä¸Š
            if isinstance(valid_goals_mask, np.ndarray):
                valid_goals_mask = torch.FloatTensor(valid_goals_mask).to(q_values.device)

            # ğŸ”¥ Qå€¼å±è”½æ³•ï¼šmask==0çš„ä½ç½®è®¾ä¸º-1e9
            masked_q_values = q_values.clone()
            masked_q_values[valid_goals_mask == 0] = -1e9
        else:
            masked_q_values = q_values

        # 3. åŠ¨ä½œé€‰æ‹©ï¼ˆepsilon-greedyï¼‰
        if np.random.rand() < epsilon:
            # æ¢ç´¢ï¼šä»æœ‰æ•ˆç›®æ ‡ä¸­éšæœºé€‰æ‹©
            if valid_goals_mask is not None:
                valid_indices = torch.nonzero(valid_goals_mask[0] > 0, as_tuple=False).squeeze()
                if valid_indices.numel() == 0:
                    goal_idx = torch.tensor(0)
                elif valid_indices.numel() == 1:
                    goal_idx = valid_indices
                else:
                    goal_idx = valid_indices[torch.randint(len(valid_indices), (1,))]
            else:
                goal_idx = torch.randint(0, q_values.size(1), (1,))
        else:
            # åˆ©ç”¨ï¼šé€‰æ‹©Qå€¼æœ€å¤§çš„
            goal_idx = torch.argmax(masked_q_values[0])

        return goal_idx, goal_emb


# ============================================
# å‘åå…¼å®¹ï¼šä¿ç•™æ—§æ¥å£
# ============================================

class HierarchicalPolicy(nn.Module):
    """
    å‘åå…¼å®¹çš„HierarchicalPolicy

    è¿™æ˜¯ä¸ºäº†ä¸ç ´åç°æœ‰ä»£ç è€Œä¿ç•™çš„æ¥å£
    å®é™…ä¸Šä¼šè°ƒç”¨é‡æ„åçš„HighLevelPolicy
    """

    def __init__(self, config):
        super().__init__()

        logger.warning("âš ï¸ HierarchicalPolicyå·²å¼ƒç”¨ï¼Œè¯·ä½¿ç”¨HighLevelPolicy")
        logger.warning("   å½“å‰ä½¿ç”¨å…¼å®¹æ¨¡å¼ï¼ŒåŠŸèƒ½å¯èƒ½å—é™")

        # ä½¿ç”¨æ–°çš„HighLevelPolicy
        self.high_policy = HighLevelPolicy(config)

        # å¤åˆ¶å±æ€§ä»¥ä¿æŒå…¼å®¹
        self.device = self.high_policy.device
        self.gnn_output_dim = self.high_policy.gnn_output_dim
        self.num_goals = self.high_policy.num_goals

    def forward(
            self,
            graph_emb: torch.Tensor,
            goal_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        å…¼å®¹æ—§æ¥å£

        Args:
            graph_emb: å›¾åµŒå…¥ [batch, gnn_output_dim]
            goal_mask: ç›®æ ‡mask [batch, num_goals]

        Returns:
            logits: åŠ¨ä½œlogits [batch, num_goals]
        """
        # è°ƒç”¨æ–°æ¥å£
        q_values, _, _ = self.high_policy(graph_emb, return_subgoal=False)

        # åº”ç”¨mask
        if goal_mask is not None:
            q_values = q_values.masked_fill(goal_mask == 0, float('-inf'))

        return q_values