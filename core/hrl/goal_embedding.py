"""
===============================================================================
core/hrl/goal_embedding_final.py
Goal Embedding æœ€ç»ˆä¼˜åŒ–ç‰ˆæœ¬
===============================================================================

æ•´åˆæ‰€æœ‰åé¦ˆçš„æ”¹è¿›ç‚¹ï¼š
1. åŠ¨æ€ç¼©æ”¾å› å­ï¼ˆRelative Goalï¼‰
2. è‡ªé€‚åº”å­ç›®æ ‡è·ç¦»ï¼ˆSubgoalï¼‰
3. Softmax Option é€‰æ‹©ç­–ç•¥
4. è¿­ä»£ç›®æ ‡ä¼˜åŒ–ï¼ˆHybridï¼‰
5. å¹¶è¡ŒåŒ–æ‰¹é‡ç´¢å¼•

å‚è€ƒè®ºæ–‡ï¼š
- HIRO: Data-Efficient Hierarchical Reinforcement Learning
- HAC: Hierarchical Actor-Critic
- Option-Critic: End-to-End Learning of Options
- FuN: FeUdal Networks for Hierarchical Reinforcement Learning

===============================================================================
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple, Optional, Dict
import numpy as np


# ============================================
# æ–¹æ¡ˆ Aï¼šå¢å¼ºç‰ˆç›¸å¯¹ç›®æ ‡åµŒå…¥
# ============================================

class EnhancedRelativeGoalEmbedding(nn.Module):
    """
    å¢å¼ºç‰ˆç›¸å¯¹ç›®æ ‡åµŒå…¥

    æ”¹è¿›ç‚¹ï¼š
    1. å¯å­¦ä¹ çš„ç¼©æ”¾å› å­ï¼ˆåŠ¨æ€è°ƒæ•´ç›®æ ‡èŒƒå›´ï¼‰
    2. å¤šå°ºåº¦ç›®æ ‡è¡¨ç¤º
    3. æ³¨æ„åŠ›æœºåˆ¶èåˆ
    """

    def __init__(
            self,
            node_feat_dim: int = 32,
            goal_dim: int = 64,
            use_learned_scaling: bool = True,
            use_attention: bool = True
    ):
        super().__init__()

        self.node_feat_dim = node_feat_dim
        self.goal_dim = goal_dim
        self.use_learned_scaling = use_learned_scaling
        self.use_attention = use_attention

        # ğŸ”¥ æ”¹è¿› 1: å¯å­¦ä¹ çš„ç¼©æ”¾å› å­
        if use_learned_scaling:
            self.scale_factor = nn.Parameter(
                torch.ones(1) * 0.5  # åˆå§‹å€¼ 0.5
            )
        else:
            self.register_buffer('scale_factor', torch.tensor([1.0]))

        # ç›®æ ‡ç”Ÿæˆå™¨ï¼ˆå¤šå±‚ï¼‰
        self.goal_generator = nn.Sequential(
            nn.Linear(node_feat_dim * 2, goal_dim * 2),
            nn.LayerNorm(goal_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(goal_dim * 2, goal_dim),
            nn.Tanh()  # è¾“å‡ºèŒƒå›´ [-1, 1]
        )

        # ğŸ”¥ æ”¹è¿› 2: æ³¨æ„åŠ›æœºåˆ¶ï¼ˆèåˆå½“å‰å’Œç›®æ ‡ç‰¹å¾ï¼‰
        if use_attention:
            self.attention = nn.MultiheadAttention(
                embed_dim=node_feat_dim,
                num_heads=4,
                dropout=0.1,
                batch_first=True
            )

            self.attention_proj = nn.Linear(node_feat_dim, goal_dim)

    def forward(
            self,
            current_node_feat: torch.Tensor,
            target_node_feat: torch.Tensor
    ) -> Tuple[torch.Tensor, Dict]:
        """
        ç”Ÿæˆç›¸å¯¹ç›®æ ‡åµŒå…¥

        Args:
            current_node_feat: [batch, node_feat_dim]
            target_node_feat: [batch, node_feat_dim]

        Returns:
            goal_emb: [batch, goal_dim]
            info: è¯Šæ–­ä¿¡æ¯
        """
        info = {}

        # æ‹¼æ¥ç‰¹å¾
        x = torch.cat([current_node_feat, target_node_feat], dim=-1)

        # ç”ŸæˆåŸºç¡€ç›®æ ‡
        base_goal = self.goal_generator(x)

        # ğŸ”¥ åº”ç”¨å¯å­¦ä¹ çš„ç¼©æ”¾
        scaled_goal = self.scale_factor * base_goal

        # ğŸ”¥ æ³¨æ„åŠ›å¢å¼ºï¼ˆå¯é€‰ï¼‰
        if self.use_attention:
            # å †å ç‰¹å¾ç”¨äºæ³¨æ„åŠ›
            feats = torch.stack([current_node_feat, target_node_feat], dim=1)

            # Self-attention
            attn_out, attn_weights = self.attention(feats, feats, feats)

            # æŠ•å½±åˆ°ç›®æ ‡ç©ºé—´
            attn_goal = self.attention_proj(attn_out.mean(dim=1))

            # èåˆ
            goal_emb = scaled_goal + 0.3 * attn_goal

            info['attention_weights'] = attn_weights
        else:
            goal_emb = scaled_goal

        # å½’ä¸€åŒ–
        goal_emb = F.normalize(goal_emb, p=2, dim=-1)

        info['scale_factor'] = self.scale_factor.item()
        info['base_goal_norm'] = base_goal.norm(dim=-1).mean().item()

        return goal_emb, info


# ============================================
# æ–¹æ¡ˆ Bï¼šè‡ªé€‚åº”å­ç›®æ ‡åµŒå…¥
# ============================================

class AdaptiveSubgoalEmbedding(nn.Module):
    """
    è‡ªé€‚åº”å­ç›®æ ‡åµŒå…¥

    æ”¹è¿›ç‚¹ï¼š
    1. åŠ¨æ€è°ƒæ•´å­ç›®æ ‡è·ç¦»
    2. åŸºäºä»»åŠ¡å¤æ‚åº¦çš„è‡ªé€‚åº”
    3. å­ç›®æ ‡å¯è¾¾æ€§é¢„æµ‹
    """

    def __init__(
            self,
            state_dim: int = 32,
            goal_dim: int = 64,
            init_subgoal_distance: float = 5.0,
            adaptive_distance: bool = True
    ):
        super().__init__()

        self.state_dim = state_dim
        self.goal_dim = goal_dim
        self.adaptive_distance = adaptive_distance

        # Subgoal Generator
        self.subgoal_generator = nn.Sequential(
            nn.Linear(state_dim, goal_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(goal_dim * 2, state_dim),
            nn.Tanh()
        )

        # ğŸ”¥ æ”¹è¿› 1: åŠ¨æ€è·ç¦»é¢„æµ‹å™¨
        if adaptive_distance:
            self.distance_predictor = nn.Sequential(
                nn.Linear(state_dim, 64),
                nn.ReLU(),
                nn.Linear(64, 1),
                nn.Softplus()  # ç¡®ä¿è¾“å‡ºä¸ºæ­£
            )
        else:
            self.register_buffer(
                'max_subgoal_distance',
                torch.tensor([init_subgoal_distance])
            )

        # ğŸ”¥ æ”¹è¿› 2: å­ç›®æ ‡å¯è¾¾æ€§é¢„æµ‹å™¨
        self.reachability_predictor = nn.Sequential(
            nn.Linear(state_dim * 2, 64),  # current + subgoal
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()  # å¯è¾¾æ€§æ¦‚ç‡ [0, 1]
        )

    def forward(
            self,
            current_state: torch.Tensor,
            task_complexity: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, Dict]:
        """
        ç”Ÿæˆè‡ªé€‚åº”å­ç›®æ ‡

        Args:
            current_state: [batch, state_dim]
            task_complexity: [batch, 1] ä»»åŠ¡å¤æ‚åº¦ï¼ˆå¯é€‰ï¼‰

        Returns:
            subgoal: [batch, state_dim]
            info: è¯Šæ–­ä¿¡æ¯
        """
        batch_size = current_state.size(0)
        info = {}

        # ç”Ÿæˆç›¸å¯¹å˜åŒ–
        delta = self.subgoal_generator(current_state)

        # ğŸ”¥ åŠ¨æ€è°ƒæ•´è·ç¦»
        if self.adaptive_distance:
            # åŸºäºå½“å‰çŠ¶æ€é¢„æµ‹æœ€ä¼˜è·ç¦»
            max_distance = self.distance_predictor(current_state)

            # å¦‚æœæä¾›äº†ä»»åŠ¡å¤æ‚åº¦ï¼Œè¿›ä¸€æ­¥è°ƒæ•´
            if task_complexity is not None:
                max_distance = max_distance * (1 + task_complexity)

            info['predicted_distance'] = max_distance.mean().item()
        else:
            max_distance = self.max_subgoal_distance.expand(batch_size, 1)

        # ç¼©æ”¾åˆ°åˆé€‚çš„è·ç¦»
        delta = delta * max_distance

        # ç”Ÿæˆå­ç›®æ ‡
        subgoal = current_state + delta

        # ğŸ”¥ é¢„æµ‹å¯è¾¾æ€§
        reachability_input = torch.cat([current_state, subgoal], dim=-1)
        reachability = self.reachability_predictor(reachability_input)

        info['reachability'] = reachability.mean().item()
        info['delta_norm'] = delta.norm(dim=-1).mean().item()

        return subgoal, info

    def compute_reward(
            self,
            achieved_state: torch.Tensor,
            subgoal: torch.Tensor
    ) -> torch.Tensor:
        """
        è®¡ç®—å†…åœ¨å¥–åŠ±ï¼ˆè€ƒè™‘å¯è¾¾æ€§ï¼‰
        """
        # L2 è·ç¦»
        distance = torch.norm(achieved_state - subgoal, dim=-1)

        # åŸºç¡€å¥–åŠ±
        base_reward = -distance

        # å¯è¾¾æ€§åŠ æƒ
        reachability_input = torch.cat([achieved_state, subgoal], dim=-1)
        reachability = self.reachability_predictor(reachability_input).squeeze(-1)

        # å¦‚æœå­ç›®æ ‡ä¸å¯è¾¾ï¼Œé™ä½æƒ©ç½š
        adjusted_reward = base_reward * reachability

        return adjusted_reward


    def compute_intrinsic_reward(self, next_state, subgoal):
        """è®¡ç®—å†…åœ¨å¥–åŠ±"""
        try:
            # ç®€å•çš„è·ç¦»å¥–åŠ±
            if hasattr(next_state, 'x'):
                # ä½¿ç”¨çŠ¶æ€ç‰¹å¾
                state_emb = next_state.x.mean(dim=0)
            else:
                # Fallback
                return 0.0

            # è®¡ç®—è·ç¦»
            distance = torch.norm(state_emb - subgoal.squeeze())
            reward = -distance.item() * 0.1
            return reward
        except:
            return 0.0

# ============================================
# æ–¹æ¡ˆ Cï¼šå¢å¼ºç‰ˆ Option åµŒå…¥
# ============================================

class EnhancedOptionEmbedding(nn.Module):
    """
    å¢å¼ºç‰ˆ Option åµŒå…¥

    æ”¹è¿›ç‚¹ï¼š
    1. Softmax Option é€‰æ‹©ç­–ç•¥
    2. Option ä»·å€¼ä¼°è®¡
    3. åŠ¨æ€ Option ç»ˆæ­¢
    """

    def __init__(
            self,
            num_options: int = 4,
            option_dim: int = 64,
            state_dim: int = 32,
            temperature: float = 1.0
    ):
        super().__init__()

        self.num_options = num_options
        self.option_dim = option_dim
        self.state_dim = state_dim
        self.temperature = temperature

        # Option åµŒå…¥
        self.option_embeddings = nn.Embedding(num_options, option_dim)

        # ğŸ”¥ æ”¹è¿› 1: Option é€‰æ‹©ç­–ç•¥ï¼ˆåŸºäºçŠ¶æ€ï¼‰
        self.option_policy = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, num_options)
        )

        # ğŸ”¥ æ”¹è¿› 2: Option ä»·å€¼ä¼°è®¡
        self.option_value = nn.Sequential(
            nn.Linear(state_dim + option_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )

        # Option ç»ˆæ­¢ç½‘ç»œ
        self.termination_net = nn.Sequential(
            nn.Linear(state_dim + option_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )

    def get_option_probs(
            self,
            state: torch.Tensor,
            temperature: Optional[float] = None
    ) -> torch.Tensor:
        """
        è·å– Option é€‰æ‹©æ¦‚ç‡

        Args:
            state: [batch, state_dim]
            temperature: æ¸©åº¦å‚æ•°ï¼ˆæ§åˆ¶æ¢ç´¢ï¼‰

        Returns:
            probs: [batch, num_options]
        """
        if temperature is None:
            temperature = self.temperature

        # è®¡ç®— logits
        logits = self.option_policy(state)

        # æ¸©åº¦ç¼©æ”¾
        scaled_logits = logits / temperature

        # Softmax
        probs = F.softmax(scaled_logits, dim=-1)

        return probs

    def select_option(
            self,
            state: torch.Tensor,
            epsilon: float = 0.0
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        é€‰æ‹© Optionï¼ˆå¸¦æ¢ç´¢ï¼‰

        Args:
            state: [batch, state_dim]
            epsilon: Îµ-greedy æ¢ç´¢ç‡

        Returns:
            option_id: [batch] é€‰ä¸­çš„ Option
            option_emb: [batch, option_dim] Option åµŒå…¥
        """
        batch_size = state.size(0)

        # Îµ-greedy æ¢ç´¢
        if torch.rand(1).item() < epsilon:
            # éšæœºé€‰æ‹©
            option_id = torch.randint(0, self.num_options, (batch_size,))
        else:
            # æ ¹æ®ç­–ç•¥é€‰æ‹©
            probs = self.get_option_probs(state)
            option_id = torch.multinomial(probs, num_samples=1).squeeze(-1)

        # è·å– Option åµŒå…¥
        option_emb = self.option_embeddings(option_id)

        return option_id, option_emb

    def compute_option_value(
            self,
            state: torch.Tensor,
            option_emb: torch.Tensor
    ) -> torch.Tensor:
        """
        è®¡ç®— Option çš„ä»·å€¼

        Returns:
            value: [batch, 1]
        """
        x = torch.cat([state, option_emb], dim=-1)
        value = self.option_value(x)

        return value

    def should_terminate(
            self,
            state: torch.Tensor,
            option_emb: torch.Tensor,
            deterministic: bool = False
    ) -> torch.Tensor:
        """
        åˆ¤æ–­ Option æ˜¯å¦åº”è¯¥ç»ˆæ­¢

        Args:
            state: [batch, state_dim]
            option_emb: [batch, option_dim]
            deterministic: æ˜¯å¦ç¡®å®šæ€§ç»ˆæ­¢

        Returns:
            terminate: [batch] bool tensor
        """
        x = torch.cat([state, option_emb], dim=-1)
        termination_prob = self.termination_net(x).squeeze(-1)

        if deterministic:
            # ç¡®å®šæ€§ï¼šæ¦‚ç‡ > 0.5 åˆ™ç»ˆæ­¢
            terminate = termination_prob > 0.5
        else:
            # éšæœºï¼šé‡‡æ ·
            terminate = torch.bernoulli(termination_prob).bool()

        return terminate


# ============================================
# æ–¹æ¡ˆ Dï¼šè¿­ä»£ä¼˜åŒ–çš„æ··åˆ Goal Embedding
# ============================================

class IterativeHybridGoalEmbedding(nn.Module):
    """
    è¿­ä»£ä¼˜åŒ–çš„æ··åˆ Goal Embedding

    æ”¹è¿›ç‚¹ï¼š
    1. å¤šæ­¥è¿­ä»£ä¼˜åŒ–å­ç›®æ ‡
    2. ç²¾ç»†åŒ– Goal ç¼–ç 
    3. è‡ªæ³¨æ„åŠ›æœºåˆ¶
    """

    def __init__(
            self,
            local_state_dim: int = 32,
            goal_dim: int = 64,
            subgoal_horizon: int = 5,
            num_refinement_steps: int = 3
    ):
        super().__init__()

        self.local_state_dim = local_state_dim
        self.goal_dim = goal_dim
        self.subgoal_horizon = subgoal_horizon
        self.num_refinement_steps = num_refinement_steps

        # åˆå§‹ Subgoal Generator
        self.initial_subgoal_generator = nn.Sequential(
            nn.Linear(local_state_dim, goal_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(goal_dim * 2, goal_dim),
            nn.Tanh()
        )

        # ğŸ”¥ æ”¹è¿› 1: è¿­ä»£ä¼˜åŒ–å™¨
        self.refinement_steps = nn.ModuleList([
            nn.Sequential(
                nn.Linear(goal_dim * 2, goal_dim),  # goal + context
                nn.ReLU(),
                nn.Linear(goal_dim, goal_dim),
                nn.Tanh()
            )
            for _ in range(num_refinement_steps)
        ])

        # ğŸ”¥ æ”¹è¿› 2: Goal ç¼–ç å™¨ï¼ˆå¤šå±‚ + è‡ªæ³¨æ„åŠ›ï¼‰
        self.goal_encoder = nn.Sequential(
            nn.Linear(goal_dim, goal_dim * 2),
            nn.LayerNorm(goal_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(goal_dim * 2, goal_dim)
        )

        # è‡ªæ³¨æ„åŠ›ï¼ˆç”¨äºç²¾ç»†åŒ–ï¼‰
        self.self_attention = nn.MultiheadAttention(
            embed_dim=goal_dim,
            num_heads=4,
            dropout=0.1,
            batch_first=True
        )

        # ä¸Šä¸‹æ–‡ç¼–ç å™¨
        self.context_encoder = nn.Sequential(
            nn.Linear(local_state_dim, goal_dim),
            nn.ReLU()
        )

    def forward(
            self,
            current_local_state: torch.Tensor,
            return_refinement_history: bool = False
    ) -> Tuple[torch.Tensor, torch.Tensor, Optional[list]]:
        """
        ç”Ÿæˆè¿­ä»£ä¼˜åŒ–çš„ Goal Embedding

        Args:
            current_local_state: [batch, local_state_dim]
            return_refinement_history: æ˜¯å¦è¿”å›ä¼˜åŒ–å†å²

        Returns:
            subgoal: [batch, goal_dim] æœ€ç»ˆå­ç›®æ ‡
            goal_emb: [batch, goal_dim] Goal Embedding
            refinement_history: ä¼˜åŒ–å†å²ï¼ˆå¯é€‰ï¼‰
        """
        batch_size = current_local_state.size(0)

        # 1. ç”Ÿæˆåˆå§‹å­ç›®æ ‡
        subgoal = self.initial_subgoal_generator(current_local_state)

        # ç¼–ç ä¸Šä¸‹æ–‡
        context = self.context_encoder(current_local_state)

        # ğŸ”¥ 2. è¿­ä»£ä¼˜åŒ–
        refinement_history = [subgoal.clone()]

        for refine_step in self.refinement_steps:
            # æ‹¼æ¥å½“å‰å­ç›®æ ‡å’Œä¸Šä¸‹æ–‡
            refine_input = torch.cat([subgoal, context], dim=-1)

            # ç”Ÿæˆä¿®æ­£
            delta = refine_step(refine_input)

            # æ›´æ–°å­ç›®æ ‡
            subgoal = subgoal + 0.3 * delta  # è¾ƒå°çš„æ­¥é•¿

            refinement_history.append(subgoal.clone())

        # ğŸ”¥ 3. è‡ªæ³¨æ„åŠ›ç²¾ç»†åŒ–
        # å°†æ‰€æœ‰ä¼˜åŒ–æ­¥éª¤çš„å­ç›®æ ‡å †å 
        subgoal_sequence = torch.stack(refinement_history, dim=1)  # [batch, steps, goal_dim]

        # è‡ªæ³¨æ„åŠ›
        attn_out, _ = self.self_attention(
            subgoal_sequence, subgoal_sequence, subgoal_sequence
        )

        # å–æœ€åä¸€ä¸ªè¾“å‡º
        refined_subgoal = attn_out[:, -1, :]

        # 4. ç¼–ç ä¸º Goal Embedding
        goal_emb = self.goal_encoder(refined_subgoal)

        # å½’ä¸€åŒ–
        goal_emb = F.normalize(goal_emb, p=2, dim=-1)

        if return_refinement_history:
            return refined_subgoal, goal_emb, refinement_history
        else:
            return refined_subgoal, goal_emb, None

    def compute_intrinsic_reward(
            self,
            achieved_state: torch.Tensor,
            subgoal: torch.Tensor
    ) -> torch.Tensor:
        """
        è®¡ç®—å†…åœ¨å¥–åŠ±
        """
        # å°† achieved_state æŠ•å½±åˆ° subgoal ç©ºé—´
        achieved_proj = self.initial_subgoal_generator(achieved_state)

        # ä½™å¼¦ç›¸ä¼¼åº¦
        similarity = F.cosine_similarity(achieved_proj, subgoal, dim=-1)

        # å¥–åŠ±
        reward = similarity

        return reward


# ============================================
# ä¼˜åŒ–çš„æ‰¹é‡ç´¢å¼•ï¼ˆå¹¶è¡ŒåŒ–ï¼‰
# ============================================

def optimized_batch_indexing(
        node_embeddings: torch.Tensor,
        target_nodes: torch.Tensor,
        batch: torch.Tensor
) -> torch.Tensor:
    """
    ä¼˜åŒ–çš„æ‰¹é‡ç´¢å¼•ï¼ˆå¹¶è¡ŒåŒ–ï¼‰

    æ”¹è¿›ç‚¹ï¼š
    - ä½¿ç”¨ scatter/gather æ“ä½œ
    - å‡å°‘å¾ªç¯
    - æ›´é«˜æ•ˆçš„å†…å­˜è®¿é—®

    Args:
        node_embeddings: [total_nodes, dim]
        target_nodes: [batch_size] å±€éƒ¨ç´¢å¼•
        batch: [total_nodes] å›¾ ID

    Returns:
        target_embs: [batch_size, dim]
    """
    device = node_embeddings.device
    batch_size = target_nodes.size(0)

    # ğŸ”¥ ä¼˜åŒ–ï¼šå¹¶è¡Œè®¡ç®—æ‰€æœ‰åç§»
    # æ‰¾åˆ°æ¯ä¸ªå›¾çš„èµ·å§‹ç´¢å¼•
    unique_batches = torch.unique(batch, sorted=True)
    num_graphs = len(unique_batches)

    # åˆ›å»ºåç§»è¡¨
    offsets = torch.zeros(num_graphs, dtype=torch.long, device=device)

    for i, b in enumerate(unique_batches):
        mask = (batch == b)
        offsets[i] = mask.nonzero(as_tuple=True)[0][0]

    # ğŸ”¥ å‘é‡åŒ–ç´¢å¼•
    # å‡è®¾ batch_size == num_graphsï¼ˆæ¯ä¸ªå›¾ä¸€ä¸ªç›®æ ‡ï¼‰
    global_indices = target_nodes + offsets

    # å®‰å…¨ç´¢å¼•
    target_embs = node_embeddings[global_indices]

    return target_embs


# ============================================
# ä½¿ç”¨ç¤ºä¾‹
# ============================================

if __name__ == "__main__":
    print("=" * 70)
    print("Goal Embedding æœ€ç»ˆä¼˜åŒ–ç‰ˆæœ¬")
    print("=" * 70)

    # æµ‹è¯•å¢å¼ºç‰ˆ Relative Goal
    print("\n1. Enhanced Relative Goal Embedding")
    rel_goal = EnhancedRelativeGoalEmbedding(
        node_feat_dim=32,
        goal_dim=64,
        use_learned_scaling=True,
        use_attention=True
    )

    current = torch.randn(4, 32)
    target = torch.randn(4, 32)
    goal_emb, info = rel_goal(current, target)

    print(f"   Goal shape: {goal_emb.shape}")
    print(f"   Scale factor: {info['scale_factor']:.3f}")
    print(f"   Base goal norm: {info['base_goal_norm']:.3f}")

    # æµ‹è¯•è‡ªé€‚åº” Subgoal
    print("\n2. Adaptive Subgoal Embedding")
    subgoal_gen = AdaptiveSubgoalEmbedding(
        state_dim=32,
        goal_dim=64,
        adaptive_distance=True
    )

    state = torch.randn(4, 32)
    complexity = torch.rand(4, 1) * 0.5  # å¤æ‚åº¦ [0, 0.5]
    subgoal, info = subgoal_gen(state, complexity)

    print(f"   Subgoal shape: {subgoal.shape}")
    print(f"   Predicted distance: {info['predicted_distance']:.3f}")
    print(f"   Reachability: {info['reachability']:.3f}")

    # æµ‹è¯•å¢å¼ºç‰ˆ Option
    print("\n3. Enhanced Option Embedding")
    option_gen = EnhancedOptionEmbedding(
        num_options=4,
        option_dim=64,
        state_dim=32
    )

    probs = option_gen.get_option_probs(state)
    option_id, option_emb = option_gen.select_option(state)
    value = option_gen.compute_option_value(state, option_emb)

    print(f"   Option probs: {probs[0].tolist()}")
    print(f"   Selected option: {option_id[0].item()}")
    print(f"   Option value: {value[0].item():.3f}")

    # æµ‹è¯•è¿­ä»£ Hybrid
    print("\n4. Iterative Hybrid Goal Embedding")
    hybrid = IterativeHybridGoalEmbedding(
        local_state_dim=32,
        goal_dim=64,
        num_refinement_steps=3
    )

    subgoal, goal_emb, history = hybrid(state, return_refinement_history=True)

    print(f"   Final subgoal shape: {subgoal.shape}")
    print(f"   Goal embedding shape: {goal_emb.shape}")
    print(f"   Refinement steps: {len(history)}")

    print("\n" + "=" * 70)
    print("æ‰€æœ‰æ”¹è¿›ç‚¹å·²å®ç°ï¼š")
    print("  âœ… 1. åŠ¨æ€ç¼©æ”¾å› å­")
    print("  âœ… 2. è‡ªé€‚åº”å­ç›®æ ‡è·ç¦»")
    print("  âœ… 3. Softmax Option é€‰æ‹©")
    print("  âœ… 4. è¿­ä»£ç›®æ ‡ä¼˜åŒ–")
    print("  âœ… 5. å¹¶è¡ŒåŒ–æ‰¹é‡ç´¢å¼•")
    print("  âœ… 6. å­ç›®æ ‡å¯è¾¾æ€§é¢„æµ‹")
    print("  âœ… 7. æ³¨æ„åŠ›æœºåˆ¶èåˆ")
    print("=" * 70)