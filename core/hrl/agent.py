#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HRL Agent (é‡æ„ç‰ˆ - å®Œå…¨ä¿®å¤)
æ•´åˆHigh-Levelå’ŒLow-Levelç­–ç•¥ï¼Œæä¾›ç»Ÿä¸€çš„åˆ†å±‚å†³ç­–æ¥å£
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import random
import logging
from collections import deque
from typing import Tuple, Dict, Any, Optional

from torch_geometric.nn import global_mean_pool

logger = logging.getLogger(__name__)
logger.setLevel(logging.WARNING)


class HRLAgent:
    """
    Hierarchical RL Agent (é‡æ„ç‰ˆ)

    èŒè´£ï¼š
    - æ•´åˆHigh-Levelå’ŒLow-Levelç­–ç•¥
    - ç®¡ç†åˆ†å±‚å†³ç­–æµç¨‹
    - å¤„ç†åˆ†å±‚ç»éªŒå›æ”¾
    - æ”¯æŒDAggerï¼ˆä¸“å®¶æŒ‡å¯¼ï¼‰

    æ¶æ„ï¼š
    High-Level: é€‰æ‹©ç›®æ ‡èŠ‚ç‚¹ï¼ˆsubgoal selectionï¼‰
    Low-Level: æ‰§è¡Œè·¯å¾„è§„åˆ’ï¼ˆgoal-conditionedï¼‰
    """

    def __init__(self, config, encoder=None, phase=3, goal_strategy='adaptive', **kwargs):
        self.config = config
        self.phase = int(phase)
        self.goal_strategy = goal_strategy

        # è®¾å¤‡é…ç½®
        use_cuda = config.get('use_cuda', False)
        self.device = torch.device('cuda' if torch.cuda.is_available() and use_cuda else 'cpu')

        # ============================================
        # ç¯å¢ƒå‚æ•°
        # ============================================
        env_cfg = config.get('environment', config.get('env', {}))
        self.n_actions = kwargs.get('low_action_dim', env_cfg.get('nb_low_level_actions', 50))
        self.n_goals = kwargs.get('high_action_dim', env_cfg.get('nb_high_level_goals', 10))

        # ============================================
        # HRLå‚æ•°
        # ============================================
        hrl_cfg = config.get('hrl', {})
        self.state_dim = hrl_cfg.get('state_dim', 128)
        self.goal_dim = hrl_cfg.get('goal_dim', 64)
        self.hidden_dim = hrl_cfg.get('hidden_dim', 128)

        self.subgoal_horizon = hrl_cfg.get('subgoal_horizon', 20)
        self.intrinsic_reward_weight = hrl_cfg.get('intrinsic_reward_weight', 0.3)

        # ============================================
        # Encoderï¼ˆGNNç‰¹å¾æå–ï¼‰
        # ============================================
        if encoder is not None:
            self.encoder = encoder.to(self.device)
            self.encoder.eval()
            for param in self.encoder.parameters():
                param.requires_grad = False
            logger.info("âœ… ä½¿ç”¨é¢„è®­ç»ƒEncoder")
        else:
            self.encoder = None
            logger.info("âš ï¸ æœªæä¾›Encoderï¼Œå°†åœ¨è¿è¡Œæ—¶åˆ›å»º")

        # ============================================
        # High-Level Policy
        # ============================================
        from core.hrl.high_policy import HighLevelPolicy

        high_config = {
            'use_cuda': config.get('use_cuda', False),
            'hidden_dim': self.hidden_dim,
            'goal_dim': self.goal_dim,
            'gnn_output_dim': self.state_dim,
            'environment': {
                'nb_high_level_goals': self.n_goals
            },
            'dropout': config.get('dropout', 0.1)
        }

        self.high_policy = HighLevelPolicy(high_config).to(self.device)

        # ============================================
        # Low-Level Policy
        # ============================================
        from core.hrl.low_policy import GoalConditionedLowLevelPolicy

        low_config = {
            'use_cuda': config.get('use_cuda', False),
            'state_dim': self.state_dim,
            'goal_dim': self.goal_dim,
            'hidden_dim': self.hidden_dim,
            'environment': {
                'nb_low_level_actions': self.n_actions
            },
            'dropout': config.get('dropout', 0.1)
        }

        self.low_policy = GoalConditionedLowLevelPolicy(low_config).to(self.device)

        # ============================================
        # Target Networks
        # ============================================
        self.target_high_policy = HighLevelPolicy(high_config).to(self.device)
        self.target_high_policy.load_state_dict(self.high_policy.state_dict())

        self.target_low_policy = GoalConditionedLowLevelPolicy(low_config).to(self.device)
        self.target_low_policy.load_state_dict(self.low_policy.state_dict())

        # ============================================
        # Optimizers
        # ============================================
        training_cfg = config.get('training', {})

        lr_high = training_cfg.get('lr_high', training_cfg.get('learning_rate', 1e-4))
        lr_low = training_cfg.get('lr_low', training_cfg.get('learning_rate', 1e-4))

        self.optimizer_high = optim.Adam(self.high_policy.parameters(), lr=lr_high)
        self.optimizer_low = optim.Adam(self.low_policy.parameters(), lr=lr_low)

        # ============================================
        # è®­ç»ƒå‚æ•°
        # ============================================
        self.batch_size = int(training_cfg.get('batch_size', 32))
        self.gamma = float(training_cfg.get('gamma', 0.99))
        self.target_update_freq = int(training_cfg.get('target_update_freq', 1000))

        # Epsiloné…ç½®
        epsilon_cfg = training_cfg.get('epsilon', {})

        self.epsilon_high_start = float(epsilon_cfg.get('initial_high', epsilon_cfg.get('initial', 0.3)))
        self.epsilon_high_end = float(epsilon_cfg.get('final_high', epsilon_cfg.get('final', 0.05)))
        self.epsilon_high = self.epsilon_high_start

        self.epsilon_low_start = float(epsilon_cfg.get('initial_low', epsilon_cfg.get('initial', 0.3)))
        self.epsilon_low_end = float(epsilon_cfg.get('final_low', epsilon_cfg.get('final', 0.05)))
        self.epsilon_low = self.epsilon_low_start

        self.epsilon_decay = float(epsilon_cfg.get('decay_steps', 50000))

        # ============================================
        # ç»éªŒå›æ”¾
        # ============================================
        buffer_size = int(training_cfg.get('buffer_size', 50000))

        self.high_memory = deque(maxlen=buffer_size // 10)
        self.low_memory = deque(maxlen=buffer_size)

        # ============================================
        # ============================================
        # çŠ¶æ€ç®¡ç†
        # ============================================
        self.current_subgoal = None  # æ•´æ•°ï¼ˆèŠ‚ç‚¹IDï¼‰
        self.current_subgoal_emb = None  # Tensorå½¢å¼çš„subgoal embedding
        self.current_goal_emb = None  # Goal embedding
        self.subgoal_steps = 0

        # å‘åå…¼å®¹ï¼šæ—§ä»£ç å¯èƒ½ä½¿ç”¨ subgoal_step_count
        self.subgoal_step_count = 0

        self.steps_done = 0
        self.update_count = 0

        self._training = True

        # ============================================
        # å‘åå…¼å®¹ï¼šæ·»åŠ  goal_embedding
        # ============================================
        from core.hrl.goal_embedding import (
            AdaptiveSubgoalEmbedding,
            EnhancedRelativeGoalEmbedding,
            IterativeHybridGoalEmbedding
        )

        if self.goal_strategy == 'adaptive':
            self.goal_embedding = AdaptiveSubgoalEmbedding(
                state_dim=self.state_dim,
                goal_dim=self.goal_dim
            ).to(self.device)
        elif self.goal_strategy == 'hybrid':
            self.goal_embedding = IterativeHybridGoalEmbedding(
                local_state_dim=self.state_dim,
                goal_dim=self.goal_dim
            ).to(self.device)
        else:  # 'relative'
            self.goal_embedding = EnhancedRelativeGoalEmbedding(
                node_feat_dim=self.state_dim,
                goal_dim=self.goal_dim
            ).to(self.device)
        # âœ… è‡ªé€‚åº”epsilon
        self.adaptive_epsilon = config.get('hrl', {}).get('adaptive_epsilon', True)
        self.min_epsilon_low = 0.01

    def select_action(
            self,
            state: Dict,
            unconnected_dests: Optional[list] = None,
            action_mask: Optional[np.ndarray] = None,
            use_expert: bool = False,
            expert_action: Optional[int] = None,
            blacklist_info: Optional[dict] = None  # âœ… æ–°å¢å‚æ•°
    ) -> Tuple[int, int, Dict]:
        """
        åˆ†å±‚åŠ¨ä½œé€‰æ‹©ï¼ˆé»‘åå•æ„ŸçŸ¥ï¼‰

        Args:
            state: ç¯å¢ƒçŠ¶æ€
            unconnected_dests: æœªè¿æ¥çš„ç›®çš„èŠ‚ç‚¹åˆ—è¡¨
            action_mask: Low-LevelåŠ¨ä½œmask
            use_expert: æ˜¯å¦ä½¿ç”¨ä¸“å®¶
            expert_action: ä¸“å®¶åŠ¨ä½œ
            blacklist_info: é»‘åå•ä¿¡æ¯ï¼ˆæ–°å¢ï¼‰

        Returns:
            high_action: High-LevelåŠ¨ä½œï¼ˆç›®æ ‡ç´¢å¼•ï¼‰
            low_action: Low-LevelåŠ¨ä½œï¼ˆä¸‹ä¸€è·³èŠ‚ç‚¹ï¼‰
            info: ä¿¡æ¯å­—å…¸
        """
        info = {
            'high_level_decision': False,
            'subgoal': self.current_subgoal,
            'subgoal_steps': self.subgoal_steps,
            'source': 'agent',
            'blacklist_total': 0
        }

        try:
            # âœ… æ ¹æ®é»‘åå•è°ƒæ•´epsilon
            if self.adaptive_epsilon and blacklist_info:
                blacklist_count = blacklist_info.get('total', 0)
                blacklist_ratio = blacklist_count / self.n_actions if self.n_actions > 0 else 0

                # é»‘åå•å¤š â†’ é™ä½æ¢ç´¢ï¼ˆé¿å…è¸©å‘ï¼‰
                adaptive_factor = 1.0 - blacklist_ratio * 0.3
                self.epsilon_low = max(
                    self.min_epsilon_low,
                    self.epsilon_low_start * adaptive_factor
                )

                info['adaptive_epsilon'] = self.epsilon_low
                info['blacklist_ratio'] = blacklist_ratio

            # è®°å½•é»‘åå•ä¿¡æ¯
            if blacklist_info:
                info['blacklist_total'] = blacklist_info.get('total', 0)
                info['blacklist_nodes'] = blacklist_info.get('nodes', [])

            # ============================================
            # 1. åˆ¤æ–­æ˜¯å¦éœ€è¦æ–°çš„subgoal
            # ============================================
            need_new_subgoal = self._need_new_subgoal(state, unconnected_dests)

            if need_new_subgoal:
                # ============================================
                # High-Level Decision
                # ============================================
                if use_expert and expert_action is not None:
                    # âœ… ä¸“å®¶å»ºè®®ä¹Ÿè¦æ£€æŸ¥Mask
                    if action_mask is None or (
                            0 <= expert_action < len(action_mask) and action_mask[expert_action] > 0):
                        self.current_subgoal = self._extract_subgoal_from_expert(
                            expert_action, unconnected_dests
                        )
                        info['source'] = 'expert_high'
                    else:
                        # ä¸“å®¶å»ºè®®è¢«Maskï¼Œç”¨Agent
                        logger.warning(f"âš ï¸ ä¸“å®¶Highå»ºè®®{expert_action}è¢«Maskï¼Œæ”¹ç”¨Agent")
                        self.current_subgoal = self._select_subgoal(state, unconnected_dests)
                        info['source'] = 'agent_high_fallback'
                else:
                    # Agentæ¨¡å¼ï¼šä½¿ç”¨High-Levelç­–ç•¥
                    self.current_subgoal = self._select_subgoal(state, unconnected_dests)
                    info['source'] = 'agent_high'

                # ç”Ÿæˆgoal embedding
                self._generate_goal_embedding(state)

                self.subgoal_steps = 0
                info['high_level_decision'] = True
                info['subgoal'] = self.current_subgoal

                logger.debug(f"ğŸ¯ æ–°å­ç›®æ ‡: {self.current_subgoal}")

            # ============================================
            # 2. Low-Level Executionï¼ˆé»‘åå•æ„ŸçŸ¥ï¼‰
            # ============================================
            if use_expert and expert_action is not None and not need_new_subgoal:
                # âœ… ä¸“å®¶LowåŠ¨ä½œä¹Ÿè¦æ£€æŸ¥Mask
                if action_mask is None or (0 <= expert_action < len(action_mask) and action_mask[expert_action] > 0):
                    low_action = expert_action
                    info['source'] = 'expert_low'
                else:
                    logger.warning(f"âš ï¸ ä¸“å®¶LowåŠ¨ä½œ{expert_action}è¢«Maskï¼Œæ”¹ç”¨Agent")
                    low_action = self._select_low_action_with_blacklist(
                        state, action_mask, blacklist_info
                    )
                    info['source'] = 'agent_low_fallback'
            else:
                # Agentæ¨¡å¼ï¼šä½¿ç”¨Low-Levelç­–ç•¥ï¼ˆé»‘åå•æ„ŸçŸ¥ï¼‰
                low_action = self._select_low_action_with_blacklist(
                    state, action_mask, blacklist_info
                )
                info['source'] = 'agent_low'

            # è®°å½•é€‰æ‹©çš„åŠ¨ä½œ
            info['low_action'] = low_action
            if action_mask is not None:
                info['action_mask_sum'] = action_mask.sum()

            # å¦‚æœæ˜¯é»‘åå•ä¸­çš„èŠ‚ç‚¹ï¼Œè®°å½•è­¦å‘Š
            if blacklist_info and low_action in blacklist_info.get('nodes', []):
                info['blacklisted_action'] = True
                logger.warning(f"âš ï¸ Agenté€‰æ‹©äº†é»‘åå•ä¸­çš„èŠ‚ç‚¹ {low_action}")

            self.subgoal_steps += 1
            self.subgoal_step_count = self.subgoal_steps  # å‘åå…¼å®¹
            self.steps_done += 1

            # High actionï¼ˆç›®æ ‡åœ¨unconnectedä¸­çš„ç´¢å¼•ï¼‰
            high_action = 0
            if unconnected_dests and self.current_subgoal is not None:
                if self.current_subgoal in unconnected_dests:
                    high_action = unconnected_dests.index(self.current_subgoal)

            return high_action, low_action, info

        except Exception as e:
            logger.error(f"[Select Action] Error: {e}")
            import traceback
            traceback.print_exc()

            # Fallbackï¼šä½¿ç”¨åŠ¨ä½œæ©ç é€‰æ‹©æœ‰æ•ˆåŠ¨ä½œ
            if action_mask is not None:
                valid_actions = np.where(action_mask > 0)[0]
                if len(valid_actions) > 0:
                    low_action = random.choice(valid_actions)
                else:
                    low_action = random.randint(0, self.n_actions - 1)
            else:
                low_action = random.randint(0, self.n_actions - 1)

            return 0, low_action, info
    def _need_new_subgoal(self, state: Dict, unconnected_dests: Optional[list]) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦æ–°çš„subgoal"""
        # 1. æ²¡æœ‰subgoal
        if self.current_subgoal is None:
            return True

        # 2. æ²¡æœ‰æœªè¿æ¥èŠ‚ç‚¹
        if not unconnected_dests or len(unconnected_dests) == 0:
            return False

        # 3. Subgoalå·²è¿æ¥
        if self.current_subgoal not in unconnected_dests:
            return True

        # 4. Subgoalè¶…æ—¶
        if self.subgoal_steps >= self.subgoal_horizon:
            logger.debug(f"âš ï¸ Subgoalè¶…æ—¶ (steps={self.subgoal_steps})")
            return True

        # 5. å·²åˆ°è¾¾subgoal
        current_pos = state.get('current_position', -1)
        if current_pos == self.current_subgoal:
            return True

        return False

    def _select_subgoal(self, state: Dict, unconnected_dests: list) -> int:
        """High-Levelç­–ç•¥é€‰æ‹©subgoal"""
        if not unconnected_dests or len(unconnected_dests) == 0:
            return None

        # è·å–å›¾åµŒå…¥
        graph_emb = self._get_graph_embedding(state)

        # åˆ›å»ºmaskï¼ˆåªå…è®¸é€‰æ‹©æœªè¿æ¥èŠ‚ç‚¹ï¼‰
        valid_goals = torch.zeros(1, self.n_goals, device=self.device)
        for i, dest in enumerate(unconnected_dests):
            if i < self.n_goals:
                valid_goals[0, i] = 1

        # High-Levelç­–ç•¥é€‰æ‹©
        with torch.no_grad():
            goal_idx, goal_emb = self.high_policy.select_goal(
                graph_emb,
                valid_goals,
                epsilon=self.epsilon_high
            )

        # ä¿å­˜goal embedding
        self.current_goal_emb = goal_emb

        # æ˜ å°„å›å®é™…èŠ‚ç‚¹
        goal_idx = goal_idx.item()
        if goal_idx < len(unconnected_dests):
            subgoal = unconnected_dests[goal_idx]
        else:
            subgoal = unconnected_dests[0]

        return subgoal

    def _generate_goal_embedding(self, state: Dict):
        """ç”Ÿæˆgoal embedding"""
        try:
            graph_emb = self._get_graph_embedding(state)

            with torch.no_grad():
                _, goal_emb, _ = self.high_policy(graph_emb, return_subgoal=True)

            self.current_goal_emb = goal_emb

        except Exception as e:
            logger.error(f"[Goal Embedding] Error: {e}")
            self.current_goal_emb = torch.zeros(1, self.goal_dim, device=self.device)

    def _select_low_action(self, state: Dict, action_mask: Optional[np.ndarray]) -> int:
        """Low-Levelç­–ç•¥é€‰æ‹©åŠ¨ä½œ"""
        # è·å–å›¾åµŒå…¥
        graph_emb = self._get_graph_embedding(state)

        # Goal embedding
        if self.current_goal_emb is None:
            self._generate_goal_embedding(state)

        # è½¬æ¢mask
        if action_mask is not None:
            mask_tensor = torch.FloatTensor(action_mask).unsqueeze(0).to(self.device)
        else:
            mask_tensor = None

        # Low-Levelç­–ç•¥é€‰æ‹©
        with torch.no_grad():
            action, _ = self.low_policy.select_action(
                graph_emb,
                self.current_goal_emb,
                mask_tensor,
                epsilon=self.epsilon_low
            )

        return action.item()

    def _get_graph_embedding(self, state):
        """
        ğŸ”¥ [æ ¸å¿ƒä¿®å¤] è·å–å›¾åµŒå…¥ï¼Œæ”¯æŒ Batch å¤„ç†
        """
        # 1. å°è¯•ä½¿ç”¨çœŸå®çš„ Encoder (å¦‚æœæœ‰)
        if self.encoder is not None:
            try:
                # æƒ…å†µ A: PyG Batch å¯¹è±¡ (è®­ç»ƒæ—¶)
                if hasattr(state, 'batch') and state.batch is not None:
                    return self.encoder(state.x, state.edge_index, state.batch)

                # æƒ…å†µ B: å•ä¸ª Data å¯¹è±¡ (æ¨ç†æ—¶)
                # æ„é€ ä¸€ä¸ªå…¨0çš„ batch å‘é‡
                batch = torch.zeros(state.x.size(0), dtype=torch.long, device=self.device)
                return self.encoder(state.x, state.edge_index, batch)

            except Exception as e:
                logger.error(f"Encoder forward failed: {e}")
                # å¦‚æœå‡ºé”™ï¼Œå‘ä¸‹æ‰§è¡Œ Fallback

        # 2. Fallback (ä»…ç”¨äºé˜²æ­¢å´©æºƒï¼Œè¾“å‡ºéšæœºå™ªå£°)
        # å¿…é¡»è¿”å›æ­£ç¡®çš„ Batch Sizeï¼Œå¦åˆ™ Loss è®¡ç®—ä¼šæŠ¥é”™
        batch_size = 1
        if hasattr(state, 'num_graphs'):
            # PyG Batch å¯¹è±¡åŒ…å« num_graphs å±æ€§
            batch_size = state.num_graphs
        elif hasattr(state, 'batch') and state.batch is not None:
            batch_size = state.batch.max().item() + 1

        # logger.warning(f"Using random embedding fallback (Batch Size: {batch_size})")
        return torch.randn(batch_size, self.state_dim, device=self.device)
    def _extract_subgoal_from_expert(self, expert_action: int, unconnected_dests: list) -> int:
        """ä»ä¸“å®¶åŠ¨ä½œæå–subgoal"""
        if unconnected_dests and expert_action in unconnected_dests:
            return expert_action

        if unconnected_dests and len(unconnected_dests) > 0:
            return unconnected_dests[0]

        return None

    # ============================================
    # å‘åå…¼å®¹æ–¹æ³•
    # ============================================

    def _generate_and_encode_subgoal(self, state: Dict):
        """
        å‘åå…¼å®¹ï¼šç”Ÿæˆå¹¶ç¼–ç subgoal

        è¿™æ˜¯æ—§ç‰ˆæœ¬çš„æ–¹æ³•åï¼Œè°ƒç”¨æ–°çš„ _generate_goal_embedding
        åŒæ—¶ä½¿ç”¨ goal_embedding æ¨¡å—ç”Ÿæˆæ›´å¥½çš„åµŒå…¥

        Args:
            state: ç¯å¢ƒçŠ¶æ€

        æ³¨æ„:
        - current_subgoal åº”è¯¥æ˜¯æ•´æ•°ï¼ˆèŠ‚ç‚¹IDï¼‰ï¼Œç”± _select_subgoal è®¾ç½®
        - current_subgoal_emb æ˜¯tensorå½¢å¼çš„embedding
        - current_goal_emb æ˜¯goal embedding
        """
        try:
            # è·å–å›¾åµŒå…¥
            graph_emb = self._get_graph_embedding(state)

            # ä½¿ç”¨ goal_embedding ç”Ÿæˆsubgoal embedding
            with torch.no_grad():
                if self.goal_strategy == 'adaptive':
                    complexity = torch.tensor([[0.5]], device=self.device)
                    subgoal_emb, info = self.goal_embedding(graph_emb, complexity)
                    # AdaptiveSubgoalEmbedding è¿”å› (subgoal, info)
                    # éœ€è¦æ‰‹åŠ¨ç”Ÿæˆ goal_emb
                    if subgoal_emb.shape[-1] >= self.goal_dim:
                        goal_emb = subgoal_emb[..., :self.goal_dim]
                    else:
                        # å¦‚æœ subgoal ç»´åº¦å°äº goal_dimï¼Œå¡«å……
                        padding = torch.zeros(
                            subgoal_emb.size(0),
                            self.goal_dim - subgoal_emb.size(-1),
                            device=self.device
                        )
                        goal_emb = torch.cat([subgoal_emb, padding], dim=-1)

                elif self.goal_strategy == 'hybrid':
                    subgoal_emb, goal_emb, _ = self.goal_embedding(graph_emb)

                else:  # 'relative'
                    target_emb = torch.randn_like(graph_emb)  # ä¸´æ—¶ç›®æ ‡
                    goal_emb, info = self.goal_embedding(graph_emb, target_emb)
                    # EnhancedRelativeGoalEmbedding è¿”å› (goal_emb, info)
                    # ä½¿ç”¨ goal_emb ä½œä¸º subgoal_emb
                    subgoal_emb = goal_emb

            # ç¡®ä¿ç»´åº¦æ­£ç¡®
            if subgoal_emb.shape[-1] != self.goal_dim:
                subgoal_emb = subgoal_emb[..., :self.goal_dim]
            if goal_emb.shape[-1] != self.goal_dim:
                goal_emb = goal_emb[..., :self.goal_dim]

            # è®¾ç½®embeddings
            # æ³¨æ„ï¼šcurrent_subgoal æ˜¯æ•´æ•°ï¼ˆèŠ‚ç‚¹IDï¼‰ï¼Œç”± _select_subgoal è®¾ç½®
            # current_subgoal_emb æ˜¯tensorå½¢å¼çš„embedding
            self.current_subgoal_emb = subgoal_emb  # tensor
            self.current_goal_emb = goal_emb
            self.subgoal_step_count = 0

        except Exception as e:
            logger.error(f"[Generate Subgoal] Error: {e}")
            # Fallback
            self.current_subgoal_emb = torch.zeros(1, self.goal_dim, device=self.device)
            self.current_goal_emb = torch.zeros(1, self.goal_dim, device=self.device)
            self.subgoal_step_count = 0

    def store_transition_high(
            self, state: Dict, goal: int, reward: float, next_state: Dict, done: bool
    ):
        """å­˜å‚¨High-Levelç»éªŒ (ä¿®å¤ç‰ˆ: é˜²æ­¢ç´¢å¼•è¶Šç•Œ)"""

        # ğŸ”¥ [å…³é”®ä¿®å¤] ç¡®ä¿ goal ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
        # High-Level Qç½‘ç»œåªæœ‰ n_goals ä¸ªè¾“å‡º (ä¾‹å¦‚10)
        # å¦‚æœ goal æ˜¯ç‰©ç†èŠ‚ç‚¹ID (ä¾‹å¦‚24)ï¼Œä¼šå¯¼è‡´ç´¢å¼•è¶Šç•Œ
        goal_idx = goal
        if isinstance(goal, (int, np.integer)):
            if goal >= self.n_goals:
                # ä½¿ç”¨æ¨¡è¿ç®—æ˜ å°„åˆ°æœ‰æ•ˆèŒƒå›´ [0, n_goals-1]
                # è¿™æ˜¯ä¸€ä¸ªå…œåº•ç­–ç•¥ï¼Œé˜²æ­¢ç¨‹åºå´©æºƒ
                goal_idx = goal % self.n_goals
                # logger.debug(f"âš ï¸ Goalæ˜ å°„: {goal} -> {goal_idx} (Max: {self.n_goals})")
            elif goal < 0:
                goal_idx = 0  # é»˜è®¤å€¼

        self.high_memory.append({
            'state': state,
            'goal': goal_idx,  # âœ… å­˜å‚¨ä¿®æ­£åçš„ç´¢å¼•
            'reward': reward,
            'next_state': next_state,
            'done': done
        })

    def store_transition_low(
            self, state: Dict, action: int, reward: float, next_state: Dict, done: bool
    ):
        """å­˜å‚¨Low-Levelç»éªŒ"""
        self.low_memory.append({
            'state': state,
            'action': action,
            'reward': reward,
            'next_state': next_state,
            'done': done,
            'goal_emb': self.current_goal_emb
        })

    # ============================================
    # å‘åå…¼å®¹ï¼šä¿ç•™æ—§æ¥å£
    # ============================================

    def store_transition(self, state, action, reward, next_state, done, goal=None, next_valid_actions=None):
        """å‘åå…¼å®¹çš„store_transition"""
        # åˆ†å‘åˆ°å¯¹åº”çš„å­˜å‚¨å‡½æ•°
        if isinstance(action, (list, tuple)) and len(action) == 2:
            high_action, low_action = action
            # åˆ†åˆ«å­˜å‚¨
            if goal is not None:
                self.store_transition_high(state, goal, reward, next_state, done)
            self.store_transition_low(state, low_action, reward, next_state, done)
        else:
            # é»˜è®¤å­˜å‚¨åˆ°Low-Level
            self.store_transition_low(state, action, reward, next_state, done)

    def update(self) -> float:
        """å‘åå…¼å®¹çš„updateæ¥å£"""
        losses = self.update_policies()

        # è¿”å›æ€»loss
        total_loss = losses.get('high_loss', 0.0) + losses.get('low_loss', 0.0)
        return total_loss

    def update_policies(self) -> Dict[str, float]:
        """æ›´æ–°ç­–ç•¥ï¼ˆæ–°æ¥å£ï¼‰"""
        losses = {}

        # High-Levelæ›´æ–°
        if len(self.high_memory) >= self.batch_size // 4:
            high_loss = self._update_high_level()
            losses['high_loss'] = high_loss

        # Low-Levelæ›´æ–°
        if len(self.low_memory) >= self.batch_size:
            low_loss = self._update_low_level()
            losses['low_loss'] = low_loss

        # æ›´æ–°target networks
        self._update_target_networks()

        # æ›´æ–°epsilon
        self._update_epsilon()

        return losses

    def _update_high_level(self) -> float:
        """æ›´æ–°High-Levelç­–ç•¥ (Double DQN)"""
        # 1. æ£€æŸ¥æ ·æœ¬æ•°é‡
        if len(self.high_memory) < self.batch_size:
            return 0.0

        try:
            # 2. é‡‡æ ·
            batch = random.sample(self.high_memory, self.batch_size)

            # 3. å‡†å¤‡æ•°æ®
            # æå– Graph Embedding
            # æ³¨æ„ï¼šå¦‚æœ graph embedding è®¡ç®—è¾ƒæ…¢ï¼Œè¿™é‡Œä¼šæ˜¯ç“¶é¢ˆ
            state_embs = [self._get_graph_embedding(x['state']) for x in batch]
            next_state_embs = [self._get_graph_embedding(x['next_state']) for x in batch]

            # å †å å¹¶ç¡®ä¿åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            state_tensor = torch.cat(state_embs, dim=0).to(self.device)
            next_state_tensor = torch.cat(next_state_embs, dim=0).to(self.device)

            goals = torch.tensor([x['goal'] for x in batch], device=self.device).long().unsqueeze(1)
            rewards = torch.tensor([x['reward'] for x in batch], device=self.device).float().unsqueeze(1)
            dones = torch.tensor([x['done'] for x in batch], device=self.device).float().unsqueeze(1)

            # 4. è®¡ç®— Current Q
            # ğŸš€ ä¼˜åŒ–ï¼šä¼ å…¥ return_subgoal=Falseï¼Œåªè®¡ç®— Q å€¼ï¼Œä¸ç”Ÿæˆ Subgoalï¼ŒèŠ‚çœç®—åŠ›
            # HighPolicy forward è¿”å›: (q_values, subgoal_emb, value)
            curr_q_values, _, _ = self.high_policy(state_tensor, return_subgoal=False)
            curr_q = curr_q_values.gather(1, goals)

            # 5. è®¡ç®— Target Q (Double DQN)
            with torch.no_grad():
                # Online Net é€‰åŠ¨ä½œ
                next_q_online, _, _ = self.high_policy(next_state_tensor, return_subgoal=False)
                next_actions = next_q_online.argmax(dim=1, keepdim=True)

                # Target Net è¯„ä»·å€¼
                next_q_target, _, _ = self.target_high_policy(next_state_tensor, return_subgoal=False)
                next_q = next_q_target.gather(1, next_actions)

                target_q = rewards + (1 - dones) * self.gamma * next_q

            # 6. è®¡ç®— Loss & æ›´æ–°
            loss = F.smooth_l1_loss(curr_q, target_q)

            self.optimizer_high.zero_grad()
            loss.backward()
            # æ¢¯åº¦è£å‰ª
            nn.utils.clip_grad_norm_(self.high_policy.parameters(), 1.0)
            self.optimizer_high.step()

            return loss.item()

        except Exception as e:
            logger.error(f"[Update High Level] Error: {e}")
            import traceback
            traceback.print_exc()
            return 0.0

    def _update_low_level(self) -> float:
        """æ›´æ–°Low-Levelç­–ç•¥ (Goal-Conditioned Double DQN)"""
        # 1. æ£€æŸ¥æ ·æœ¬æ•°é‡
        if len(self.low_memory) < self.batch_size:
            return 0.0

        try:
            # 2. é‡‡æ ·
            batch = random.sample(self.low_memory, self.batch_size)

            # 3. å‡†å¤‡æ•°æ®
            state_embs = [self._get_graph_embedding(x['state']) for x in batch]
            next_state_embs = [self._get_graph_embedding(x['next_state']) for x in batch]

            state_tensor = torch.cat(state_embs, dim=0).to(self.device)
            next_state_tensor = torch.cat(next_state_embs, dim=0).to(self.device)

            actions = torch.tensor([x['action'] for x in batch], device=self.device).long().unsqueeze(1)
            rewards = torch.tensor([x['reward'] for x in batch], device=self.device).float().unsqueeze(1)
            dones = torch.tensor([x['done'] for x in batch], device=self.device).float().unsqueeze(1)

            # =======================================================
            # ğŸ”¥ [å…³é”®ä¿®å¤] è¿‡æ»¤æ‰ action = -1 çš„æ— æ•ˆæ•°æ®
            # =======================================================
            valid_mask = (actions >= 0).squeeze()

            # å¦‚æœæ•´ä¸ª batch éƒ½æ˜¯æ— æ•ˆåŠ¨ä½œï¼ˆæç«¯æƒ…å†µï¼‰ï¼Œè·³è¿‡
            if valid_mask.sum() == 0:
                return 0.0

            # åªä¿ç•™æœ‰æ•ˆçš„æ•°æ®
            state_tensor = state_tensor[valid_mask]
            next_state_tensor = next_state_tensor[valid_mask]
            actions = actions[valid_mask]
            rewards = rewards[valid_mask]
            dones = dones[valid_mask]

            # é‡æ–°è¿‡æ»¤ batch åˆ—è¡¨ï¼Œä»¥ä¾¿åç»­å¤„ç† goal_embs
            # valid_mask æ˜¯ tensorï¼Œè½¬å› numpy æˆ– list ç´¢å¼•å¤„ç†æ¯”è¾ƒéº»çƒ¦
            # æ›´ç®€å•çš„åšæ³•æ˜¯ï¼šå…ˆå¤„ç†å¥½ tensorï¼Œå†å¤„ç† goal_embs
            # =======================================================

            # å¤„ç† Goal Embedding
            goal_embs = []
            # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å¾—é‡æ–°éå† batchï¼Œæˆ–è€…ç”¨ mask ç­›é€‰
            # ç®€å•èµ·è§ï¼Œæˆ‘ä»¬ç›´æ¥éå† batch å¹¶åº”ç”¨åŒæ ·çš„ mask é€»è¾‘
            # ä½†ä¸Šé¢çš„ valid_mask æ˜¯åŸºäº actions tensor çš„
            # æ‰€ä»¥æˆ‘ä»¬å¯ä»¥è¿™æ ·åšï¼š

            valid_indices = torch.nonzero(valid_mask).squeeze().cpu().tolist()
            if not isinstance(valid_indices, list):  # å•ä¸ªå…ƒç´ æ—¶å¯èƒ½æ˜¯ int
                valid_indices = [valid_indices]

            for idx in valid_indices:
                x = batch[idx]
                g = x.get('goal_emb')
                if g is None:
                    g = torch.zeros(1, self.goal_dim, device=self.device)
                else:
                    g = g.to(self.device)  # ç¡®ä¿åœ¨è®¾å¤‡ä¸Š
                    if g.dim() == 1: g = g.unsqueeze(0)

                    # ç»´åº¦å®‰å…¨æ£€æŸ¥
                    if g.size(1) != self.goal_dim:
                        if g.size(1) > self.goal_dim:
                            g = g[:, :self.goal_dim]
                        else:
                            padding = torch.zeros(g.size(0), self.goal_dim - g.size(1), device=self.device)
                            g = torch.cat([g, padding], dim=1)
                goal_embs.append(g)

            goal_tensor = torch.cat(goal_embs, dim=0)

            # 4. è®¡ç®— Current Q
            # LowPolicy forward è¿”å›: (logits, value)
            curr_logits, _ = self.low_policy(state_tensor, goal_tensor)

            # ğŸ”¥ ç°åœ¨ actions é‡Œæ²¡æœ‰ -1 äº†ï¼Œgather ä¸ä¼šæŠ¥é”™
            curr_q = curr_logits.gather(1, actions)

            # 5. è®¡ç®— Target Q (Double DQN)
            with torch.no_grad():
                # Online Net é€‰åŠ¨ä½œ
                next_logits, _ = self.low_policy(next_state_tensor, goal_tensor)
                next_actions = next_logits.argmax(dim=1, keepdim=True)

                # Target Net è¯„ä»·å€¼
                target_logits, _ = self.target_low_policy(next_state_tensor, goal_tensor)
                next_q = target_logits.gather(1, next_actions)

                target_q = rewards + (1 - dones) * self.gamma * next_q

            # 6. è®¡ç®— Loss & æ›´æ–°
            loss = F.smooth_l1_loss(curr_q, target_q)

            self.optimizer_low.zero_grad()
            loss.backward()
            nn.utils.clip_grad_norm_(self.low_policy.parameters(), 1.0)
            self.optimizer_low.step()

            return loss.item()

        except Exception as e:
            logger.error(f"[Update Low Level] Error: {e}")
            import traceback
            traceback.print_exc()
            return 0.0
    def _update_target_networks(self):
        """æ›´æ–°target networks"""
        self.update_count += 1

        if self.update_count % self.target_update_freq == 0:
            self.target_high_policy.load_state_dict(self.high_policy.state_dict())
            self.target_low_policy.load_state_dict(self.low_policy.state_dict())

    def _update_epsilon(self):
        """æ›´æ–°epsilon"""
        progress = min(self.steps_done / self.epsilon_decay, 1.0)

        self.epsilon_high = self.epsilon_high_start + (
                self.epsilon_high_end - self.epsilon_high_start
        ) * progress

        self.epsilon_low = self.epsilon_low_start + (
                self.epsilon_low_end - self.epsilon_low_start
        ) * progress

    def update_epsilon(self):
        """å‘åå…¼å®¹çš„epsilonæ›´æ–°"""
        self._update_epsilon()

    def train(self):
        """è®­ç»ƒæ¨¡å¼"""
        self._training = True
        self.high_policy.train()
        self.low_policy.train()

    def eval(self):
        """è¯„ä¼°æ¨¡å¼"""
        self._training = False
        self.high_policy.eval()
        self.low_policy.eval()

    def save(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'high_policy': self.high_policy.state_dict(),
            'low_policy': self.low_policy.state_dict(),
            'optimizer_high': self.optimizer_high.state_dict(),
            'optimizer_low': self.optimizer_low.state_dict(),
            'epsilon_high': self.epsilon_high,
            'epsilon_low': self.epsilon_low,
            'steps_done': self.steps_done,
            'config': self.config
        }, path)

        logger.info(f"âœ… æ¨¡å‹å·²ä¿å­˜: {path}")

    def load(self, path: str):
        """åŠ è½½æ¨¡å‹"""
        import os
        if not os.path.exists(path):
            logger.warning(f"âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {path}")
            return

        checkpoint = torch.load(path, map_location=self.device)

        if 'high_policy' in checkpoint:
            self.high_policy.load_state_dict(checkpoint['high_policy'])
            self.target_high_policy.load_state_dict(checkpoint['high_policy'])

        if 'low_policy' in checkpoint:
            self.low_policy.load_state_dict(checkpoint['low_policy'])
            self.target_low_policy.load_state_dict(checkpoint['low_policy'])

        if 'optimizer_high' in checkpoint:
            self.optimizer_high.load_state_dict(checkpoint['optimizer_high'])
        if 'optimizer_low' in checkpoint:
            self.optimizer_low.load_state_dict(checkpoint['optimizer_low'])

        if 'epsilon_high' in checkpoint:
            self.epsilon_high = checkpoint['epsilon_high']
        if 'epsilon_low' in checkpoint:
            self.epsilon_low = checkpoint['epsilon_low']
        if 'steps_done' in checkpoint:
            self.steps_done = checkpoint['steps_done']

        logger.info(f"âœ… æ¨¡å‹å·²åŠ è½½: {path}")

    def _select_low_action_with_blacklist(
            self,
            state: Any,
            action_mask: Optional[np.ndarray] = None,
            blacklist_info: Optional[dict] = None
    ) -> int:
        try:
            # 1. è·å–åŸºç¡€åŠ¨ä½œå’Œæƒé‡
            if action_mask is not None and len(action_mask) > 0:
                valid_mask = action_mask > 0
                valid_actions = np.where(valid_mask)[0]
                if len(valid_actions) == 0: return random.randint(0, self.n_actions - 1)
                action_weights = action_mask[valid_actions].copy()
            else:
                valid_actions = np.arange(self.n_actions)
                action_weights = np.ones(len(valid_actions))

            # 2. é»‘åå•æƒé‡æƒ©ç½š
            if blacklist_info:
                blacklist_nodes = blacklist_info.get('nodes', [])
                for i, action in enumerate(valid_actions):
                    if action in blacklist_nodes: action_weights[i] *= 0.1

            # 3. ç­–ç•¥é€‰æ‹©
            if random.random() < self.epsilon_low:
                # æ¢ç´¢ï¼šåŸºäºæƒé‡çš„æ¦‚ç‡é‡‡æ ·
                p = action_weights / action_weights.sum()
                return int(np.random.choice(valid_actions, p=p))
            else:
                # åˆ©ç”¨ï¼šQç½‘ç»œå†³ç­–
                if self.low_policy is not None:
                    state_emb = self._extract_state_embedding(state)
                    if self.current_goal_emb is None: self._generate_goal_embedding(state)

                    with torch.no_grad():
                        # ğŸ”¥ ä¿®å¤é‡ç‚¹ï¼šå¤„ç† low_policy è¿”å›çš„å…ƒç»„ (logits, value)
                        policy_output = self.low_policy(state_emb, self.current_goal_emb.to(state_emb.device))

                        if isinstance(policy_output, tuple):
                            q_values = policy_output[0].cpu().numpy().flatten()
                        else:
                            q_values = policy_output.cpu().numpy().flatten()

                    # Qå€¼ä¸Maskæƒé‡èåˆ
                    combined_scores = q_values[valid_actions] * action_weights
                    return int(valid_actions[np.argmax(combined_scores)])

                return int(valid_actions[np.argmax(action_weights)])

        except Exception as e:
            logger.error(f"[Select Low Action] Error: {e}")
            return random.randint(0, self.n_actions - 1)

    def _extract_state_embedding(self, state):
        """ğŸ”¥ ä¿®å¤ Data å¯¹è±¡æ— æ³•ç›´æ¥æŠ•å½±çš„é—®é¢˜"""
        try:
            # A. å¤„ç† PyG Data å¯¹è±¡
            if hasattr(state, 'x') and hasattr(state, 'edge_index'):
                if self.encoder is not None:
                    return self._get_graph_embedding(state)

                # å¦‚æœæ²¡æœ‰ Encoderï¼Œæ‰‹åŠ¨æ± åŒ–èŠ‚ç‚¹ç‰¹å¾
                x = state.x
                state_tensor = torch.mean(x, dim=0, keepdim=True)  # [1, node_feat_dim]

            # B. å¤„ç†å·²ç»æ˜¯ Tensor æˆ– Numpy çš„æƒ…å†µ
            else:
                state_tensor = self._prepare_state(state)
                if state_tensor.dim() == 1: state_tensor = state_tensor.unsqueeze(0)

            # ğŸ”¥ ç»Ÿä¸€æŠ•å½±åˆ° hidden_dim (128)
            if state_tensor.size(-1) != self.state_dim:
                if not hasattr(self, '_state_projection'):
                    self._state_projection = nn.Linear(state_tensor.size(-1), self.state_dim).to(self.device)

                with torch.no_grad():
                    # ç¡®ä¿è¾“å…¥æ˜¯ Tensor è€Œé Data å¯¹è±¡
                    state_tensor = self._state_projection(state_tensor.to(self.device))

            return state_tensor

        except Exception as e:
            logger.error(f"æå–state embeddingå¤±è´¥: {e}")
            return torch.zeros(1, self.state_dim, device=self.device)

    def _prepare_state(self, state):
        """è¾…åŠ©æ–¹æ³•ï¼šå°†çŠ¶æ€è½¬æ¢ä¸ºtensor"""
        if isinstance(state, dict):
            # PyG Data æ ¼å¼
            return state
        elif isinstance(state, np.ndarray):
            return torch.FloatTensor(state).unsqueeze(0)
        else:
            # å·²ç»æ˜¯tensor
            return state
    # ============================================
    # 4. æ–°å¢ record_failure æ–¹æ³•
    # ============================================
    def record_failure(self, node_id: int, reason: str):
        """
        è®°å½•èŠ‚ç‚¹å¤±è´¥ï¼ˆç”¨äºå­¦ä¹ é»‘åå•æ¨¡å¼ï¼‰

        Args:
            node_id: å¤±è´¥çš„èŠ‚ç‚¹ID
            reason: å¤±è´¥åŸå› 
        """
        if node_id not in self.failed_nodes_counter:
            self.failed_nodes_counter[node_id] = {
                'count': 0,
                'reasons': [],
                'last_failed': self.steps_done
            }

        self.failed_nodes_counter[node_id]['count'] += 1
        self.failed_nodes_counter[node_id]['reasons'].append(reason)
        self.failed_nodes_counter[node_id]['last_failed'] = self.steps_done

        self.blacklist_history.append({
            'step': self.steps_done,
            'node': node_id,
            'reason': reason,
            'epsilon': self.epsilon_low
        })

        logger.debug(f"ğŸ“ è®°å½•å¤±è´¥: èŠ‚ç‚¹{node_id}, åŸå› :{reason}")

    # ============================================
    # 5. æ–°å¢ get_blacklist_learning_stats æ–¹æ³•
    # ============================================
    def get_blacklist_learning_stats(self) -> dict:
        """
        è·å–é»‘åå•å­¦ä¹ ç»Ÿè®¡

        Returns:
            åŒ…å«å¤±è´¥ç»Ÿè®¡çš„å­—å…¸
        """
        if not self.failed_nodes_counter:
            return {
                'total_failures': 0,
                'unique_failed_nodes': 0,
                'top_failed_nodes': [],
                'blacklist_history_size': 0
            }

        # æŒ‰å¤±è´¥æ¬¡æ•°æ’åº
        sorted_nodes = sorted(
            self.failed_nodes_counter.items(),
            key=lambda x: x[1]['count'],
            reverse=True
        )[:10]  # å–å‰10ä¸ª

        return {
            'total_failures': sum(info['count'] for info in self.failed_nodes_counter.values()),
            'unique_failed_nodes': len(self.failed_nodes_counter),
            'top_failed_nodes': [
                {
                    'node': node,
                    'count': info['count'],
                    'reasons': info['reasons'][-3:]  # æœ€è¿‘3ä¸ªåŸå› 
                }
                for node, info in sorted_nodes
            ],
            'blacklist_history_size': len(self.blacklist_history)
        }

# ============================================
# å‘åå…¼å®¹ï¼šä¿ç•™æ—§æ¥å£
# ============================================

class GoalConditionedHRLAgent(HRLAgent):
    """å‘åå…¼å®¹çš„GoalConditionedHRLAgent"""

    def __init__(self, config, phase=3, goal_strategy='adaptive', **kwargs):
        logger.warning("âš ï¸ GoalConditionedHRLAgentå·²é‡æ„ä¸ºHRLAgentï¼Œä½¿ç”¨å…¼å®¹æ¨¡å¼")
        super().__init__(config, phase=phase, goal_strategy=goal_strategy, **kwargs)


# ... (agent.py çš„å…¶ä»–ä»£ç ä¿æŒä¸å˜) ...

# ============================================
# Helper Function
# ============================================

def create_goal_conditioned_agent(config, phase=3, goal_strategy='adaptive', encoder=None, **kwargs):
    """
    åˆ›å»º HRL Agent çš„å·¥å‚å‡½æ•°

    Args:
        config: é…ç½®å­—å…¸
        phase: è®­ç»ƒé˜¶æ®µ (é»˜è®¤: 3)
        goal_strategy: Goal embedding ç­–ç•¥
        encoder: é¢„è®­ç»ƒçš„ GNN Encoder (å¯é€‰) ğŸ”¥ [å…³é”®æ–°å¢]
        **kwargs: å…¶ä»–å‚æ•°
    """
    # å®ä¾‹åŒ– HRLAgent å¹¶é€ä¼  encoder
    return HRLAgent(
        config=config,
        encoder=encoder,  # ğŸ”¥ æŠŠ encoder ä¼ è¿›å»
        phase=phase,
        goal_strategy=goal_strategy,
        **kwargs
    )


