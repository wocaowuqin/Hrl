
# !/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HRL Agent (é‡æ„ç‰ˆ - å®Œå…¨ä¿®å¤)
æ•´åˆHigh-Levelå’ŒLow-Levelç­–ç•¥ï¼Œæä¾›ç»Ÿä¸€çš„åˆ†å±‚å†³ç­–æ¥å£
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import random
import logging
from collections import deque
from typing import Tuple, Dict, Any, Optional

from torch_geometric.nn import global_mean_pool

logger = logging.getLogger(__name__)
logger.setLevel(logging.WARNING)


class HRLAgent:
    """
    Hierarchical RL Agent (é‡æ„ç‰ˆ)

    èŒè´£ï¼š
    - æ•´åˆHigh-Levelå’ŒLow-Levelç­–ç•¥
    - ç®¡ç†åˆ†å±‚å†³ç­–æµç¨‹
    - å¤„ç†åˆ†å±‚ç»éªŒå›æ”¾
    - æ”¯æŒDAggerï¼ˆä¸“å®¶æŒ‡å¯¼ï¼‰

    æ¶æ„ï¼š
    High-Level: é€‰æ‹©ç›®æ ‡èŠ‚ç‚¹ï¼ˆsubgoal selectionï¼‰
    Low-Level: æ‰§è¡Œè·¯å¾„è§„åˆ’ï¼ˆgoal-conditionedï¼‰
    """

    def __init__(self, config, encoder=None, phase=3, goal_strategy='adaptive', **kwargs):
        self.config = config
        self.phase = int(phase)
        self.goal_strategy = goal_strategy

        # è®¾å¤‡é…ç½®
        use_cuda = config.get('use_cuda', False)
        self.device = torch.device('cuda' if torch.cuda.is_available() and use_cuda else 'cpu')

        # ============================================
        # ç¯å¢ƒå‚æ•°
        # ============================================
        env_cfg = config.get('environment', config.get('env', {}))
        self.n_actions = kwargs.get('low_action_dim', env_cfg.get('nb_low_level_actions', 50))
        self.n_goals = kwargs.get('high_action_dim', env_cfg.get('nb_high_level_goals', 10))

        # ============================================
        # HRLå‚æ•°
        # ============================================
        hrl_cfg = config.get('hrl', {})
        self.state_dim = hrl_cfg.get('state_dim', 128)
        self.goal_dim = hrl_cfg.get('goal_dim', 64)
        self.hidden_dim = hrl_cfg.get('hidden_dim', 128)

        self.subgoal_horizon = hrl_cfg.get('subgoal_horizon', 20)
        self.intrinsic_reward_weight = hrl_cfg.get('intrinsic_reward_weight', 0.3)

        # ============================================
        # Encoderï¼ˆGNNç‰¹å¾æå–ï¼‰
        # ============================================
        if encoder is not None:
            self.encoder = encoder.to(self.device)
            self.encoder.eval()
            for param in self.encoder.parameters():
                param.requires_grad = False
            logger.info("âœ… ä½¿ç”¨é¢„è®­ç»ƒEncoder")
        else:
            self.encoder = None
            logger.info("âš ï¸ æœªæä¾›Encoderï¼Œå°†åœ¨è¿è¡Œæ—¶åˆ›å»º")

        # ============================================
        # High-Level Policy
        # ============================================
        from core.hrl.high_policy import HighLevelPolicy

        high_config = {
            'use_cuda': config.get('use_cuda', False),
            'hidden_dim': self.hidden_dim,
            'goal_dim': self.goal_dim,
            'gnn_output_dim': self.state_dim,
            'environment': {
                'nb_high_level_goals': self.n_goals
            },
            'dropout': config.get('dropout', 0.1)
        }

        self.high_policy = HighLevelPolicy(high_config).to(self.device)

        # ============================================
        # Low-Level Policy
        # ============================================
        from core.hrl.low_policy import GoalConditionedLowLevelPolicy

        low_config = {
            'use_cuda': config.get('use_cuda', False),
            'state_dim': self.state_dim,
            'goal_dim': self.goal_dim,
            'hidden_dim': self.hidden_dim,
            'environment': {
                'nb_low_level_actions': self.n_actions
            },
            'dropout': config.get('dropout', 0.1)
        }

        self.low_policy = GoalConditionedLowLevelPolicy(low_config).to(self.device)

        # ============================================
        # Target Networks
        # ============================================
        self.target_high_policy = HighLevelPolicy(high_config).to(self.device)
        self.target_high_policy.load_state_dict(self.high_policy.state_dict())

        self.target_low_policy = GoalConditionedLowLevelPolicy(low_config).to(self.device)
        self.target_low_policy.load_state_dict(self.low_policy.state_dict())

        # ============================================
        # Optimizers
        # ============================================
        training_cfg = config.get('training', {})

        lr_high = training_cfg.get('lr_high', training_cfg.get('learning_rate', 1e-4))
        lr_low = training_cfg.get('lr_low', training_cfg.get('learning_rate', 1e-4))

        self.optimizer_high = optim.Adam(self.high_policy.parameters(), lr=lr_high)
        self.optimizer_low = optim.Adam(self.low_policy.parameters(), lr=lr_low)

        # ============================================
        # è®­ç»ƒå‚æ•°
        # ============================================
        self.batch_size = int(training_cfg.get('batch_size', 32))
        self.gamma = float(training_cfg.get('gamma', 0.99))
        self.target_update_freq = int(training_cfg.get('target_update_freq', 1000))

        # Epsiloné…ç½®
        epsilon_cfg = training_cfg.get('epsilon', {})

        self.epsilon_high_start = float(epsilon_cfg.get('initial_high', epsilon_cfg.get('initial', 0.3)))
        self.epsilon_high_end = float(epsilon_cfg.get('final_high', epsilon_cfg.get('final', 0.05)))
        self.epsilon_high = self.epsilon_high_start

        self.epsilon_low_start = float(epsilon_cfg.get('initial_low', epsilon_cfg.get('initial', 0.3)))
        self.epsilon_low_end = float(epsilon_cfg.get('final_low', epsilon_cfg.get('final', 0.05)))
        self.epsilon_low = self.epsilon_low_start

        self.epsilon_decay = float(epsilon_cfg.get('decay_steps', 50000))

        # ============================================
        # ç»éªŒå›æ”¾
        # ============================================
        buffer_size = int(training_cfg.get('buffer_size', 50000))

        self.high_memory = deque(maxlen=buffer_size // 10)
        self.low_memory = deque(maxlen=buffer_size)

        # ============================================
        # ============================================
        # çŠ¶æ€ç®¡ç†
        # ============================================
        self.current_subgoal = None  # æ•´æ•°ï¼ˆèŠ‚ç‚¹IDï¼‰
        self.current_subgoal_emb = None  # Tensorå½¢å¼çš„subgoal embedding
        self.current_goal_emb = None  # Goal embedding
        self.subgoal_steps = 0

        # å‘åå…¼å®¹ï¼šæ—§ä»£ç å¯èƒ½ä½¿ç”¨ subgoal_step_count
        self.subgoal_step_count = 0

        self.steps_done = 0
        self.update_count = 0

        self._training = True

        # ============================================
        # å‘åå…¼å®¹ï¼šæ·»åŠ  goal_embedding
        # ============================================
        from core.hrl.goal_embedding import (
            AdaptiveSubgoalEmbedding,
            EnhancedRelativeGoalEmbedding,
            IterativeHybridGoalEmbedding
        )

        if self.goal_strategy == 'adaptive':
            self.goal_embedding = AdaptiveSubgoalEmbedding(
                state_dim=self.state_dim,
                goal_dim=self.goal_dim
            ).to(self.device)
        elif self.goal_strategy == 'hybrid':
            self.goal_embedding = IterativeHybridGoalEmbedding(
                local_state_dim=self.state_dim,
                goal_dim=self.goal_dim
            ).to(self.device)
        else:  # 'relative'
            self.goal_embedding = EnhancedRelativeGoalEmbedding(
                node_feat_dim=self.state_dim,
                goal_dim=self.goal_dim
            ).to(self.device)
        # âœ… è‡ªé€‚åº”epsilon
        self.adaptive_epsilon = config.get('hrl', {}).get('adaptive_epsilon', True)
        self.min_epsilon_low = 0.01

        # ğŸ”¥ æ–°å¢ï¼šè®­ç»ƒç¨³å®šæ€§å‚æ•°
        self.clip_grad_norm = training_cfg.get('clip_grad_norm', 1.0)
        self.tau = training_cfg.get('tau', 0.005)  # è½¯æ›´æ–°ç³»æ•°
        self.huber_delta = training_cfg.get('huber_delta', 1.0)  # Huber loss delta

        # ğŸ”¥ æ–°å¢ï¼šæŸå¤±ç»Ÿè®¡
        self.high_loss_history = deque(maxlen=100)
        self.low_loss_history = deque(maxlen=100)
        self.gradient_norms = deque(maxlen=100)

        # ğŸ”¥ æ–°å¢ï¼šé»‘åå•å­¦ä¹ ç›¸å…³
        self.failed_nodes_counter = {}  # è®°å½•èŠ‚ç‚¹å¤±è´¥æ¬¡æ•°
        self.blacklist_history = []  # é»‘åå•å†å²è®°å½•

    def select_action(
            self,
            state: Dict,
            unconnected_dests: Optional[list] = None,
            action_mask: Optional[np.ndarray] = None,
            use_expert: bool = False,
            expert_action: Optional[int] = None,
            blacklist_info: Optional[dict] = None  # âœ… æ–°å¢å‚æ•°
    ) -> Tuple[int, int, Dict]:
        """
        åˆ†å±‚åŠ¨ä½œé€‰æ‹©ï¼ˆé»‘åå•æ„ŸçŸ¥ï¼‰

        Args:
            state: ç¯å¢ƒçŠ¶æ€
            unconnected_dests: æœªè¿æ¥çš„ç›®çš„èŠ‚ç‚¹åˆ—è¡¨
            action_mask: Low-LevelåŠ¨ä½œmask
            use_expert: æ˜¯å¦ä½¿ç”¨ä¸“å®¶
            expert_action: ä¸“å®¶åŠ¨ä½œ
            blacklist_info: é»‘åå•ä¿¡æ¯ï¼ˆæ–°å¢ï¼‰

        Returns:
            high_action: High-LevelåŠ¨ä½œï¼ˆç›®æ ‡ç´¢å¼•ï¼‰
            low_action: Low-LevelåŠ¨ä½œï¼ˆä¸‹ä¸€è·³èŠ‚ç‚¹ï¼‰
            info: ä¿¡æ¯å­—å…¸
        """
        info = {
            'high_level_decision': False,
            'subgoal': self.current_subgoal,
            'subgoal_steps': self.subgoal_steps,
            'source': 'agent',
            'blacklist_total': 0
        }

        try:
            # âœ… æ ¹æ®é»‘åå•è°ƒæ•´epsilon
            if self.adaptive_epsilon and blacklist_info:
                blacklist_count = blacklist_info.get('total', 0)
                blacklist_ratio = blacklist_count / self.n_actions if self.n_actions > 0 else 0

                # é»‘åå•å¤š â†’ é™ä½æ¢ç´¢ï¼ˆé¿å…è¸©å‘ï¼‰
                adaptive_factor = 1.0 - blacklist_ratio * 0.3
                self.epsilon_low = max(
                    self.min_epsilon_low,
                    self.epsilon_low_start * adaptive_factor
                )

                info['adaptive_epsilon'] = self.epsilon_low
                info['blacklist_ratio'] = blacklist_ratio

            # è®°å½•é»‘åå•ä¿¡æ¯
            if blacklist_info:
                info['blacklist_total'] = blacklist_info.get('total', 0)
                info['blacklist_nodes'] = blacklist_info.get('nodes', [])

            # ============================================
            # 1. åˆ¤æ–­æ˜¯å¦éœ€è¦æ–°çš„subgoal
            # ============================================
            need_new_subgoal = self._need_new_subgoal(state, unconnected_dests)

            if need_new_subgoal:
                # ============================================
                # High-Level Decision
                # ============================================
                if use_expert and expert_action is not None:
                    # âœ… ä¸“å®¶å»ºè®®ä¹Ÿè¦æ£€æŸ¥Mask
                    if action_mask is None or (
                            0 <= expert_action < len(action_mask) and action_mask[expert_action] > 0):
                        self.current_subgoal = self._extract_subgoal_from_expert(
                            expert_action, unconnected_dests
                        )
                        info['source'] = 'expert_high'
                    else:
                        # ä¸“å®¶å»ºè®®è¢«Maskï¼Œç”¨Agent
                        logger.warning(f"âš ï¸ ä¸“å®¶Highå»ºè®®{expert_action}è¢«Maskï¼Œæ”¹ç”¨Agent")
                        self.current_subgoal = self._select_subgoal(state, unconnected_dests)
                        info['source'] = 'agent_high_fallback'
                else:
                    # Agentæ¨¡å¼ï¼šä½¿ç”¨High-Levelç­–ç•¥
                    self.current_subgoal = self._select_subgoal(state, unconnected_dests)
                    info['source'] = 'agent_high'

                # ç”Ÿæˆgoal embedding
                self._generate_goal_embedding(state)

                self.subgoal_steps = 0
                info['high_level_decision'] = True
                info['subgoal'] = self.current_subgoal

                logger.debug(f"ğŸ¯ æ–°å­ç›®æ ‡: {self.current_subgoal}")

            # ============================================
            # 2. Low-Level Executionï¼ˆé»‘åå•æ„ŸçŸ¥ï¼‰
            # ============================================
            if use_expert and expert_action is not None and not need_new_subgoal:
                # âœ… ä¸“å®¶LowåŠ¨ä½œä¹Ÿè¦æ£€æŸ¥Mask
                if action_mask is None or (0 <= expert_action < len(action_mask) and action_mask[expert_action] > 0):
                    low_action = expert_action
                    info['source'] = 'expert_low'
                else:
                    logger.warning(f"âš ï¸ ä¸“å®¶LowåŠ¨ä½œ{expert_action}è¢«Maskï¼Œæ”¹ç”¨Agent")
                    low_action = self._select_low_action_with_blacklist(
                        state, action_mask, blacklist_info
                    )
                    info['source'] = 'agent_low_fallback'
            else:
                # Agentæ¨¡å¼ï¼šä½¿ç”¨Low-Levelç­–ç•¥ï¼ˆé»‘åå•æ„ŸçŸ¥ï¼‰
                low_action = self._select_low_action_with_blacklist(
                    state, action_mask, blacklist_info
                )
                info['source'] = 'agent_low'

            # è®°å½•é€‰æ‹©çš„åŠ¨ä½œ
            info['low_action'] = low_action
            if action_mask is not None:
                info['action_mask_sum'] = action_mask.sum()

            # å¦‚æœæ˜¯é»‘åå•ä¸­çš„èŠ‚ç‚¹ï¼Œè®°å½•è­¦å‘Š
            if blacklist_info and low_action in blacklist_info.get('nodes', []):
                info['blacklisted_action'] = True
                logger.warning(f"âš ï¸ Agenté€‰æ‹©äº†é»‘åå•ä¸­çš„èŠ‚ç‚¹ {low_action}")

            self.subgoal_steps += 1
            self.subgoal_step_count = self.subgoal_steps  # å‘åå…¼å®¹
            self.steps_done += 1

            # High actionï¼ˆç›®æ ‡åœ¨unconnectedä¸­çš„ç´¢å¼•ï¼‰
            high_action = 0
            if unconnected_dests and self.current_subgoal is not None:
                if self.current_subgoal in unconnected_dests:
                    high_action = unconnected_dests.index(self.current_subgoal)

            return high_action, low_action, info

        except Exception as e:
            logger.error(f"[Select Action] Error: {e}")
            import traceback
            traceback.print_exc()

            # Fallbackï¼šä½¿ç”¨åŠ¨ä½œæ©ç é€‰æ‹©æœ‰æ•ˆåŠ¨ä½œ
            if action_mask is not None:
                valid_actions = np.where(action_mask > 0)[0]
                if len(valid_actions) > 0:
                    low_action = random.choice(valid_actions)
                else:
                    low_action = random.randint(0, self.n_actions - 1)
            else:
                low_action = random.randint(0, self.n_actions - 1)

            return 0, low_action, info

    def _need_new_subgoal(self, state: Dict, unconnected_dests: Optional[list]) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦æ–°çš„subgoal"""
        # 1. æ²¡æœ‰subgoal
        if self.current_subgoal is None:
            return True

        # 2. æ²¡æœ‰æœªè¿æ¥èŠ‚ç‚¹
        if not unconnected_dests or len(unconnected_dests) == 0:
            return False

        # 3. Subgoalå·²è¿æ¥
        if self.current_subgoal not in unconnected_dests:
            return True

        # 4. Subgoalè¶…æ—¶
        if self.subgoal_steps >= self.subgoal_horizon:
            logger.debug(f"âš ï¸ Subgoalè¶…æ—¶ (steps={self.subgoal_steps})")
            return True

        # 5. å·²åˆ°è¾¾subgoal
        current_pos = state.get('current_position', -1)
        if current_pos == self.current_subgoal:
            return True

        return False

    def _select_subgoal(self, state: Dict, unconnected_dests: list) -> int:
        """High-Levelç­–ç•¥é€‰æ‹©subgoal"""
        if not unconnected_dests or len(unconnected_dests) == 0:
            return None

        # è·å–å›¾åµŒå…¥
        graph_emb = self._get_graph_embedding(state)

        # åˆ›å»ºmaskï¼ˆåªå…è®¸é€‰æ‹©æœªè¿æ¥èŠ‚ç‚¹ï¼‰
        valid_goals = torch.zeros(1, self.n_goals, device=self.device)
        for i, dest in enumerate(unconnected_dests):
            if i < self.n_goals:
                valid_goals[0, i] = 1

        # High-Levelç­–ç•¥é€‰æ‹©
        with torch.no_grad():
            goal_idx, goal_emb = self.high_policy.select_goal(
                graph_emb,
                valid_goals,
                epsilon=self.epsilon_high
            )

        # ä¿å­˜goal embedding
        self.current_goal_emb = goal_emb

        # æ˜ å°„å›å®é™…èŠ‚ç‚¹
        goal_idx = goal_idx.item()
        if goal_idx < len(unconnected_dests):
            subgoal = unconnected_dests[goal_idx]
        else:
            subgoal = unconnected_dests[0]

        return subgoal

    def _generate_goal_embedding(self, state: Dict):
        """ç”Ÿæˆgoal embedding"""
        try:
            graph_emb = self._get_graph_embedding(state)

            with torch.no_grad():
                _, goal_emb, _ = self.high_policy(graph_emb, return_subgoal=True)

            self.current_goal_emb = goal_emb

        except Exception as e:
            logger.error(f"[Goal Embedding] Error: {e}")
            self.current_goal_emb = torch.zeros(1, self.goal_dim, device=self.device)

    def _select_low_action(self, state: Dict, action_mask: Optional[np.ndarray]) -> int:
        """Low-Levelç­–ç•¥é€‰æ‹©åŠ¨ä½œ"""
        # è·å–å›¾åµŒå…¥
        graph_emb = self._get_graph_embedding(state)

        # Goal embedding
        if self.current_goal_emb is None:
            self._generate_goal_embedding(state)

        # è½¬æ¢mask
        if action_mask is not None:
            mask_tensor = torch.FloatTensor(action_mask).unsqueeze(0).to(self.device)
        else:
            mask_tensor = None

        # Low-Levelç­–ç•¥é€‰æ‹©
        with torch.no_grad():
            action, _ = self.low_policy.select_action(
                graph_emb,
                self.current_goal_emb,
                mask_tensor,
                epsilon=self.epsilon_low
            )

        return action.item()

    def _get_graph_embedding(self, state):
        """
        ğŸ”¥ [æ ¸å¿ƒä¿®å¤] è·å–å›¾åµŒå…¥ï¼Œæ”¯æŒ Batch å¤„ç†
        """
        # 1. å°è¯•ä½¿ç”¨çœŸå®çš„ Encoder (å¦‚æœæœ‰)
        if self.encoder is not None:
            try:
                # æƒ…å†µ A: PyG Batch å¯¹è±¡ (è®­ç»ƒæ—¶)
                if hasattr(state, 'batch') and state.batch is not None:
                    return self.encoder(state.x, state.edge_index, state.batch)

                # æƒ…å†µ B: å•ä¸ª Data å¯¹è±¡ (æ¨ç†æ—¶)
                # æ„é€ ä¸€ä¸ªå…¨0çš„ batch å‘é‡
                batch = torch.zeros(state.x.size(0), dtype=torch.long, device=self.device)
                return self.encoder(state.x, state.edge_index, batch)

            except Exception as e:
                logger.error(f"Encoder forward failed: {e}")
                # å¦‚æœå‡ºé”™ï¼Œå‘ä¸‹æ‰§è¡Œ Fallback

        # 2. Fallback (ä»…ç”¨äºé˜²æ­¢å´©æºƒï¼Œè¾“å‡ºéšæœºå™ªå£°)
        # å¿…é¡»è¿”å›æ­£ç¡®çš„ Batch Sizeï¼Œå¦åˆ™ Loss è®¡ç®—ä¼šæŠ¥é”™
        batch_size = 1
        if hasattr(state, 'num_graphs'):
            # PyG Batch å¯¹è±¡åŒ…å« num_graphs å±æ€§
            batch_size = state.num_graphs
        elif hasattr(state, 'batch') and state.batch is not None:
            batch_size = state.batch.max().item() + 1

        # logger.warning(f"Using random embedding fallback (Batch Size: {batch_size})")
        return torch.randn(batch_size, self.state_dim, device=self.device)

    def _extract_subgoal_from_expert(self, expert_action: int, unconnected_dests: list) -> int:
        """ä»ä¸“å®¶åŠ¨ä½œæå–subgoal"""
        if unconnected_dests and expert_action in unconnected_dests:
            return expert_action

        if unconnected_dests and len(unconnected_dests) > 0:
            return unconnected_dests[0]

        return None

    # ============================================
    # å‘åå…¼å®¹æ–¹æ³•
    # ============================================

    def _generate_and_encode_subgoal(self, state: Dict):
        """
        å‘åå…¼å®¹ï¼šç”Ÿæˆå¹¶ç¼–ç subgoal

        è¿™æ˜¯æ—§ç‰ˆæœ¬çš„æ–¹æ³•åï¼Œè°ƒç”¨æ–°çš„ _generate_goal_embedding
        åŒæ—¶ä½¿ç”¨ goal_embedding æ¨¡å—ç”Ÿæˆæ›´å¥½çš„åµŒå…¥

        Args:
            state: ç¯å¢ƒçŠ¶æ€

        æ³¨æ„:
        - current_subgoal åº”è¯¥æ˜¯æ•´æ•°ï¼ˆèŠ‚ç‚¹IDï¼‰ï¼Œç”± _select_subgoal è®¾ç½®
        - current_subgoal_emb æ˜¯tensorå½¢å¼çš„embedding
        - current_goal_emb æ˜¯goal embedding
        """
        try:
            # è·å–å›¾åµŒå…¥
            graph_emb = self._get_graph_embedding(state)

            # ä½¿ç”¨ goal_embedding ç”Ÿæˆsubgoal embedding
            with torch.no_grad():
                if self.goal_strategy == 'adaptive':
                    complexity = torch.tensor([[0.5]], device=self.device)
                    subgoal_emb, info = self.goal_embedding(graph_emb, complexity)
                    # AdaptiveSubgoalEmbedding è¿”å› (subgoal, info)
                    # éœ€è¦æ‰‹åŠ¨ç”Ÿæˆ goal_emb
                    if subgoal_emb.shape[-1] >= self.goal_dim:
                        goal_emb = subgoal_emb[..., :self.goal_dim]
                    else:
                        # å¦‚æœ subgoal ç»´åº¦å°äº goal_dimï¼Œå¡«å……
                        padding = torch.zeros(
                            subgoal_emb.size(0),
                            self.goal_dim - subgoal_emb.size(-1),
                            device=self.device
                        )
                        goal_emb = torch.cat([subgoal_emb, padding], dim=-1)

                elif self.goal_strategy == 'hybrid':
                    subgoal_emb, goal_emb, _ = self.goal_embedding(graph_emb)

                else:  # 'relative'
                    target_emb = torch.randn_like(graph_emb)  # ä¸´æ—¶ç›®æ ‡
                    goal_emb, info = self.goal_embedding(graph_emb, target_emb)
                    # EnhancedRelativeGoalEmbedding è¿”å› (goal_emb, info)
                    # ä½¿ç”¨ goal_emb ä½œä¸º subgoal_emb
                    subgoal_emb = goal_emb

            # ç¡®ä¿ç»´åº¦æ­£ç¡®
            if subgoal_emb.shape[-1] != self.goal_dim:
                subgoal_emb = subgoal_emb[..., :self.goal_dim]
            if goal_emb.shape[-1] != self.goal_dim:
                goal_emb = goal_emb[..., :self.goal_dim]

            # è®¾ç½®embeddings
            # æ³¨æ„ï¼šcurrent_subgoal æ˜¯æ•´æ•°ï¼ˆèŠ‚ç‚¹IDï¼‰ï¼Œç”± _select_subgoal è®¾ç½®
            # current_subgoal_emb æ˜¯tensorå½¢å¼çš„embedding
            self.current_subgoal_emb = subgoal_emb  # tensor
            self.current_goal_emb = goal_emb
            self.subgoal_step_count = 0

        except Exception as e:
            logger.error(f"[Generate Subgoal] Error: {e}")
            # Fallback
            self.current_subgoal_emb = torch.zeros(1, self.goal_dim, device=self.device)
            self.current_goal_emb = torch.zeros(1, self.goal_dim, device=self.device)
            self.subgoal_step_count = 0

    def store_transition_high(
            self, state: Dict, goal: int, reward: float, next_state: Dict, done: bool
    ):
        """å­˜å‚¨High-Levelç»éªŒ (ä¿®å¤ç‰ˆ: é˜²æ­¢ç´¢å¼•è¶Šç•Œ)"""

        # ğŸ”¥ [å…³é”®ä¿®å¤] ç¡®ä¿ goal ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
        # High-Level Qç½‘ç»œåªæœ‰ n_goals ä¸ªè¾“å‡º (ä¾‹å¦‚10)
        # å¦‚æœ goal æ˜¯ç‰©ç†èŠ‚ç‚¹ID (ä¾‹å¦‚24)ï¼Œä¼šå¯¼è‡´ç´¢å¼•è¶Šç•Œ
        goal_idx = goal
        if isinstance(goal, (int, np.integer)):
            if goal >= self.n_goals:
                # ä½¿ç”¨æ¨¡è¿ç®—æ˜ å°„åˆ°æœ‰æ•ˆèŒƒå›´ [0, n_goals-1]
                # è¿™æ˜¯ä¸€ä¸ªå…œåº•ç­–ç•¥ï¼Œé˜²æ­¢ç¨‹åºå´©æºƒ
                goal_idx = goal % self.n_goals
                # logger.debug(f"âš ï¸ Goalæ˜ å°„: {goal} -> {goal_idx} (Max: {self.n_goals})")
            elif goal < 0:
                goal_idx = 0  # é»˜è®¤å€¼

        self.high_memory.append({
            'state': state,
            'goal': goal_idx,  # âœ… å­˜å‚¨ä¿®æ­£åçš„ç´¢å¼•
            'reward': reward,
            'next_state': next_state,
            'done': done
        })

    def store_transition_low(
            self, state: Dict, action: int, reward: float, next_state: Dict, done: bool
    ):
        """å­˜å‚¨Low-Levelç»éªŒ (é›†æˆå¼ºåŠ›ç¼©æ”¾)"""
        # ğŸ”¥ [å…³é”®ä¿®å¤] å¼ºåŠ›å¥–åŠ±ç¼©æ”¾ + æˆªæ–­
        # åŸå§‹å¥–åŠ±å¯èƒ½é«˜è¾¾ 2500+ï¼Œæˆ‘ä»¬å°†å…¶å‹ç¼©åˆ° [-5, 10] åŒºé—´
        scaled_reward = reward * 0.01  # ç¼©å°100å€ (2500 -> 25.0)

        # ç¡¬æˆªæ–­ï¼Œé˜²æ­¢æç«¯å€¼ç ´å Q å€¼ä¼°è®¡
        max_reward = 10.0
        min_reward = -5.0
        scaled_reward = max(min_reward, min(max_reward, scaled_reward))

        # ğŸ”¥ [å¯é€‰] æ·»åŠ  Intrinsic Reward (å¥½å¥‡å¿ƒå¥–åŠ±)
        if self.config.get('hrl', {}).get('use_intrinsic_reward', False):
            try:
                # ç®€å•çš„çŠ¶æ€å·®å¼‚ä½œä¸ºå¥½å¥‡å¿ƒ
                with torch.no_grad():
                    state_emb = self._extract_state_embedding(state)
                    next_state_emb = self._extract_state_embedding(next_state)
                    prediction_error = F.mse_loss(state_emb, next_state_emb).item()
                    # é™åˆ¶å†…åœ¨å¥–åŠ±å¹…åº¦
                    intrinsic_reward = min(0.1, prediction_error * 0.5)
                    scaled_reward += intrinsic_reward
            except Exception:
                pass

        self.low_memory.append({
            'state': state,
            'action': action,
            'reward': scaled_reward,  # ä½¿ç”¨ç¼©æ”¾åçš„å¥–åŠ±
            'next_state': next_state,
            'done': done,
            'goal_emb': self.current_goal_emb
        })

        # ç¼“å†²åŒºç›‘æ§ (è°ƒè¯•ç”¨)
        if len(self.low_memory) % 5000 == 0:
            logger.info(f"ğŸ“Š Low Buffer Size: {len(self.low_memory)}")

    # ============================================
    # å‘åå…¼å®¹ï¼šä¿ç•™æ—§æ¥å£
    # ============================================

    def store_transition(self, state, action, reward, next_state, done, goal=None, next_valid_actions=None):
        """å‘åå…¼å®¹çš„store_transition"""
        # åˆ†å‘åˆ°å¯¹åº”çš„å­˜å‚¨å‡½æ•°
        if isinstance(action, (list, tuple)) and len(action) == 2:
            high_action, low_action = action
            # åˆ†åˆ«å­˜å‚¨
            if goal is not None:
                self.store_transition_high(state, goal, reward, next_state, done)
            self.store_transition_low(state, low_action, reward, next_state, done)
        else:
            # é»˜è®¤å­˜å‚¨åˆ°Low-Level
            self.store_transition_low(state, action, reward, next_state, done)

    def update(self) -> float:
        """å‘åå…¼å®¹çš„updateæ¥å£"""
        losses = self.update_policies()

        # è¿”å›æ€»loss
        total_loss = losses.get('high_loss', 0.0) + losses.get('low_loss', 0.0)
        return total_loss

    def update_policies(self) -> Dict[str, float]:
        """æ›´æ–°ç­–ç•¥ï¼ˆé›†æˆç›‘æ§ä¸è°ƒåº¦ï¼‰"""
        losses = {}
        self.update_count += 1

        # High-Levelæ›´æ–°
        high_loss = 0.0
        if len(self.high_memory) >= self.batch_size // 4:
            high_loss = self._update_high_level()
            losses['high_loss'] = high_loss
            if high_loss > 0:
                self.high_loss_history.append(high_loss)

        # Low-Levelæ›´æ–°
        low_loss = 0.0
        if len(self.low_memory) >= self.batch_size:
            low_loss = self._update_low_level()
            losses['low_loss'] = low_loss
            if low_loss > 0:
                self.low_loss_history.append(low_loss)

        losses['total_loss'] = losses.get('high_loss', 0) + low_loss

        # ğŸ”¥ è½¯æ›´æ–° target networks (æ›´ç¨³å®š)
        self._soft_update_target_networks()

        # å®šæœŸç¡¬æ›´æ–° target networks
        if self.update_count % self.target_update_freq == 0:
            self._hard_update_target_networks()

        # æ›´æ–° epsilon
        self._update_epsilon()

        # ğŸ”¥ ç›‘æ§è®­ç»ƒçŠ¶æ€
        if self.update_count % 100 == 0:
            self._log_training_stats()

        return losses

    def _update_high_level(self) -> float:
        """æ›´æ–°High-Levelç­–ç•¥ (Double DQN)"""
        # 1. æ£€æŸ¥æ ·æœ¬æ•°é‡
        if len(self.high_memory) < self.batch_size:
            return 0.0

        try:
            # 2. é‡‡æ ·
            batch = random.sample(self.high_memory, self.batch_size)

            # 3. å‡†å¤‡æ•°æ®
            # æå– Graph Embedding
            state_embs = [self._get_graph_embedding(x['state']) for x in batch]
            next_state_embs = [self._get_graph_embedding(x['next_state']) for x in batch]

            # å †å å¹¶ç¡®ä¿åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            state_tensor = torch.cat(state_embs, dim=0).to(self.device)
            next_state_tensor = torch.cat(next_state_embs, dim=0).to(self.device)

            goals = torch.tensor([x['goal'] for x in batch], device=self.device).long().unsqueeze(1)
            rewards = torch.tensor([x['reward'] for x in batch], device=self.device).float().unsqueeze(1)
            dones = torch.tensor([x['done'] for x in batch], device=self.device).float().unsqueeze(1)

            # 4. è®¡ç®— Current Q
            # ğŸš€ ä¼˜åŒ–ï¼šä¼ å…¥ return_subgoal=Falseï¼Œåªè®¡ç®— Q å€¼ï¼Œä¸ç”Ÿæˆ Subgoalï¼ŒèŠ‚çœç®—åŠ›
            # HighPolicy forward è¿”å›: (q_values, subgoal_emb, value)
            curr_q_values, _, _ = self.high_policy(state_tensor, return_subgoal=False)
            curr_q = curr_q_values.gather(1, goals)

            # 5. è®¡ç®— Target Q (Double DQN)
            with torch.no_grad():
                # Online Net é€‰åŠ¨ä½œ
                next_q_online, _, _ = self.high_policy(next_state_tensor, return_subgoal=False)
                next_actions = next_q_online.argmax(dim=1, keepdim=True)

                # Target Net è¯„ä»·å€¼
                next_q_target, _, _ = self.target_high_policy(next_state_tensor, return_subgoal=False)
                next_q = next_q_target.gather(1, next_actions)

                target_q = rewards + (1 - dones) * self.gamma * next_q

                # ğŸ”¥ é™åˆ¶ç›®æ ‡Qå€¼èŒƒå›´
                target_q = torch.clamp(target_q, -10.0, 50.0)

            # 6. è®¡ç®— Loss & æ›´æ–°
            # ğŸ”¥ ä½¿ç”¨Huber Lossæé«˜ç¨³å®šæ€§
            loss = F.smooth_l1_loss(curr_q, target_q, reduction='mean')

            # æ£€æŸ¥ Loss æœ‰æ•ˆæ€§
            if torch.isnan(loss) or torch.isinf(loss):
                logger.warning("âŒ High-Level Loss å‡ºç°NaN/Infï¼Œè·³è¿‡æ›´æ–°")
                return 0.0

            self.optimizer_high.zero_grad()
            loss.backward()

            # ğŸ”¥ æ¢¯åº¦ç›‘æ§
            total_grad_norm = 0.0
            for param in self.high_policy.parameters():
                if param.grad is not None:
                    total_grad_norm += param.grad.norm().item()
            self.gradient_norms.append(total_grad_norm)

            # æ¢¯åº¦è£å‰ª
            nn.utils.clip_grad_norm_(self.high_policy.parameters(), self.clip_grad_norm)
            self.optimizer_high.step()

            return loss.item()

        except Exception as e:
            logger.error(f"[Update High Level] Error: {e}")
            import traceback
            traceback.print_exc()
            return 0.0

    def _update_low_level(self) -> float:
        """æ›´æ–°Low-Levelç­–ç•¥ (é›†æˆ Qå€¼æˆªæ–­ã€æ¢¯åº¦è£å‰ªã€è‡ªé€‚åº”LR)"""
        if len(self.low_memory) < self.batch_size:
            return 0.0

        try:
            # 2. é‡‡æ ·
            batch = random.sample(self.low_memory, self.batch_size)

            # 3. å‡†å¤‡æ•°æ®
            # ä½¿ç”¨æ–°æ·»åŠ çš„ _extract_state_embedding æ–¹æ³•
            state_embs = [self._extract_state_embedding(x['state']) for x in batch]
            next_state_embs = [self._extract_state_embedding(x['next_state']) for x in batch]

            state_tensor = torch.cat(state_embs, dim=0).to(self.device)
            next_state_tensor = torch.cat(next_state_embs, dim=0).to(self.device)

            actions = torch.tensor([x['action'] for x in batch], device=self.device).long().unsqueeze(1)
            rewards = torch.tensor([x['reward'] for x in batch], device=self.device).float().unsqueeze(1)
            dones = torch.tensor([x['done'] for x in batch], device=self.device).float().unsqueeze(1)

            # ğŸ”¥ [ä¿®å¤] è¿‡æ»¤æ— æ•ˆåŠ¨ä½œ (-1)
            valid_mask = (actions >= 0).squeeze()
            if valid_mask.sum() == 0: return 0.0

            state_tensor = state_tensor[valid_mask]
            next_state_tensor = next_state_tensor[valid_mask]
            actions = actions[valid_mask]
            rewards = rewards[valid_mask]
            dones = dones[valid_mask]

            # ğŸ”¥ [ä¿®å¤] Reward äºŒæ¬¡æ£€æŸ¥ (é˜²æ­¢å­˜å…¥åçš„å¼‚å¸¸å€¼)
            rewards = torch.clamp(rewards, -10.0, 10.0)

            # å¤„ç† Goal Embedding
            goal_embs = []
            valid_indices = torch.nonzero(valid_mask).squeeze().cpu().tolist()
            if not isinstance(valid_indices, list): valid_indices = [valid_indices]

            for idx in valid_indices:
                x = batch[idx]
                g = x.get('goal_emb')
                if g is None:
                    g = torch.zeros(1, self.goal_dim, device=self.device)
                else:
                    g = g.to(self.device)
                    if g.dim() == 1: g = g.unsqueeze(0)
                    if g.size(1) != self.goal_dim:
                        if g.size(1) > self.goal_dim:
                            g = g[:, :self.goal_dim]
                        else:
                            padding = torch.zeros(g.size(0), self.goal_dim - g.size(1), device=self.device)
                            g = torch.cat([g, padding], dim=1)
                goal_embs.append(g)

            goal_tensor = torch.cat(goal_embs, dim=0).to(self.device)

            # 4. è®¡ç®— Current Q
            policy_output = self.low_policy(state_tensor, goal_tensor)
            if isinstance(policy_output, tuple):
                curr_q_values = policy_output[0]
            else:
                curr_q_values = policy_output

            curr_q = curr_q_values.gather(1, actions)

            # ğŸ”¥ [ç›‘æ§] Q å€¼å¼‚å¸¸æ£€æµ‹
            if torch.isnan(curr_q).any() or torch.isinf(curr_q).any():
                logger.error("âŒ Q å€¼å‡ºç° NaN/Infï¼Œè§¦å‘é‡ç½®ï¼")
                self.reset_network_parameters()
                return 0.0

            # 5. è®¡ç®— Target Q (Double DQN)
            with torch.no_grad():
                # Online Net é€‰åŠ¨ä½œ
                next_output = self.low_policy(next_state_tensor, goal_tensor)
                next_q_online = next_output[0] if isinstance(next_output, tuple) else next_output
                next_actions = next_q_online.argmax(dim=1, keepdim=True)

                # Target Net è¯„ä»·å€¼
                target_output = self.target_low_policy(next_state_tensor, goal_tensor)
                next_q_target = target_output[0] if isinstance(target_output, tuple) else target_output
                next_q = next_q_target.gather(1, next_actions)

                # ğŸ”¥ [å…³é”®ä¿®å¤] Target Q æˆªæ–­
                # ç†è®ºæœ€å¤§ Q â‰ˆ 10 / (1-0.99) = 1000
                # è¿™é‡Œé™åˆ¶åœ¨ [-20, 100] é˜²æ­¢è¿‡ä¼°è®¡
                next_q = torch.clamp(next_q, -20.0, 100.0)

                target_q = rewards + (1 - dones) * self.gamma * next_q

                # ğŸ”¥ [å…³é”®ä¿®å¤] æœ€ç»ˆ Target äºŒæ¬¡æˆªæ–­
                target_q = torch.clamp(target_q, -20.0, 120.0)

            # 6. è®¡ç®— Loss & æ›´æ–°
            # ğŸ”¥ [ä¿®å¤] ä½¿ç”¨ Huber Loss (Smooth L1 Loss) æé«˜ç¨³å®šæ€§
            loss = F.smooth_l1_loss(curr_q, target_q, reduction='mean')

            # æ£€æŸ¥ Loss æœ‰æ•ˆæ€§
            if torch.isnan(loss) or torch.isinf(loss):
                logger.warning("âŒ Low-Level Loss å‡ºç°NaN/Infï¼Œè·³è¿‡æ›´æ–°")
                return 0.0

            self.optimizer_low.zero_grad()
            loss.backward()

            # ğŸ”¥ [ä¿®å¤] æ¢¯åº¦ç›‘æ§ä¸è£å‰ª
            total_grad_norm = 0.0
            for param in self.low_policy.parameters():
                if param.grad is not None:
                    total_grad_norm += param.grad.norm().item()

            self.gradient_norms.append(total_grad_norm)

            # ä¸¥æ ¼è£å‰ªæ¢¯åº¦
            nn.utils.clip_grad_norm_(self.low_policy.parameters(), self.clip_grad_norm)

            self.optimizer_low.step()

            # ğŸ”¥ [ä¿®å¤] è‡ªé€‚åº”å­¦ä¹ ç‡å¾®è°ƒ (ç®€å•ç‰ˆ)
            loss_val = loss.item()
            if loss_val < 1e-4:  # Loss å¤ªå°ï¼Œå¯èƒ½æ˜¯å­¦ä¹ ç‡å¤ªä½æˆ–é™·å…¥å±€éƒ¨æå°
                for param_group in self.optimizer_low.param_groups:
                    param_group['lr'] = min(param_group['lr'] * 1.02, 1e-3)
            elif loss_val > 5.0:  # Loss å¤ªå¤§
                for param_group in self.optimizer_low.param_groups:
                    param_group['lr'] = max(param_group['lr'] * 0.98, 1e-5)

            return loss_val

        except Exception as e:
            logger.error(f"[Update Low Level] Error: {e}")
            import traceback
            traceback.print_exc()
            return 0.0

    def _soft_update_target_networks(self):
        """è½¯æ›´æ–°target networksï¼ˆæ›´ç¨³å®šï¼‰"""
        # è½¯æ›´æ–°High-Level
        for target_param, param in zip(self.target_high_policy.parameters(), self.high_policy.parameters()):
            target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)

        # è½¯æ›´æ–°Low-Level
        for target_param, param in zip(self.target_low_policy.parameters(), self.low_policy.parameters()):
            target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)

    def _hard_update_target_networks(self):
        """ç¡¬æ›´æ–°target networks"""
        self.target_high_policy.load_state_dict(self.high_policy.state_dict())
        self.target_low_policy.load_state_dict(self.low_policy.state_dict())

    def _update_epsilon(self):
        """æ›´æ–°epsilon"""
        progress = min(self.steps_done / self.epsilon_decay, 1.0)

        self.epsilon_high = self.epsilon_high_start + (
                self.epsilon_high_end - self.epsilon_high_start
        ) * progress

        self.epsilon_low = self.epsilon_low_start + (
                self.epsilon_low_end - self.epsilon_low_start
        ) * progress

    def _log_training_stats(self):
        """è®°å½•è®­ç»ƒç»Ÿè®¡"""
        if len(self.high_loss_history) > 0 and len(self.low_loss_history) > 0:
            avg_high_loss = np.mean(self.high_loss_history)
            avg_low_loss = np.mean(self.low_loss_history)
            avg_grad_norm = np.mean(self.gradient_norms) if self.gradient_norms else 0.0

            logger.debug(f"ğŸ“Š è®­ç»ƒç»Ÿè®¡: HighLoss={avg_high_loss:.4f}, LowLoss={avg_low_loss:.4f}, "
                         f"GradNorm={avg_grad_norm:.2f}, Îµ_low={self.epsilon_low:.3f}")

    def update_epsilon(self):
        """å‘åå…¼å®¹çš„epsilonæ›´æ–°"""
        self._update_epsilon()

    def train(self):
        """è®­ç»ƒæ¨¡å¼"""
        self._training = True
        self.high_policy.train()
        self.low_policy.train()

    def eval(self):
        """è¯„ä¼°æ¨¡å¼"""
        self._training = False
        self.high_policy.eval()
        self.low_policy.eval()

    def save(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'high_policy': self.high_policy.state_dict(),
            'low_policy': self.low_policy.state_dict(),
            'optimizer_high': self.optimizer_high.state_dict(),
            'optimizer_low': self.optimizer_low.state_dict(),
            'epsilon_high': self.epsilon_high,
            'epsilon_low': self.epsilon_low,
            'steps_done': self.steps_done,
            'config': self.config
        }, path)

        logger.info(f"âœ… æ¨¡å‹å·²ä¿å­˜: {path}")

    def load(self, path: str):
        """åŠ è½½æ¨¡å‹"""
        import os
        if not os.path.exists(path):
            logger.warning(f"âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {path}")
            return

        checkpoint = torch.load(path, map_location=self.device)

        if 'high_policy' in checkpoint:
            self.high_policy.load_state_dict(checkpoint['high_policy'])
            self.target_high_policy.load_state_dict(checkpoint['high_policy'])

        if 'low_policy' in checkpoint:
            self.low_policy.load_state_dict(checkpoint['low_policy'])
            self.target_low_policy.load_state_dict(checkpoint['low_policy'])

        if 'optimizer_high' in checkpoint:
            self.optimizer_high.load_state_dict(checkpoint['optimizer_high'])
        if 'optimizer_low' in checkpoint:
            self.optimizer_low.load_state_dict(checkpoint['optimizer_low'])

        if 'epsilon_high' in checkpoint:
            self.epsilon_high = checkpoint['epsilon_high']
        if 'epsilon_low' in checkpoint:
            self.epsilon_low = checkpoint['epsilon_low']
        if 'steps_done' in checkpoint:
            self.steps_done = checkpoint['steps_done']

        logger.info(f"âœ… æ¨¡å‹å·²åŠ è½½: {path}")

    def _select_low_action_with_blacklist(
            self,
            state: Any,
            action_mask: Optional[np.ndarray] = None,
            blacklist_info: Optional[dict] = None
    ) -> int:
        """
        ğŸ”¥ [V2.0 å®Œå…¨ä¿®å¤ç‰ˆ] ä½å±‚åŠ¨ä½œé€‰æ‹©ï¼ˆé»‘åå•æ„ŸçŸ¥ + æ­£ç¡®çš„Maskå¤„ç†ï¼‰

        å…³é”®ä¿®å¤ï¼š
        1. âœ… Qå€¼å±è”½æ³•ï¼ˆä¸æ˜¯æƒé‡èåˆï¼‰
        2. âœ… Maskåœ¨argmaxä¹‹å‰åº”ç”¨
        3. âœ… æ¢ç´¢æ—¶ä»æœ‰æ•ˆåŠ¨ä½œé‡‡æ ·
        """
        try:
            # 1. è·å–çŠ¶æ€åµŒå…¥
            state_emb = self._extract_state_embedding(state)
            if self.current_goal_emb is None:
                self._generate_goal_embedding(state)

            # 2. ğŸ”¥ å‡†å¤‡å®Œæ•´çš„Maskï¼ˆèåˆaction_maskå’Œblacklistï¼‰
            if action_mask is not None and len(action_mask) > 0:
                # ç¡®ä¿maské•¿åº¦æ­£ç¡®
                if len(action_mask) < self.n_actions:
                    # å¦‚æœmaskå¤ªçŸ­ï¼Œå¡«å……0
                    full_mask = np.zeros(self.n_actions, dtype=np.float32)
                    full_mask[:len(action_mask)] = action_mask
                else:
                    full_mask = action_mask[:self.n_actions].copy()
            else:
                # æ²¡æœ‰maskï¼Œé»˜è®¤å…¨éƒ¨å…è®¸
                full_mask = np.ones(self.n_actions, dtype=np.float32)

            # 3. åº”ç”¨é»‘åå•ï¼ˆé™ä½æƒé‡è€Œéå®Œå…¨ç¦æ­¢ï¼‰
            if blacklist_info:
                blacklist_nodes = blacklist_info.get('nodes', [])
                for node in blacklist_nodes:
                    if 0 <= node < self.n_actions:
                        full_mask[node] *= 0.1  # é™ä½åˆ°10%

            # 4. æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆåŠ¨ä½œ
            valid_actions = np.where(full_mask > 0)[0]
            if len(valid_actions) == 0:
                # å®Œå…¨æ²¡æœ‰æœ‰æ•ˆåŠ¨ä½œï¼Œéšæœºè¿”å›
                return random.randint(0, self.n_actions - 1)

            # 5. åŠ¨ä½œé€‰æ‹©ï¼ˆepsilon-greedyï¼‰
            if random.random() < self.epsilon_low:
                # ========== æ¢ç´¢ï¼šåŸºäºmaskæƒé‡çš„æ¦‚ç‡é‡‡æ · ==========
                action_weights = full_mask[valid_actions]
                p = action_weights / action_weights.sum()
                return int(np.random.choice(valid_actions, p=p))

            else:
                # ========== åˆ©ç”¨ï¼šQç½‘ç»œå†³ç­– ==========
                if self.low_policy is not None:
                    with torch.no_grad():
                        # å‰å‘ä¼ æ’­è·å–Qå€¼
                        policy_output = self.low_policy(
                            state_emb,
                            self.current_goal_emb.to(state_emb.device)
                        )

                        # å¤„ç†å¯èƒ½çš„tupleè¿”å›
                        if isinstance(policy_output, tuple):
                            q_values = policy_output[0].cpu().numpy().flatten()
                        else:
                            q_values = policy_output.cpu().numpy().flatten()

                        # ç¡®ä¿Qå€¼é•¿åº¦æ­£ç¡®
                        if len(q_values) < self.n_actions:
                            # Qå€¼å¤ªçŸ­ï¼Œå¡«å……æœ€å°å€¼
                            full_q = np.full(self.n_actions, -1e9, dtype=np.float32)
                            full_q[:len(q_values)] = q_values
                            q_values = full_q
                        elif len(q_values) > self.n_actions:
                            q_values = q_values[:self.n_actions]

                    # ğŸ”¥ğŸ”¥ğŸ”¥ å…³é”®ä¿®å¤ï¼šQå€¼å±è”½æ³•ï¼ˆä¸æ˜¯æƒé‡èåˆï¼‰
                    masked_q_values = q_values.copy()
                    masked_q_values[full_mask == 0] = -1e9  # mask==0çš„ä½ç½®è®¾ä¸ºæå°å€¼

                    # argmaxé€‰æ‹©æœ€å¤§Qå€¼
                    return int(np.argmax(masked_q_values))

                else:
                    # æ²¡æœ‰Qç½‘ç»œï¼ŒåŸºäºmaskæƒé‡é€‰æ‹©
                    action_weights = full_mask[valid_actions]
                    return int(valid_actions[np.argmax(action_weights)])

        except Exception as e:
            logger.error(f"[Select Low Action] Error: {e}")
            import traceback
            traceback.print_exc()
            return random.randint(0, self.n_actions - 1)

    def _prepare_state(self, state):
        """è¾…åŠ©æ–¹æ³•ï¼šå°†çŠ¶æ€è½¬æ¢ä¸ºtensor"""
        if isinstance(state, dict):
            # PyG Data æ ¼å¼
            return state
        elif isinstance(state, np.ndarray):
            return torch.FloatTensor(state).unsqueeze(0)
        else:
            # å·²ç»æ˜¯tensor
            return state

    # ============================================
    # 4. æ–°å¢ record_failure æ–¹æ³•
    # ============================================
    def record_failure(self, node_id: int, reason: str):
        """
        è®°å½•èŠ‚ç‚¹å¤±è´¥ï¼ˆç”¨äºå­¦ä¹ é»‘åå•æ¨¡å¼ï¼‰

        Args:
            node_id: å¤±è´¥çš„èŠ‚ç‚¹ID
            reason: å¤±è´¥åŸå› 
        """
        if node_id not in self.failed_nodes_counter:
            self.failed_nodes_counter[node_id] = {
                'count': 0,
                'reasons': [],
                'last_failed': self.steps_done
            }

        self.failed_nodes_counter[node_id]['count'] += 1
        self.failed_nodes_counter[node_id]['reasons'].append(reason)
        self.failed_nodes_counter[node_id]['last_failed'] = self.steps_done

        self.blacklist_history.append({
            'step': self.steps_done,
            'node': node_id,
            'reason': reason,
            'epsilon': self.epsilon_low
        })

        logger.debug(f"ğŸ“ è®°å½•å¤±è´¥: èŠ‚ç‚¹{node_id}, åŸå› :{reason}")

    # ============================================
    # 5. æ–°å¢ get_blacklist_learning_stats æ–¹æ³•
    # ============================================
    def get_blacklist_learning_stats(self) -> dict:
        """
        è·å–é»‘åå•å­¦ä¹ ç»Ÿè®¡

        Returns:
            åŒ…å«å¤±è´¥ç»Ÿè®¡çš„å­—å…¸
        """
        if not self.failed_nodes_counter:
            return {
                'total_failures': 0,
                'unique_failed_nodes': 0,
                'top_failed_nodes': [],
                'blacklist_history_size': 0
            }

        # æŒ‰å¤±è´¥æ¬¡æ•°æ’åº
        sorted_nodes = sorted(
            self.failed_nodes_counter.items(),
            key=lambda x: x[1]['count'],
            reverse=True
        )[:10]  # å–å‰10ä¸ª

        return {
            'total_failures': sum(info['count'] for info in self.failed_nodes_counter.values()),
            'unique_failed_nodes': len(self.failed_nodes_counter),
            'top_failed_nodes': [
                {
                    'node': node,
                    'count': info['count'],
                    'reasons': info['reasons'][-3:]  # æœ€è¿‘3ä¸ªåŸå› 
                }
                for node, info in sorted_nodes
            ],
            'blacklist_history_size': len(self.blacklist_history)
        }

    def _extract_state_embedding(self, state):
        """ğŸ”¥ [æ–°å¢] æå–çŠ¶æ€åµŒå…¥ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        try:
            return self._get_graph_embedding(state)
        except Exception as e:
            logger.error(f"[Extract State Embedding] Error: {e}")
            # è¿”å›éšæœºåµŒå…¥ä½œä¸º fallbackï¼Œç¡®ä¿ç»´åº¦åŒ¹é…
            return torch.randn(1, self.state_dim, device=self.device)

    def reset_network_parameters(self):
        """ğŸ”¥ [æ–°å¢] é‡ç½®ç½‘ç»œå‚æ•°ï¼ˆç”¨äºè®­ç»ƒå¼‚å¸¸æ—¶çš„è‡ªæ„ˆï¼‰"""
        logger.warning("ğŸ”„ [Auto-Fix] æ­£åœ¨é‡ç½®ç½‘ç»œå‚æ•°...")

        # é‡ç½® High-Level ç½‘ç»œ
        if hasattr(self.high_policy, 'reset_parameters'):
            self.high_policy.reset_parameters()
        self.target_high_policy.load_state_dict(self.high_policy.state_dict())

        # é‡ç½® Low-Level ç½‘ç»œ
        if hasattr(self.low_policy, 'reset_parameters'):
            self.low_policy.reset_parameters()
        self.target_low_policy.load_state_dict(self.low_policy.state_dict())

        # é‡ç½®ä¼˜åŒ–å™¨ (æ¢å¤åˆå§‹å­¦ä¹ ç‡)
        training_cfg = self.config.get('training', {})
        lr_high = training_cfg.get('lr_high', 1e-4)
        lr_low = training_cfg.get('lr_low', 1e-4)

        self.optimizer_high = optim.Adam(self.high_policy.parameters(), lr=lr_high)
        self.optimizer_low = optim.Adam(self.low_policy.parameters(), lr=lr_low)

        logger.info("âœ… ç½‘ç»œå‚æ•°é‡ç½®å®Œæˆ")


# ============================================
# å‘åå…¼å®¹ï¼šä¿ç•™æ—§æ¥å£
# ============================================

class GoalConditionedHRLAgent(HRLAgent):
    """å‘åå…¼å®¹çš„GoalConditionedHRLAgent"""

    def __init__(self, config, phase=3, goal_strategy='adaptive', **kwargs):
        logger.warning("âš ï¸ GoalConditionedHRLAgentå·²é‡æ„ä¸ºHRLAgentï¼Œä½¿ç”¨å…¼å®¹æ¨¡å¼")
        super().__init__(config, phase=phase, goal_strategy=goal_strategy, **kwargs)


# ... (agent.py çš„å…¶ä»–ä»£ç ä¿æŒä¸å˜) ...

# ============================================
# Helper Function
# ============================================

def create_goal_conditioned_agent(config, phase=3, goal_strategy='adaptive', encoder=None, **kwargs):
    """
    åˆ›å»º HRL Agent çš„å·¥å‚å‡½æ•°

    Args:
        config: é…ç½®å­—å…¸
        phase: è®­ç»ƒé˜¶æ®µ (é»˜è®¤: 3)
        goal_strategy: Goal embedding ç­–ç•¥
        encoder: é¢„è®­ç»ƒçš„ GNN Encoder (å¯é€‰) ğŸ”¥ [å…³é”®æ–°å¢]
        **kwargs: å…¶ä»–å‚æ•°
    """
    # å®ä¾‹åŒ– HRLAgent å¹¶é€ä¼  encoder
    return HRLAgent(
        config=config,
        encoder=encoder,  # ğŸ”¥ æŠŠ encoder ä¼ è¿›å»
        phase=phase,
        goal_strategy=goal_strategy,
        **kwargs
    )

