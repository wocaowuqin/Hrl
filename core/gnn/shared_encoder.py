"""
core/gnn/shared_encoder.py
GNN å…±äº«ç¼–ç å™¨ - V2.0 åŠ¨æ€ç‰¹å¾å¼ºåŒ–ç‰ˆ
æ ¸å¿ƒæ”¹è¿›ï¼š
1. æ˜¾å¼åˆ†ç¦»é™æ€æ‹“æ‰‘ç‰¹å¾å’ŒåŠ¨æ€çŠ¶æ€ç‰¹å¾
2. åŠ¨æ€ç‰¹å¾ç‹¬ç«‹å¤„ç†é“¾è·¯ï¼ˆMLP + éçº¿æ€§ï¼‰
3. å¼ºåŠ›èåˆç¡®ä¿åŠ¨æ€ä¿¡å·èƒ½ä¸»å¯¼å†³ç­–
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import logging
from torch_geometric.nn import GATv2Conv

logger = logging.getLogger(__name__)

class SharedEncoder(nn.Module):
    def __init__(self, *args, **kwargs):
        super(SharedEncoder, self).__init__()

        # ====================================================
        # 1. ä¸‡èƒ½å‚æ•°è§£æ (å…¼å®¹ Configå¯¹è±¡ / å­—å…¸ / ä½ç½®å‚æ•°)
        # ====================================================

        # é»˜è®¤é…ç½®
        node_feat_dim = 17
        edge_feat_dim = 5
        request_dim = 24
        hidden_dim = 128
        self.num_layers = kwargs.get('num_layers', 2)

        # ğŸ”¥ æ–°å¢ï¼šåŠ¨æ€ç‰¹å¾ç»´åº¦ï¼ˆé»˜è®¤æœ€å3ç»´ï¼‰
        self.num_dynamic_features = kwargs.get('num_dynamic_features', 3)

        # æƒ…å†µ A: ä¼ å…¥äº†ä¸€ä¸ª config å¯¹è±¡æˆ–å­—å…¸
        if len(args) == 1 and (isinstance(args[0], dict) or hasattr(args[0], 'get') or hasattr(args[0], 'gnn')):
            cfg = args[0]
            def get_cfg(key, default):
                if isinstance(cfg, dict): return cfg.get(key, default)
                val = getattr(cfg, key, None)
                if val is not None: return val
                if hasattr(cfg, 'gnn'):
                    gnn = getattr(cfg, 'gnn')
                    return gnn.get(key, default) if isinstance(gnn, dict) else getattr(gnn, key, default)
                return default

            node_feat_dim = get_cfg('node_feat_dim', node_feat_dim)
            edge_feat_dim = get_cfg('edge_feat_dim', edge_feat_dim)
            request_dim = get_cfg('request_feat_dim', request_dim)
            hidden_dim = get_cfg('hidden_dim', hidden_dim)
            self.num_dynamic_features = get_cfg('num_dynamic_features', self.num_dynamic_features)

        # æƒ…å†µ B: ä½ç½®å‚æ•°
        elif len(args) >= 3:
            node_feat_dim = args[0]
            edge_feat_dim = args[1]
            request_dim = args[2]
            if len(args) > 3: hidden_dim = args[3]

        # æƒ…å†µ C: å…³é”®å­—å‚æ•°è¦†ç›–
        node_feat_dim = kwargs.get('node_feat_dim', node_feat_dim)
        edge_feat_dim = kwargs.get('edge_feat_dim', edge_feat_dim)
        request_dim = kwargs.get('request_feat_dim', request_dim)
        hidden_dim = kwargs.get('hidden_dim', hidden_dim)

        # ä¿å­˜æœŸæœ›ç»´åº¦
        self.node_feat_dim = int(node_feat_dim)
        self.edge_feat_dim = int(edge_feat_dim)
        self.request_dim = int(request_dim)
        self.hidden_dim = int(hidden_dim)

        # ğŸ”¥ è®¡ç®—é™æ€ç‰¹å¾ç»´åº¦
        self.static_feat_dim = self.node_feat_dim - self.num_dynamic_features

        logger.info(f"ğŸ” [SharedEncoder V2.0] Init:")
        logger.info(f"   Total Node Feat: {self.node_feat_dim}")
        logger.info(f"   Static Feat: {self.static_feat_dim}")
        logger.info(f"   Dynamic Feat: {self.num_dynamic_features}")
        logger.info(f"   Edge: {self.edge_feat_dim}, Req: {self.request_dim}, Hidden: {self.hidden_dim}")

        # ====================================================
        # 2. ç½‘ç»œæ„å»º - åŒæµæ¶æ„
        # ====================================================

        # ğŸ”µ é™æ€æµï¼šæ‹“æ‰‘ç»“æ„æ„ŸçŸ¥ï¼ˆGATï¼‰
        self.conv1 = GATv2Conv(
            in_channels=self.static_feat_dim,
            out_channels=self.hidden_dim,
            heads=4,
            edge_dim=self.edge_feat_dim if self.edge_feat_dim > 0 else None,
            concat=False
        )

        self.conv2 = GATv2Conv(
            in_channels=self.hidden_dim,
            out_channels=self.hidden_dim,
            heads=4,
            edge_dim=self.edge_feat_dim if self.edge_feat_dim > 0 else None,
            concat=False
        )

        # ğŸŸ¢ åŠ¨æ€æµï¼šçŠ¶æ€æ„ŸçŸ¥ï¼ˆMLP + å¼ºéçº¿æ€§ï¼‰
        # ä¸“é—¨å¤„ç† [tree_mask, fork_mask, progress_ratio] ç­‰åŠ¨æ€ç‰¹å¾
        self.state_fc1 = nn.Linear(self.num_dynamic_features, self.hidden_dim // 2)
        self.state_fc2 = nn.Linear(self.hidden_dim // 2, self.hidden_dim)

        # ğŸ”¥ åŠ¨æ€ç‰¹å¾é—¨æ§æœºåˆ¶ï¼ˆè®©åŠ¨æ€ç‰¹å¾èƒ½"ä¸€ç¥¨å¦å†³"é™æ€åå¥½ï¼‰
        self.gate_fc = nn.Linear(self.num_dynamic_features, self.hidden_dim)

        # ğŸŸ¡ è¯·æ±‚ç‰¹å¾å±‚
        if self.request_dim > 0:
            self.req_fc = nn.Linear(self.request_dim, self.hidden_dim)
        else:
            self.req_fc = None

        # ğŸ”µ èåˆå±‚ï¼ˆä¸‰è·¯èåˆï¼šé™æ€ + åŠ¨æ€ + è¯·æ±‚ï¼‰
        self.fusion = nn.Linear(self.hidden_dim * 2, self.hidden_dim)
        self.output_dim = self.hidden_dim

        # çŠ¶æ€æ ‡è®°
        self._warned = set()
        self._step_count = 0

    def _fix_dim(self, tensor, expected_dim, name="tensor"):
        """è‡ªåŠ¨ä¿®å¤ç»´åº¦ä¸åŒ¹é…"""
        if tensor is None: return None

        # è·å–æœ€åä¸€ä¸ªç»´åº¦
        actual_dim = tensor.shape[-1]

        if actual_dim == expected_dim:
            return tensor

        if name not in self._warned:
            logger.warning(f"âš ï¸ [SharedEncoder] {name} dim mismatch! Expected {expected_dim}, got {actual_dim}. Auto-fixing...")
            self._warned.add(name)

        if actual_dim < expected_dim:
            # å¡«å……
            padding_shape = list(tensor.shape)
            padding_shape[-1] = expected_dim - actual_dim
            padding = torch.zeros(padding_shape, device=tensor.device)
            return torch.cat([tensor, padding], dim=-1)
        else:
            # æˆªæ–­
            return tensor[..., :expected_dim]

    def forward(self, x, edge_index, edge_attr=None, req_vec=None, batch=None):
        device = x.device
        self._step_count += 1

        # 1. è‡ªåŠ¨ä¿®å¤èŠ‚ç‚¹ç‰¹å¾
        x = self._fix_dim(x, self.node_feat_dim, "node_feat")

        # ğŸ”¥ğŸ”¥ğŸ”¥ 2. æ˜¾å¼åˆ†ç¦»é™æ€å’ŒåŠ¨æ€ç‰¹å¾
        static_x = x[:, :-self.num_dynamic_features]  # å‰N-3ç»´ï¼šé™æ€æ‹“æ‰‘ç‰¹å¾
        dynamic_x = x[:, -self.num_dynamic_features:]  # å3ç»´ï¼š[tree_mask, connected_mask, progress_ratio]

        # 3. æå–è¿›åº¦ä¿¡å·ï¼ˆå‡è®¾åŠ¨æ€ç‰¹å¾ç¬¬3ç»´æ˜¯è¿›åº¦ï¼‰
        # progress_ratio èŒƒå›´ [0.0, 1.0]
        progress_ratio = dynamic_x[:, -1:]
        is_completed = (progress_ratio >= 0.99).float()  # åˆ¤å®šä»»åŠ¡æ˜¯å¦å·²éƒ¨ç½²å®Œæˆ

        # 4. è‡ªåŠ¨ä¿®å¤è¾¹ç¼˜ç‰¹å¾
        if self.edge_feat_dim > 0:
            if edge_attr is None:
                num_edges = edge_index.shape[1]
                edge_attr = torch.zeros(num_edges, self.edge_feat_dim, device=device)
            else:
                edge_attr = self._fix_dim(edge_attr, self.edge_feat_dim, "edge_attr")

        # ğŸ”µ 5. é™æ€æµï¼šGATå·ç§¯æ•æ‰æ‹“æ‰‘ç»“æ„
        try:
            static_emb = self.conv1(static_x, edge_index, edge_attr=edge_attr)
            static_emb = F.relu(static_emb)
            static_emb = self.conv2(static_emb, edge_index, edge_attr=edge_attr)
            static_emb = F.relu(static_emb)
        except RuntimeError as e:
            logger.error(f"âŒ [SharedEncoder] GAT Forward Failed: {e}")
            raise e

        # ğŸŸ¢ 6. åŠ¨æ€æµï¼šMLPå¤„ç†çŠ¶æ€ç‰¹å¾
        state_emb = F.relu(self.state_fc1(dynamic_x))
        state_emb = F.relu(self.state_fc2(state_emb))

        # ğŸ”¥ 7. å¢å¼ºç‰ˆé—¨æ§æœºåˆ¶ï¼šå¼•å…¥å®Œæˆæ€å¼ºåŠ›æŠ‘åˆ¶
        # åŸæœ‰çš„æ³¨æ„åŠ›æƒé‡
        gate_weights = torch.sigmoid(self.gate_fc(dynamic_x))

        # ğŸ”¥ [æ–°å¢é€»è¾‘] å¦‚æœè¿›åº¦å·²æ»¡ï¼Œå¤§å¹…é™ä½é™æ€ç‰¹å¾(æ‹“æ‰‘æƒ¯æ€§)çš„å½±å“åŠ›
        # è¿™ä¼šå‡å¼± Agent ç•™åœ¨æ—§æœ‰æ ‘èŠ‚ç‚¹(é™æ€ç‰¹å¾å¼º)çš„å€¾å‘ï¼Œè¿«ä½¿å®ƒå…³æ³¨ç›®çš„åœ°
        completion_inhibition = 1.0 - (is_completed * 0.8)  # è¿›åº¦æ»¡æ—¶å‰Šå¼± 80% æ‹“æ‰‘ç‰¹å¾
        gate_weights = gate_weights * completion_inhibition

        # ğŸ”¥ èåˆï¼šåŠ¨æ€ç‰¹å¾é€šè¿‡å¢å¼ºé—¨æ§è°ƒåˆ¶é™æ€ç‰¹å¾
        modulated_static = static_emb * gate_weights

        # è¿›åº¦æ»¡æ—¶ï¼Œè®© state_embï¼ˆåŒ…å«è¿›åº¦ä¿¡æ¯ï¼‰å æ®ä¸»å¯¼åœ°ä½
        node_emb = modulated_static + (state_emb * (1.0 + is_completed * 2.0))

        # 8. è¯·æ±‚ç‰¹å¾å¤„ç† (ä¿æŒåŸæœ‰é€»è¾‘)
        if self.req_fc is not None:
            if req_vec is None:
                batch_size = 1 if batch is None else (batch.max().item() + 1)
                req_emb = torch.zeros(batch_size, self.hidden_dim, device=device)
            else:
                if req_vec.dim() == 1: req_vec = req_vec.unsqueeze(0)
                req_vec = self._fix_dim(req_vec, self.request_dim, "req_vec")
                req_emb = self.req_fc(req_vec)
        else:
            batch_size = 1 if batch is None else (batch.max().item() + 1)
            req_emb = torch.zeros(batch_size, self.hidden_dim, device=device)

        # 9. æ™ºèƒ½æ‰©å±•
        if batch is None:
            batch = torch.zeros(node_emb.size(0), dtype=torch.long, device=device)

        if req_emb.dim() == 1:
            req_expanded = req_emb.unsqueeze(0).expand(node_emb.size(0), -1)
        else:
            max_batch_idx = batch.max().item()
            if req_emb.size(0) <= max_batch_idx:
                req_expanded = req_emb[0].unsqueeze(0).expand(node_emb.size(0), -1)
            else:
                req_expanded = req_emb[batch]

        # 10. æœ€ç»ˆèåˆ
        combined = torch.cat([node_emb, req_expanded], dim=-1)
        out = self.fusion(combined)

        return out