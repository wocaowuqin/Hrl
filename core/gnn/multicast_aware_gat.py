import torch
import torch.nn as nn
from core.gnn.shared_encoder import SharedEncoder


class MulticastAwareGAT(nn.Module):
    """
    ä¿®å¤åçš„ MulticastAwareGAT (é€‚é… Vectorized Wrapper)
    """

    def __init__(self,
                 node_feat_dim,
                 edge_feat_dim,
                 request_dim,
                 hidden_dim,
                 action_dim=None,
                 num_layers=3,
                 num_heads=4,
                 dropout=0.0,
                 **kwargs):
        super().__init__()

        # 1. å®ä¾‹åŒ–å…±äº« Encoder
        self.encoder = SharedEncoder(
            node_feat_dim=node_feat_dim,
            edge_feat_dim=edge_feat_dim,
            request_feat_dim=request_dim,  # æ³¨æ„å‚æ•°åå¯¹é½
            hidden_dim=hidden_dim,
            num_layers=num_layers,
            heads=num_heads
        )

        self.hidden_dim = hidden_dim
        self.output_layer = None

    def forward(self, x, edge_index, edge_attr, req_vec, batch=None, dest_indices=None, **kwargs):
        """
        Args:
            x: [N, F_node]
            edge_index: [2, E]
            edge_attr: [E, F_edge]
            req_vec: [Batch, F_req] æˆ– [F_req]
            batch: [N]
            dest_indices: (Wrapper ä¼ è¿›æ¥çš„ï¼ŒSharedEncoder æš‚æ—¶ä¸ç”¨ï¼Œç”¨ **kwargs æ¥æ”¶é˜²æ­¢æŠ¥é”™)
        """

        # 1. è°ƒç”¨ Encoder è·å–èŠ‚ç‚¹åµŒå…¥
        # æ³¨æ„ï¼šè¿™é‡Œ SharedEncoder å†…éƒ¨å·²ç»åšäº† Request Fusion
        # z çš„å½¢çŠ¶: [N, hidden_dim]
        z = self.encoder(x, edge_index, edge_attr=edge_attr, req_vec=req_vec, batch=batch)

        # 2. ğŸ”¥ å…³é”®ä¿®å¤ï¼šè¿”å› tuple ä»¥åŒ¹é… Wrapper çš„è§£åŒ… (node_embeddings, _, _)
        # åä¸¤ä¸ªè¿”å›å€¼é¢„ç•™ç»™å¯èƒ½çš„ edge_weights æˆ– attention_weightsï¼Œç›®å‰ç»™ None
        return z, None, None