# file: evaluator/phase2_evaluator.py
# !/usr/bin/env python3
# -*- coding: utf-8 -*-

import torch
import numpy as np
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from torch_geometric.data import Batch
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns

logger = logging.getLogger(__name__)


class Phase2Evaluator:
    """Phase 2 æ¨¡å‹è¯„ä¼°å™¨"""

    def __init__(self, agent, env, config: Dict):
        self.agent = agent
        self.env = env
        self.config = config
        self.device = next(agent.policy_net.parameters()).device

        # ä»é…ç½®è·å–å‚æ•°
        self.num_nodes = config.get('num_nodes', 28)
        self.request_dim = config.get('request_feat_dim', 24)

        # è¯„ä¼°ç»“æœå­˜å‚¨
        self.results = {
            'accuracy': 0.0,
            'precision': 0.0,
            'recall': 0.0,
            'f1_score': 0.0,
            'confusion_matrix': None,
            'action_distribution': None,
            'per_node_accuracy': {}
        }

    def load_model(self, checkpoint_path: str):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        logger.info(f"ğŸ“‚ åŠ è½½æ¨¡å‹: {checkpoint_path}")

        checkpoint = torch.load(checkpoint_path, map_location=self.device)

        # åŠ è½½æ¨¡å‹æƒé‡
        if 'policy_net' in checkpoint:
            self.agent.policy_net.load_state_dict(checkpoint['policy_net'])
        elif 'model_state_dict' in checkpoint:
            self.agent.policy_net.load_state_dict(checkpoint['model_state_dict'])
        else:
            # ç›´æ¥æ˜¯æ¨¡å‹æƒé‡
            self.agent.policy_net.load_state_dict(checkpoint)

        self.agent.policy_net.eval()
        logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå·²åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼")

        return checkpoint.get('epoch', 0), checkpoint.get('val_loss', 0.0)

    def evaluate_on_dataset(self, expert_data_path: str):
        """åœ¨æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°"""
        logger.info("=" * 60)
        logger.info("ğŸ§ª å¼€å§‹æ•°æ®é›†è¯„ä¼°")
        logger.info("=" * 60)

        # åŠ è½½æ•°æ®é›†
        from trainer.phase2_il_trainer import ExpertDataset
        dataset = ExpertDataset(expert_data_path)

        if len(dataset) == 0:
            logger.error("âŒ æ•°æ®é›†ä¸ºç©º")
            return self.results

        # åˆ›å»ºDataLoader
        from torch.utils.data import DataLoader

        dataloader = DataLoader(
            dataset,
            batch_size=32,  # ä½¿ç”¨å°batch_sizeä»¥ä¾¿å¿«é€Ÿè¯„ä¼°
            shuffle=False,
            num_workers=0,
            collate_fn=self._collate_fn
        )

        # å¼€å§‹è¯„ä¼°
        all_predictions = []
        all_targets = []
        all_correct = []

        with torch.no_grad():
            for i, batch in enumerate(dataloader):
                if batch is None:
                    continue

                states, req_vecs, targets = batch

                # ç§»åˆ°è®¾å¤‡
                states = states.to(self.device)
                req_vecs = req_vecs.to(self.device)
                targets = targets.to(self.device)

                # å‰å‘ä¼ æ’­
                outputs = self.agent.policy_net(
                    x=states.x,
                    edge_index=states.edge_index,
                    edge_attr=states.edge_attr if hasattr(states, 'edge_attr') else None,
                    req_vec=req_vecs,
                    batch=states.batch
                )

                # è·å–é¢„æµ‹
                logits = outputs[0] if isinstance(outputs, tuple) else outputs

                # å¤„ç†logitså½¢çŠ¶
                batch_size = targets.size(0)
                if logits.size(0) == batch_size * self.num_nodes:
                    # é‡å¡‘å¹¶é€‰æ‹©
                    num_actions = logits.size(1)
                    logits = logits.view(batch_size, self.num_nodes, num_actions)

                    # ä½¿ç”¨ç›®æ ‡åŠ¨ä½œé€‰æ‹©logits
                    if torch.max(targets) < self.num_nodes:
                        expanded_targets = targets.unsqueeze(-1).unsqueeze(-1)
                        expanded_targets = expanded_targets.expand(-1, 1, num_actions)
                        logits_selected = torch.gather(logits, dim=1, index=expanded_targets)
                        logits = logits_selected.squeeze(1)
                    else:
                        # ä½¿ç”¨softmaxé€‰æ‹©
                        logits = logits.mean(dim=1)

                # é¢„æµ‹
                predictions = torch.argmax(logits, dim=1)

                # è®¡ç®—å‡†ç¡®ç‡
                correct = (predictions == targets).float()

                # ä¿å­˜ç»“æœ
                all_predictions.extend(predictions.cpu().numpy())
                all_targets.extend(targets.cpu().numpy())
                all_correct.extend(correct.cpu().numpy())

                if (i + 1) % 10 == 0:
                    batch_acc = correct.mean().item()
                    logger.info(f"  Batch {i + 1}/{len(dataloader)}: å‡†ç¡®ç‡ = {batch_acc:.4f}")

        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        all_predictions = np.array(all_predictions)
        all_targets = np.array(all_targets)
        all_correct = np.array(all_correct)

        # åŸºç¡€æŒ‡æ ‡
        accuracy = np.mean(all_correct)
        precision, recall, f1 = self._calculate_classification_metrics(all_predictions, all_targets)

        # ä¿å­˜ç»“æœ
        self.results['accuracy'] = accuracy
        self.results['precision'] = precision
        self.results['recall'] = recall
        self.results['f1_score'] = f1
        self.results['confusion_matrix'] = confusion_matrix(all_targets, all_predictions)
        self.results['action_distribution'] = self._analyze_action_distribution(all_predictions, all_targets)
        self.results['per_node_accuracy'] = self._calculate_per_node_accuracy(all_predictions, all_targets)

        # è¾“å‡ºç»“æœ
        logger.info("=" * 60)
        logger.info("ğŸ“Š è¯„ä¼°ç»“æœ:")
        logger.info(f"  âœ… æ€»ä½“å‡†ç¡®ç‡: {accuracy:.4f}")
        logger.info(f"  âœ… ç²¾ç¡®ç‡ (Precision): {precision:.4f}")
        logger.info(f"  âœ… å¬å›ç‡ (Recall): {recall:.4f}")
        logger.info(f"  âœ… F1åˆ†æ•°: {f1:.4f}")
        logger.info("=" * 60)

        # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
        logger.info("\nğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        logger.info(classification_report(all_targets, all_predictions, digits=4))

        return self.results

    def evaluate_in_environment(self, num_episodes: int = 50):
        """åœ¨çœŸå®ç¯å¢ƒä¸­è¯„ä¼°"""
        logger.info("=" * 60)
        logger.info("ğŸ® å¼€å§‹ç¯å¢ƒè¯„ä¼°")
        logger.info("=" * 60)

        env_results = {
            'success_rate': 0.0,
            'avg_reward': 0.0,
            'avg_steps': 0.0,
            'completion_rate': 0.0,
            'blocking_rate': 0.0,
            'episode_details': []
        }

        total_reward = 0
        total_success = 0
        total_steps = 0
        total_completed = 0
        total_blocked = 0

        for ep in range(num_episodes):
            try:
                # é‡ç½®ç¯å¢ƒ
                reset_result = self.env.reset()
                state = reset_result[0] if isinstance(reset_result, tuple) else reset_result

                if self.env.current_request is None:
                    continue

                # è·Ÿè¸ªè½¨è¿¹
                episode_reward = 0
                episode_steps = 0
                completed = False
                blocked = False
                trajectory = []

                while True:
                    # è·å–åŠ¨ä½œæ©ç 
                    high_mask = self.env.get_high_level_action_mask()
                    low_mask = self.env.get_low_level_action_mask()

                    # ä½¿ç”¨æ¨¡å‹é¢„æµ‹åŠ¨ä½œ
                    with torch.no_grad():
                        # å‡†å¤‡è¾“å…¥
                        state_tensor = self._state_to_tensor(state)
                        req_vec = self._get_request_vector()

                        # æ¨¡å‹é¢„æµ‹
                        outputs = self.agent.policy_net(
                            x=state_tensor.x,
                            edge_index=state_tensor.edge_index,
                            edge_attr=state_tensor.edge_attr if hasattr(state_tensor, 'edge_attr') else None,
                            req_vec=req_vec,
                            batch=state_tensor.batch
                        )

                        logits = outputs[0] if isinstance(outputs, tuple) else outputs

                        # åº”ç”¨æ©ç 
                        logits = logits.cpu().numpy()
                        masked_logits = logits.copy()

                        # åªè€ƒè™‘æœ‰æ•ˆçš„ä½å±‚åŠ¨ä½œ
                        valid_actions = np.where(low_mask)[0]
                        if len(valid_actions) > 0:
                            # å°†æ— æ•ˆåŠ¨ä½œçš„æ¦‚ç‡è®¾ä¸ºæå°å€¼
                            invalid_actions = np.where(~low_mask)[0]
                            masked_logits[invalid_actions] = -1e9

                            # é€‰æ‹©åŠ¨ä½œ
                            if len(masked_logits.shape) > 1:
                                action = np.argmax(masked_logits[0])  # å–ç¬¬ä¸€ä¸ªbatch
                            else:
                                action = np.argmax(masked_logits)
                        else:
                            action = 0  # é»˜è®¤åŠ¨ä½œ

                    # æ‰§è¡ŒåŠ¨ä½œ
                    self.env.step_high_level(action)  # å‡è®¾é«˜å±‚åŠ¨ä½œä¸ä½å±‚ç›¸åŒ
                    step_result = self.env.step_low_level(action)

                    if len(step_result) == 5:
                        next_state, reward, done, truncated, info = step_result
                    else:
                        next_state, reward, done, info = step_result

                    # æ›´æ–°ç»Ÿè®¡
                    episode_reward += reward
                    episode_steps += 1
                    trajectory.append(action)

                    # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                    if info.get('request_completed', False):
                        completed = True
                        total_completed += 1

                    if done:
                        if not completed:
                            blocked = True
                            total_blocked += 1
                        break

                    state = next_state

                # æ›´æ–°æ€»ä½“ç»Ÿè®¡
                total_reward += episode_reward
                total_steps += episode_steps
                if completed:
                    total_success += 1

                # ä¿å­˜episodeè¯¦æƒ…
                episode_detail = {
                    'episode': ep,
                    'reward': episode_reward,
                    'steps': episode_steps,
                    'completed': completed,
                    'blocked': blocked,
                    'trajectory': trajectory[:10]  # åªä¿å­˜å‰10ä¸ªåŠ¨ä½œ
                }
                env_results['episode_details'].append(episode_detail)

                if (ep + 1) % 10 == 0:
                    logger.info(f"  Episode {ep + 1}/{num_episodes}: "
                                f"Reward={episode_reward:.2f}, "
                                f"Steps={episode_steps}, "
                                f"Completed={completed}")

            except Exception as e:
                logger.error(f"âŒ Episode {ep} å¤±è´¥: {e}")
                continue

        # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
        env_results['success_rate'] = total_success / max(1, num_episodes)
        env_results['avg_reward'] = total_reward / max(1, num_episodes)
        env_results['avg_steps'] = total_steps / max(1, num_episodes)
        env_results['completion_rate'] = total_completed / max(1, num_episodes)
        env_results['blocking_rate'] = total_blocked / max(1, num_episodes)

        # è¾“å‡ºç»“æœ
        logger.info("=" * 60)
        logger.info("ğŸ“Š ç¯å¢ƒè¯„ä¼°ç»“æœ:")
        logger.info(f"  âœ… æˆåŠŸç‡: {env_results['success_rate']:.4f}")
        logger.info(f"  âœ… å¹³å‡å¥–åŠ±: {env_results['avg_reward']:.4f}")
        logger.info(f"  âœ… å¹³å‡æ­¥æ•°: {env_results['avg_steps']:.2f}")
        logger.info(f"  âœ… å®Œæˆç‡: {env_results['completion_rate']:.4f}")
        logger.info(f"  âœ… é˜»å¡ç‡: {env_results['blocking_rate']:.4f}")
        logger.info("=" * 60)

        return env_results

    def _collate_fn(self, batch):
        """æ•°æ®æ‰¹å¤„ç†å‡½æ•°"""
        from trainer.phase2_il_trainer import Batch as GraphBatch

        states = []
        actions = []
        req_vecs = []

        for item in batch:
            state = item.get('state') or item.get('network_state')
            if state is None:
                continue

            action = item.get('action')
            if action is None:
                continue

            states.append(state)
            actions.append(action)

            if hasattr(state, 'req_vec'):
                req_vecs.append(state.req_vec)
            elif hasattr(state, 'req'):
                req_vecs.append(state.req)
            else:
                req_vecs.append(torch.zeros(self.request_dim))

        if not states:
            return None

        try:
            graph_batch = GraphBatch.from_data_list(states)
            req_vecs = torch.stack(req_vecs, dim=0).float()
            actions = torch.tensor(actions, dtype=torch.long)

            return graph_batch, req_vecs, actions
        except Exception as e:
            logger.error(f"æ‰¹å¤„ç†å¤±è´¥: {e}")
            return None

    def _state_to_tensor(self, state):
        """å°†çŠ¶æ€è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼"""
        # è¿™é‡Œéœ€è¦æ ¹æ®æ‚¨çš„ç¯å¢ƒçŠ¶æ€æ ¼å¼è¿›è¡Œè°ƒæ•´
        # å‡è®¾stateå·²ç»æ˜¯torch_geometric Dataå¯¹è±¡
        return state

    def _get_request_vector(self):
        """ä»ç¯å¢ƒä¸­è·å–è¯·æ±‚å‘é‡"""
        # è¿™é‡Œéœ€è¦æ ¹æ®æ‚¨çš„ç¯å¢ƒå®ç°è°ƒæ•´
        # è¿”å›å½¢çŠ¶ä¸º[1, request_dim]çš„å¼ é‡
        return torch.randn(1, self.request_dim).to(self.device)

    def _calculate_classification_metrics(self, predictions, targets):
        """è®¡ç®—åˆ†ç±»æŒ‡æ ‡"""
        from sklearn.metrics import precision_score, recall_score, f1_score

        # è®¡ç®—å¾®è§‚å¹³å‡ï¼ˆå¯¹æ‰€æœ‰æ ·æœ¬å¹³ç­‰å¯¹å¾…ï¼‰
        precision = precision_score(targets, predictions, average='micro', zero_division=0)
        recall = recall_score(targets, predictions, average='micro', zero_division=0)
        f1 = f1_score(targets, predictions, average='micro', zero_division=0)

        return precision, recall, f1

    def _analyze_action_distribution(self, predictions, targets):
        """åˆ†æåŠ¨ä½œåˆ†å¸ƒ"""
        unique_preds, pred_counts = np.unique(predictions, return_counts=True)
        unique_targets, target_counts = np.unique(targets, return_counts=True)

        return {
            'prediction_distribution': dict(zip(unique_preds, pred_counts)),
            'target_distribution': dict(zip(unique_targets, target_counts)),
            'prediction_entropy': self._calculate_entropy(pred_counts),
            'target_entropy': self._calculate_entropy(target_counts)
        }

    def _calculate_per_node_accuracy(self, predictions, targets):
        """è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„å‡†ç¡®ç‡"""
        per_node_acc = {}
        for node in range(self.num_nodes):
            mask = targets == node
            if np.sum(mask) > 0:
                node_correct = np.sum(predictions[mask] == targets[mask])
                node_total = np.sum(mask)
                per_node_acc[node] = node_correct / node_total

        return per_node_acc

    def _calculate_entropy(self, counts):
        """è®¡ç®—åˆ†å¸ƒçš„ç†µ"""
        probs = counts / np.sum(counts)
        entropy = -np.sum(probs * np.log2(probs + 1e-10))
        return entropy

    def visualize_results(self, output_dir: str):
        """å¯è§†åŒ–è¯„ä¼°ç»“æœ"""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        # 1. æ··æ·†çŸ©é˜µçƒ­å›¾
        if self.results['confusion_matrix'] is not None:
            plt.figure(figsize=(10, 8))
            cm = self.results['confusion_matrix']

            # å½’ä¸€åŒ–æ··æ·†çŸ©é˜µ
            cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

            sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',
                        xticklabels=range(self.num_nodes),
                        yticklabels=range(self.num_nodes))

            plt.title('Normalized Confusion Matrix')
            plt.xlabel('Predicted Node')
            plt.ylabel('True Node')
            plt.tight_layout()
            plt.savefig(output_path / 'confusion_matrix.png')
            plt.close()

        # 2. å‡†ç¡®ç‡æŸ±çŠ¶å›¾
        if self.results['per_node_accuracy']:
            plt.figure(figsize=(12, 6))
            nodes = list(self.results['per_node_accuracy'].keys())
            accuracies = list(self.results['per_node_accuracy'].values())

            plt.bar(nodes, accuracies)
            plt.axhline(y=self.results['accuracy'], color='r', linestyle='--',
                        label=f'Overall Accuracy: {self.results["accuracy"]:.4f}')

            plt.title('Per-Node Accuracy')
            plt.xlabel('Node ID')
            plt.ylabel('Accuracy')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig(output_path / 'per_node_accuracy.png')
            plt.close()

        # 3. åŠ¨ä½œåˆ†å¸ƒå›¾
        if self.results['action_distribution']:
            plt.figure(figsize=(14, 6))

            # é¢„æµ‹åˆ†å¸ƒ
            plt.subplot(1, 2, 1)
            pred_dist = self.results['action_distribution']['prediction_distribution']
            plt.bar(list(pred_dist.keys()), list(pred_dist.values()))
            plt.title(
                f'Prediction Distribution (Entropy: {self.results["action_distribution"]["prediction_entropy"]:.4f})')
            plt.xlabel('Node')
            plt.ylabel('Count')

            # ç›®æ ‡åˆ†å¸ƒ
            plt.subplot(1, 2, 2)
            target_dist = self.results['action_distribution']['target_distribution']
            plt.bar(list(target_dist.keys()), list(target_dist.values()))
            plt.title(f'Target Distribution (Entropy: {self.results["action_distribution"]["target_entropy"]:.4f})')
            plt.xlabel('Node')
            plt.ylabel('Count')

            plt.tight_layout()
            plt.savefig(output_path / 'action_distribution.png')
            plt.close()

        logger.info(f"âœ… å¯è§†åŒ–ç»“æœå·²ä¿å­˜åˆ°: {output_path}")