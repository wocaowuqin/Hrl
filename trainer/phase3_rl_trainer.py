
# core/trainer/phase3_rl_trainer.py
# !/usr/bin/env python3
# -*- coding: utf-8 -*-
import logging
import os

import numpy as np
import random
import pickle
from pathlib import Path
from tqdm import tqdm
from torch.utils.tensorboard import SummaryWriter
import torch
from utils.visualizer import SFCVisualizer
logger = logging.getLogger(__name__)
class Phase3RLTrainer:
    """Phase 3: Goal-Conditioned RL Trainer with DAgger + Time Slot System"""
    def __init__(self, env, agent, output_dir, config, coordinator=None):
        self.env = env
        self.agent = agent
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.cfg = config
        self.coordinator = coordinator

        # ğŸ”¥ åˆå§‹åŒ–å¯è§†åŒ–å™¨
        self.visualizer = None
        if hasattr(env, 'topo'):
            try:
                self.visualizer = SFCVisualizer(env.topo, output_dir)
                logger.info("ğŸ¨ å¯è§†åŒ–å™¨å·²å°±ç»ª (plots å°†ä¿å­˜åœ¨ outputs/checkpoints/plots)")
            except Exception as e:
                logger.warning(f"âš ï¸ å¯è§†åŒ–å™¨åˆå§‹åŒ–å¤±è´¥: {e}")

        phase3_cfg = config.get("phase3", {})
        self.max_episodes = phase3_cfg.get("episodes", 1000)
        self.save_freq = phase3_cfg.get("save_every", 100)

        # ğŸ”§ æ–°å¢ï¼šè®­ç»ƒå‚æ•°
        self.max_steps_per_episode = phase3_cfg.get("max_steps", 600)
        self.update_frequency = phase3_cfg.get("update_frequency", 4)  # æ¯4æ­¥æ›´æ–°ä¸€æ¬¡
        self.warmup_steps = phase3_cfg.get("warmup_steps", 100)  # é¢„çƒ­æ­¥æ•°

        # 1. Epsilon é…ç½®
        epsilon_cfg = phase3_cfg.get("epsilon", {})
        self.epsilon_initial = epsilon_cfg.get("initial", 0.7)
        self.epsilon_final = epsilon_cfg.get("final", 0.01)
        self.epsilon_decay_steps = epsilon_cfg.get("decay_steps", 5000)

        # 2. DAgger é…ç½®
        dagger_cfg = phase3_cfg.get("dagger", {})
        self.use_dagger = dagger_cfg.get("enabled", True)
        self.beta = dagger_cfg.get("initial_beta", 0.8)
        self.beta_final = dagger_cfg.get("final_beta", 0.05)
        self.beta_decay_steps = dagger_cfg.get("decay_steps", 10000)

        # ğŸ”¥ æ–°å¢ï¼šæ—¶é—´æ§½ç³»ç»Ÿé…ç½®
        timeslot_cfg = phase3_cfg.get("timeslot", {})
        self.use_timeslot = timeslot_cfg.get("enabled", True)
        self.log_timeslot_info = timeslot_cfg.get("log_timeslot_info", True)
        self.log_timeslot_jumps = timeslot_cfg.get("log_jumps", True)

        # TensorBoard
        self.writer = SummaryWriter(log_dir=str(self.output_dir / "runs"))

        # ç»Ÿè®¡ä¿¡æ¯å®¹å™¨
        self.stats = {
            "rewards": [],
            "acceptance_rates": [],
            "blocking_rates": [],
            "resource_levels": [],
            "subgoal_completion_rate": [],
            "time_slots_covered": [],
            "decision_steps": [],
            "requests_per_episode": [],
            "losses": [],
            "high_losses": [],
            "low_losses": []

        }
        self.global_step = 0
        self.total_updates = 0  # ğŸ”§ æ–°å¢ï¼šæ€»æ›´æ–°æ¬¡æ•°

        # ğŸ”¥ æ–°å¢ï¼šæ—¶é—´æ§½ç›¸å…³ç»Ÿè®¡
        self.timeslot_stats = {
            'total_time_slots': 0,
            'total_decision_steps': 0,
            'avg_steps_per_request': 0,
            'timeslot_jumps': []
        }
        # ğŸ”¥ æ·»åŠ è¯Šæ–­å¼€å…³
        self.enable_diagnosis = config.get('enable_diagnosis', False)
        self.diagnosis_interval = config.get('diagnosis_interval', 10)
    def run(self):
        """ğŸš€ Phase 3 è®­ç»ƒä¸»å¾ªç¯ - å®Œæ•´ç‰ˆå«è¯Šæ–­"""

        # ğŸ”¥ğŸ”¥ğŸ”¥ è¯Šæ–­ä»£ç å—1ï¼šè®­ç»ƒå¼€å§‹å‰è¯Šæ–­ ğŸ”¥ğŸ”¥ğŸ”¥
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ” è®­ç»ƒå‰ç¯å¢ƒè¯Šæ–­")
        logger.info("=" * 80)

        logger.info(f"\n1ï¸âƒ£ èµ„æºç®¡ç†å™¨ç»“æ„:")
        logger.info(f"   ç±»å‹: {type(self.env.resource_mgr).__name__}")
        logger.info(f"   nodesç±»å‹: {type(self.env.resource_mgr.nodes)}")

        if isinstance(self.env.resource_mgr.nodes, dict):
            keys = list(self.env.resource_mgr.nodes.keys())[:5]
            logger.info(f"   nodesé”®: {keys}")

            if 'cpu' in self.env.resource_mgr.nodes:
                logger.info(f"   âš ï¸ ç»“æ„: åˆ—è¡¨å­—å…¸ {{'cpu': [...], 'mem': [...]}}")
                cpu_list = self.env.resource_mgr.nodes.get('cpu', [])
                mem_list = self.env.resource_mgr.nodes.get('mem', [])
                logger.info(f"   CPUåˆ—è¡¨é•¿åº¦: {len(cpu_list)}")
                logger.info(f"   å‰3ä¸ªèŠ‚ç‚¹CPU: {cpu_list[:3]}")
                logger.info(f"   å‰3ä¸ªèŠ‚ç‚¹Mem: {mem_list[:3]}")
            elif 0 in self.env.resource_mgr.nodes:
                logger.info(f"   âœ… ç»“æ„: èŠ‚ç‚¹å­—å…¸ {{0: {{}}, 1: {{}}, ...}}")
                for i in range(min(3, len(self.env.resource_mgr.nodes))):
                    node = self.env.resource_mgr.nodes.get(i, {})
                    logger.info(f"   èŠ‚ç‚¹{i}: CPU={node.get('cpu', 'N/A')}, Mem={node.get('mem', 'N/A')}")

        logger.info(f"\n2ï¸âƒ£ ç¯å¢ƒé‡ç½®æµ‹è¯•:")
        self.env.reset()
        logger.info(f"   è¯·æ±‚å­˜åœ¨: {self.env.current_request is not None}")
        if self.env.current_request:
            vnf_list = self.env.current_request.get('vnf', [])
            logger.info(f"   VNFæ•°é‡: {len(vnf_list)}")
            logger.info(f"   æºèŠ‚ç‚¹: {self.env.current_request.get('source')}")
            logger.info(f"   ç›®çš„èŠ‚ç‚¹: {self.env.current_request.get('dest', [])}")

        logger.info(f"\n3ï¸âƒ£ é«˜å±‚åŠ¨ä½œæ©ç :")
        mask = self.env.get_high_level_action_mask()
        available = np.where(mask)[0]
        logger.info(f"   å¯ç”¨åŠ¨ä½œæ•°: {len(available)}")
        logger.info(f"   å¯ç”¨èŠ‚ç‚¹: {available[:10]}")

        logger.info(f"\n4ï¸âƒ£ èŠ‚ç‚¹è¯¦æƒ… (å‰10ä¸ª):")

        # ğŸ”¥ æ­£ç¡®è·å–èµ„æº
        if isinstance(self.env.resource_mgr.nodes, dict) and 'cpu' in self.env.resource_mgr.nodes:
            cpu_list = self.env.resource_mgr.nodes.get('cpu', [])
            mem_list = self.env.resource_mgr.nodes.get('memory', [])

            for node in range(min(10, self.env.n)):
                is_valid = self.env._is_valid_node(node)
                is_dc = node in getattr(self.env, 'dc_nodes', [])

                cpu = cpu_list[node] if node < len(cpu_list) else 'N/A'
                mem = mem_list[node] if node < len(mem_list) else 'N/A'
                mask_val = mask[node]

                logger.info(f"   èŠ‚ç‚¹{node}: æœ‰æ•ˆ={'âœ…' if is_valid else 'âŒ'}, DC={'âœ…' if is_dc else 'âŒ'}, "
                            f"CPU={cpu}, Mem={mem}, Mask={mask_val}")
        else:
            # åŸæ¥çš„é€»è¾‘
            for node in range(min(10, self.env.n)):
                is_valid = self.env._is_valid_node(node)
                is_dc = node in getattr(self.env, 'dc_nodes', [])
                node_info = self.env.resource_mgr.nodes.get(node, {})
                cpu = node_info.get('cpu', 'N/A')
                mem = node_info.get('mem', 'N/A')
                mask_val = mask[node]

                logger.info(f"   èŠ‚ç‚¹{node}: æœ‰æ•ˆ={'âœ…' if is_valid else 'âŒ'}, DC={'âœ…' if is_dc else 'âŒ'}, "
                            f"CPU={cpu}, Mem={mem}, Mask={mask_val}")

        logger.info(f"\n5ï¸âƒ£ æµ‹è¯•é«˜å±‚æ‰§è¡Œ:")
        if len(available) > 0:
            test_action = available[0]
            logger.info(f"   æµ‹è¯•èŠ‚ç‚¹: {test_action}")

            before_phase = getattr(self.env, 'current_phase', 'unknown')
            before_vnf = self.env._get_total_vnf_progress()

            _, reward, done, trunc, info = self.env.step_high_level(test_action)

            logger.info(f"   æ‰§è¡Œå‰: é˜¶æ®µ={before_phase}, VNFè¿›åº¦={before_vnf}")
            logger.info(f"   æ‰§è¡Œå: reward={reward}, done={done}, trunc={trunc}")
            logger.info(f"   Info: {info}")

            if 'error' in info:
                logger.error(f"\n   âš ï¸âš ï¸âš ï¸ æ£€æµ‹åˆ°é”™è¯¯: {info['error']}")
                logger.error(f"   âš ï¸âš ï¸âš ï¸ è¿™å¯èƒ½å°±æ˜¯å¾ªç¯çš„åŸå› !")

        logger.info("=" * 80 + "\n")
        # ğŸ”¥ğŸ”¥ğŸ”¥ è¯Šæ–­ä»£ç å—1 ç»“æŸ ğŸ”¥ğŸ”¥ğŸ”¥

        # ====================================================================
        # ä¸»è®­ç»ƒå¾ªç¯
        # ====================================================================
        num_episodes = self.cfg.get('num_episodes', 1000)

        for episode in range(num_episodes):
            state = self.env.reset()
            episode_reward = 0
            done = False
            step_count = 0

            # ğŸ”¥ğŸ”¥ğŸ”¥ è¯Šæ–­å˜é‡ ğŸ”¥ğŸ”¥ğŸ”¥
            consecutive_same_high_action = 0
            last_high_action = None
            low_timeout_count = 0
            high_error_count = 0
            # ğŸ”¥ğŸ”¥ğŸ”¥

            while not done:
                # ============================================================
                # å¦‚æœæœ‰ Coordinatorï¼Œä½¿ç”¨ Coordinator æ‰§è¡Œ
                # ============================================================
                # ============================================================
                # å¦‚æœæœ‰ Coordinatorï¼Œä½¿ç”¨ Coordinator æ‰§è¡Œ
                # ============================================================
                if self.coordinator:
                    result = self.coordinator.step()

                    # ğŸ”¥ ä¿®å¤ï¼šå¤„ç† tuple è¿”å›å€¼
                    if isinstance(result, tuple):
                        # Coordinator.step() è¿”å› (state, reward, done, truncated, info)
                        state, reward, done, truncated, info = result

                        # ä» info ä¸­æå–ä¿¡æ¯
                        high_action = info.get('high_action') if isinstance(info, dict) else None

                        # ğŸ”¥ğŸ”¥ğŸ”¥ è¯Šæ–­ä»£ç å—2ï¼šCoordinatorç»“æœè¯Šæ–­ ğŸ”¥ğŸ”¥ğŸ”¥
                        if high_action == last_high_action:
                            consecutive_same_high_action += 1
                            if consecutive_same_high_action >= 3:
                                logger.warning(f"\nâš ï¸ [Episode {episode}, Step {step_count}] å¾ªç¯è­¦å‘Š!")
                                logger.warning(f"   è¿ç»­{consecutive_same_high_action}æ¬¡é€‰æ‹©èŠ‚ç‚¹{high_action}")
                                logger.warning(f"   å½“å‰ä½ç½®: èŠ‚ç‚¹{self.env.current_node_location}")
                                logger.warning(f"   å½“å‰é˜¶æ®µ: {getattr(self.env, 'current_phase', 'unknown')}")
                                vnf_list = self.env.current_request.get('vnf', []) if self.env.current_request else []
                                logger.warning(f"   VNFè¿›åº¦: {self.env._get_total_vnf_progress()}/{len(vnf_list)}")
                                logger.warning(f"   ç›®æ ‡èŠ‚ç‚¹: {getattr(self.env, 'current_deployment_target', 'N/A')}")
                                # æ£€æŸ¥èŠ‚ç‚¹èµ„æºï¼ˆä¿®å¤ç‰ˆï¼‰
                                if high_action is not None:  # ğŸ”¥ åŠ ä¸Šè¿™ä¸ªæ£€æŸ¥
                                    if isinstance(self.env.resource_mgr.nodes,
                                                  dict) and 'cpu' in self.env.resource_mgr.nodes:
                                        cpu_list = self.env.resource_mgr.nodes.get('cpu', [])
                                        mem_list = self.env.resource_mgr.nodes.get('memory', [])
                                        cpu = cpu_list[high_action] if high_action < len(cpu_list) else 'N/A'
                                        mem = mem_list[high_action] if high_action < len(mem_list) else 'N/A'
                                    else:
                                        node_info = self.env.resource_mgr.nodes.get(high_action, {})
                                        cpu = node_info.get('cpu', 'N/A')
                                        mem = node_info.get('mem', 'N/A')

                                    logger.warning(f"   èŠ‚ç‚¹{high_action}èµ„æº: CPU={cpu}, Mem={mem}")
                                    logger.warning(
                                        f"   èŠ‚ç‚¹{high_action}æ˜¯DCèŠ‚ç‚¹: {high_action in getattr(self.env, 'dc_nodes', [])}")
                                else:
                                    logger.warning(f"   âš ï¸ high_action æ˜¯ None! infoå†…å®¹: {info}")
                                # æ£€æŸ¥èŠ‚ç‚¹èµ„æºï¼ˆä¿®å¤ç‰ˆï¼‰
                                if isinstance(self.env.resource_mgr.nodes,
                                              dict) and 'cpu' in self.env.resource_mgr.nodes:
                                    cpu_list = self.env.resource_mgr.nodes.get('cpu', [])
                                    mem_list = self.env.resource_mgr.nodes.get('memory', [])
                                    cpu = cpu_list[high_action] if high_action < len(cpu_list) else 'N/A'
                                    mem = mem_list[high_action] if high_action < len(mem_list) else 'N/A'
                                else:
                                    node_info = self.env.resource_mgr.nodes.get(high_action, {})
                                    cpu = node_info.get('cpu', 'N/A')
                                    mem = node_info.get('mem', 'N/A')

                                logger.warning(f"   èŠ‚ç‚¹{high_action}èµ„æº: CPU={cpu}, Mem={mem}")
                                logger.warning(
                                    f"   èŠ‚ç‚¹{high_action}æ˜¯DCèŠ‚ç‚¹: {high_action in getattr(self.env, 'dc_nodes', [])}")

                                # å¼ºåˆ¶ä¸­æ–­
                                if consecutive_same_high_action >= 5:
                                    logger.error(f"   âŒ è¿ç»­{consecutive_same_high_action}æ¬¡ï¼Œå¼ºåˆ¶ç»ˆæ­¢episode")
                                    break
                        else:
                            consecutive_same_high_action = 0

                        last_high_action = high_action

                        # æ£€æµ‹é”™è¯¯
                        if isinstance(info, dict) and 'error' in info:
                            high_error_count += 1
                            logger.error(f"\nâš ï¸ [Episode {episode}, Step {step_count}] é«˜å±‚é”™è¯¯!")
                            logger.error(f"   é”™è¯¯ä¿¡æ¯: {info['error']}")
                            logger.error(f"   ç´¯è®¡é”™è¯¯æ¬¡æ•°: {high_error_count}")

                        # æ£€æµ‹ä½å±‚è¶…æ—¶
                        if isinstance(info, dict) and info.get('low_timeout'):
                            low_timeout_count += 1
                            logger.warning(f"\nâš ï¸ [Episode {episode}, Step {step_count}] ä½å±‚è¶…æ—¶!")
                            logger.warning(f"   å½“å‰ä½ç½®: èŠ‚ç‚¹{self.env.current_node_location}")
                            target = getattr(self.env, 'current_deployment_target',
                                             getattr(self.env, 'current_target_node', 'N/A'))
                            logger.warning(f"   ç›®æ ‡èŠ‚ç‚¹: {target}")
                            logger.warning(f"   ç´¯è®¡è¶…æ—¶æ¬¡æ•°: {low_timeout_count}")

                        # ğŸ”¥ğŸ”¥ğŸ”¥ è¯Šæ–­ä»£ç å—2 ç»“æŸ ğŸ”¥ğŸ”¥ğŸ”¥

                        episode_reward += reward
                        # done å·²ç»ä» tuple ä¸­æå–

                    else:
                        # å…¼å®¹å­—å…¸æ ¼å¼ï¼ˆå¦‚æœ Coordinator è¿”å›å­—å…¸ï¼‰
                        high_action = result.get('high_action')

                        # å¾ªç¯æ£€æµ‹ï¼ˆå­—å…¸æ ¼å¼ï¼‰
                        if high_action == last_high_action:
                            consecutive_same_high_action += 1
                            if consecutive_same_high_action >= 3:
                                logger.warning(f"\nâš ï¸ [Episode {episode}, Step {step_count}] å¾ªç¯è­¦å‘Š!")
                                logger.warning(f"   è¿ç»­{consecutive_same_high_action}æ¬¡é€‰æ‹©èŠ‚ç‚¹{high_action}")

                                if consecutive_same_high_action >= 5:
                                    logger.error(f"   âŒ å¼ºåˆ¶ç»ˆæ­¢")
                                    break
                        else:
                            consecutive_same_high_action = 0

                        last_high_action = high_action

                        # æ£€æµ‹é”™è¯¯
                        if 'error' in result:
                            high_error_count += 1
                            logger.error(f"\nâš ï¸ é«˜å±‚é”™è¯¯: {result['error']}")

                        # æ£€æµ‹ä½å±‚è¶…æ—¶
                        if result.get('low_timeout'):
                            low_timeout_count += 1
                            logger.warning(f"\nâš ï¸ ä½å±‚è¶…æ—¶! ç´¯è®¡{low_timeout_count}æ¬¡")

                        episode_reward += result.get('reward', 0)
                        done = result.get('done', False)

                # ============================================================
                # å¦‚æœæ²¡æœ‰ Coordinatorï¼Œæ‰‹åŠ¨æ‰§è¡Œé«˜å±‚+ä½å±‚
                # ============================================================
                else:
                    # é«˜å±‚å†³ç­–
                    high_state = self.env.get_high_level_state_graph()
                    high_mask = self.env.get_high_level_action_mask()

                    # Agent é€‰æ‹©åŠ¨ä½œ
                    with torch.no_grad():
                        high_action = self.agent.select_action(high_state, high_mask, explore=True)

                    # ğŸ”¥ğŸ”¥ğŸ”¥ è¯Šæ–­ä»£ç å—3ï¼šæ‰‹åŠ¨æ¨¡å¼å¾ªç¯æ£€æµ‹ ğŸ”¥ğŸ”¥ğŸ”¥
                    if high_action == last_high_action:
                        consecutive_same_high_action += 1
                        if consecutive_same_high_action >= 3:
                            logger.warning(f"\nâš ï¸ [Episode {episode}, Step {step_count}] å¾ªç¯è­¦å‘Š!")
                            logger.warning(f"   è¿ç»­{consecutive_same_high_action}æ¬¡é€‰æ‹©èŠ‚ç‚¹{high_action}")

                            if consecutive_same_high_action >= 5:
                                logger.error(f"   âŒ å¼ºåˆ¶ç»ˆæ­¢")
                                break
                    else:
                        consecutive_same_high_action = 0

                    last_high_action = high_action
                    # ğŸ”¥ğŸ”¥ğŸ”¥

                    # æ‰§è¡Œé«˜å±‚åŠ¨ä½œ
                    _, high_reward, high_done, high_trunc, high_info = self.env.step_high_level(high_action)

                    # ğŸ”¥ é”™è¯¯æ£€æµ‹
                    if 'error' in high_info:
                        high_error_count += 1
                        logger.error(f"\nâš ï¸ é«˜å±‚é”™è¯¯: {high_info['error']}")

                    episode_reward += high_reward

                    # å¦‚æœæ²¡ç»“æŸï¼Œæ‰§è¡Œä½å±‚
                    if not high_done and not high_trunc:
                        low_done = False
                        low_step = 0
                        max_low_steps = 50

                        while not low_done and low_step < max_low_steps:
                            low_state = self.env.get_state()
                            low_mask = self.env.get_low_level_action_mask()

                            with torch.no_grad():
                                low_action = self.agent.select_action(low_state, low_mask, explore=True)

                            _, low_reward, low_done, low_trunc, low_info = self.env.step_low_level(low_action)

                            episode_reward += low_reward
                            low_step += 1

                            # ğŸ”¥ è¶…æ—¶æ£€æµ‹
                            if low_info.get('timeout'):
                                low_timeout_count += 1
                                logger.warning(f"\nâš ï¸ ä½å±‚è¶…æ—¶! ç´¯è®¡{low_timeout_count}æ¬¡")
                                break

                            if low_trunc:
                                break

                        if low_step >= max_low_steps:
                            logger.warning(f"âš ï¸ ä½å±‚è¾¾åˆ°æœ€å¤§æ­¥æ•°{max_low_steps}")

                    done = high_done

                # ============================================================
                # é€šç”¨ï¼šæ­¥æ•°ä¿æŠ¤
                # ============================================================
                step_count += 1

                # ğŸ”¥ğŸ”¥ğŸ”¥ è¯Šæ–­ä»£ç å—4ï¼šæ­¥æ•°ä¿æŠ¤ ğŸ”¥ğŸ”¥ğŸ”¥
                if step_count > 200:
                    logger.error(f"\nâŒ [Episode {episode}] æ­¥æ•°è¶…é™({step_count})ï¼Œå¼ºåˆ¶ç»ˆæ­¢")
                    logger.error(f"   å½“å‰é˜¶æ®µ: {getattr(self.env, 'current_phase', 'unknown')}")
                    logger.error(f"   VNFè¿›åº¦: {self.env._get_total_vnf_progress()}")
                    logger.error(f"   ä½å±‚è¶…æ—¶æ¬¡æ•°: {low_timeout_count}")
                    logger.error(f"   é«˜å±‚é”™è¯¯æ¬¡æ•°: {high_error_count}")
                    break
                # ğŸ”¥ğŸ”¥ğŸ”¥

            # ====================================================================
            # Episode ç»“æŸç»Ÿè®¡
            # ====================================================================
            logger.info(f"\nEpisode {episode}: Reward={episode_reward:.2f}, Steps={step_count}, "
                        f"ä½å±‚è¶…æ—¶={low_timeout_count}æ¬¡, é«˜å±‚é”™è¯¯={high_error_count}æ¬¡")

            # ğŸ”¥ æ¯10ä¸ªepisodeè¯¦ç»†è¾“å‡º
            if episode % 10 == 0:
                if self.env.current_request:
                    vnf_list = self.env.current_request.get('vnf', [])
                    logger.info(f"   VNFè¿›åº¦: {self.env._get_total_vnf_progress()}/{len(vnf_list)}")
                    connected = len(self.env.current_tree.get('connected_dests', set()))
                    dests = len(self.env.current_request.get('dest', []))
                    logger.info(f"   å·²è¿æ¥ç›®çš„åœ°: {connected}/{dests}")

            # ====================================================================
            # å®šæœŸä¿å­˜æ¨¡å‹ (æ¯100ä¸ªepisode)
            # ====================================================================
            if episode > 0 and episode % 100 == 0:
                save_path = os.path.join(self.output_dir, f"checkpoint_ep{episode}.pth")
                try:
                    torch.save({
                        'episode': episode,
                        'agent_state': self.agent.state_dict() if hasattr(self.agent, 'state_dict') else None,
                        'config': self.cfg
                    }, save_path)
                    logger.info(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {save_path}")
                except Exception as e:
                    logger.warning(f"âš ï¸ ä¿å­˜å¤±è´¥: {e}")

        # ====================================================================
        # è®­ç»ƒç»“æŸï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹
        # ====================================================================
        final_path = os.path.join(self.output_dir, "phase3_final.pth")
        try:
            torch.save({
                'agent_state': self.agent.state_dict() if hasattr(self.agent, 'state_dict') else None,
                'config': self.config
            }, final_path)
            logger.info(f"âœ… è®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆæ¨¡å‹: {final_path}")
        except Exception as e:
            logger.error(f"âŒ æœ€ç»ˆæ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
    def _run_episode(self, episode_idx: int):
        """
        ğŸ”¥ [V32.0 HRL Coordinator é›†æˆç‰ˆ]

        è¿è¡Œä¸€ä¸ªepisodeï¼ˆé›†æˆ Coordinator + é»‘åå• + DAgger + æ—¶é—´æ§½ç³»ç»Ÿ + Lossç›‘æ§ï¼‰

        æ ¸å¿ƒé€»è¾‘:
        1. ä¼˜å…ˆä½¿ç”¨ HRL Coordinatorï¼ˆå¦‚æœå¯ç”¨ï¼‰
        2. Coordinator è‡ªåŠ¨ç®¡ç†é«˜ä½å±‚äº¤äº’
        3. å›é€€åˆ°ç›´æ¥è°ƒç”¨ env.stepï¼ˆå…¼å®¹æ¨¡å¼ï¼‰
        """
        import numpy as np
        import random

        # ========================================
        # åˆå§‹åŒ–
        # ========================================
        # ğŸ”§ é¢„çƒ­æ£€æŸ¥
        if self.agent.steps_done < self.warmup_steps:
            logger.debug(f"ğŸ”¥ é¢„çƒ­é˜¶æ®µ: {self.agent.steps_done}/{self.warmup_steps}")

        max_steps = self.max_steps_per_episode

        # âœ… é‡ç½®ç¯å¢ƒ
        reset_result = self.env.reset()
        if isinstance(reset_result, tuple) and len(reset_result) == 2:
            state, reset_info = reset_result
        else:
            state = reset_result
            reset_info = {}

        # ğŸ”¥ è·å–æ—¶é—´æ§½ä¿¡æ¯
        initial_time_slot = reset_info.get('time_slot', 0)
        current_time_slot = initial_time_slot
        request_id = reset_info.get('request_id')
        last_time_slot = current_time_slot

        # è·å–åˆå§‹ mask å’Œ info
        action_mask = reset_info.get('action_mask')
        blacklist_info = reset_info.get('blacklist_info', {})
        unconnected_dests = self._get_current_destinations()

        # Episode çŠ¶æ€
        done = False
        steps = 0
        decision_steps = 0
        episode_reward = 0

        # ğŸ”¥ Loss ç»Ÿè®¡å®¹å™¨
        episode_losses = []
        episode_high_losses = []
        episode_low_losses = []

        # DAgger ç»Ÿè®¡
        expert_steps = 0
        masked_expert_steps = 0

        # ç»éªŒå­˜å‚¨ç»Ÿè®¡
        stored_high_transitions = 0
        stored_low_transitions = 0

        # åˆå§‹åŒ– step_info
        step_info = {'success': False, 'request_completed': False}

        # ========================================
        # ğŸ”¥ æ£€æµ‹æ˜¯å¦ä½¿ç”¨ Coordinator
        # ========================================
        use_coordinator = (self.coordinator is not None)

        if use_coordinator:
            logger.debug(f"âœ… Episode {episode_idx}: ä½¿ç”¨ HRL Coordinator æ¨¡å¼")
        else:
            logger.debug(f"âš ï¸ Episode {episode_idx}: ä½¿ç”¨å›é€€æ¨¡å¼ï¼ˆç›´æ¥è°ƒç”¨ env.stepï¼‰")

        # ========================================
        # ä¸»å¾ªç¯
        # ========================================
        while not done and steps < max_steps:

            # ============================================================
            # ğŸ”¥ğŸ”¥ğŸ”¥ æ–¹æ¡ˆ A: ä½¿ç”¨ HRL Coordinator
            # ============================================================
            if use_coordinator:
                try:
                    # Coordinator è‡ªåŠ¨ç®¡ç†é«˜ä½å±‚äº¤äº’
                    next_state, reward, done, truncated, step_info = self.coordinator.step()

                    # ä» Coordinator è·å–æ‰§è¡Œçš„åŠ¨ä½œä¿¡æ¯
                    if hasattr(self.coordinator, 'last_transition'):
                        transition = self.coordinator.last_transition
                        if transition and len(transition) == 5:
                            trans_state, low_action, trans_reward, trans_next_state, trans_done = transition

                            # å­˜å‚¨ä½å±‚ç»éªŒ
                            self.agent.store_transition_low(
                                trans_state, low_action, trans_reward, trans_next_state, trans_done
                            )
                            stored_low_transitions += 1

                    # å¦‚æœ Coordinator è§¦å‘äº†é«˜å±‚å†³ç­–ï¼Œå¯èƒ½éœ€è¦å•ç‹¬å­˜å‚¨
                    if hasattr(self.coordinator, 'last_high_action'):
                        high_action = self.coordinator.last_high_action
                        if high_action is not None and unconnected_dests:
                            goal = unconnected_dests[high_action] if high_action < len(unconnected_dests) else -1
                            if goal != -1:
                                self.agent.store_transition_high(
                                    state, goal, reward, next_state, done or truncated
                                )
                                stored_high_transitions += 1

                    # æ›´æ–°çŠ¶æ€
                    state = next_state
                    episode_reward += reward
                    steps += 1

                    # æ›´æ–°æ—¶é—´æ§½ä¿¡æ¯
                    new_time_slot = step_info.get('time_slot', current_time_slot)
                    new_decision_steps = step_info.get('decision_steps', decision_steps)

                    if self.use_timeslot and new_time_slot != last_time_slot:
                        if self.log_timeslot_jumps:
                            logger.debug(f"â° [Ep {episode_idx}] Time Slot: {last_time_slot} â†’ {new_time_slot}")
                        self.timeslot_stats['timeslot_jumps'].append((last_time_slot, new_time_slot))
                        last_time_slot = new_time_slot

                    current_time_slot = new_time_slot
                    decision_steps = new_decision_steps

                    # æ›´æ–°ç›®æ ‡ä¿¡æ¯
                    unconnected_dests = self._get_current_destinations()

                except Exception as e:
                    logger.error(f"âŒ Coordinator.step å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
                    # å‘ç”Ÿé”™è¯¯æ—¶ç»ˆæ­¢ episode
                    break

            # ============================================================
            # ğŸ”¥ğŸ”¥ğŸ”¥ æ–¹æ¡ˆ B: å›é€€æ¨¡å¼ï¼ˆæ—  Coordinatorï¼‰
            # ============================================================
            else:
                # ----------------------------------------
                # 1. æå– Action Mask
                # ----------------------------------------
                action_mask = None

                # æ–¹å¼1: ä»PyG Dataå¯¹è±¡ä¸­æå–
                if hasattr(state, 'action_mask'):
                    action_mask = state.action_mask
                    if hasattr(action_mask, 'cpu'):
                        action_mask = action_mask.cpu().numpy()
                    if action_mask.ndim > 1:
                        action_mask = action_mask.squeeze()

                # æ–¹å¼2: ä»step_infoä¸­æå–
                elif 'action_mask' in step_info:
                    action_mask = step_info['action_mask']

                # æ–¹å¼3: ç›´æ¥è°ƒç”¨ç¯å¢ƒæ–¹æ³•
                if action_mask is None and hasattr(self.env, 'get_low_level_action_mask'):
                    action_mask = self.env.get_low_level_action_mask()

                # ğŸ”¥ ç¡®ä¿maskæ˜¯numpyæ•°ç»„
                if action_mask is not None:
                    if hasattr(action_mask, 'numpy'):
                        action_mask = action_mask.numpy()
                    if isinstance(action_mask, list):
                        action_mask = np.array(action_mask)

                # ----------------------------------------
                # 2. DAgger é€»è¾‘
                # ----------------------------------------
                beta = self.beta
                use_dagger = self.use_dagger
                use_expert = False
                expert_action = None

                if use_dagger and random.random() < beta:
                    expert_suggestion = self._get_expert_action(state)
                    if action_mask is None:
                        use_expert = True
                        expert_action = expert_suggestion
                    else:
                        valid_actions = np.where(action_mask > 0)[0]
                        if expert_suggestion in valid_actions:
                            use_expert = True
                            expert_action = expert_suggestion
                            expert_steps += 1
                        else:
                            masked_expert_steps += 1

                # ----------------------------------------
                # 3. Agent é€‰æ‹©åŠ¨ä½œ
                # ----------------------------------------
                high_action, low_action, action_info = self.agent.select_action(
                    state=state,
                    unconnected_dests=unconnected_dests,
                    action_mask=action_mask,
                    use_expert=use_expert,
                    expert_action=expert_action,
                    blacklist_info=blacklist_info
                )

                # ğŸ›¡ï¸ é˜²å¾¡ï¼šå¦‚æœ Agent è¿”å› -1 (æ— æ•ˆ)ï¼Œç»ˆæ­¢ episode
                if low_action == -1:
                    logger.warning(f"âš ï¸ Agent returned -1 (No Valid Actions). Terminating Episode {episode_idx}.")
                    return episode_reward, {
                        'success': False,
                        'blocking_rate': 1.0,
                        'message': 'no_valid_actions',
                        'time_slot': current_time_slot,
                        'decision_steps': decision_steps,
                        'time_slots_covered': current_time_slot - initial_time_slot,
                        'avg_loss': 0.0,
                        'avg_high_loss': 0.0,
                        'avg_low_loss': 0.0
                    }

                # ----------------------------------------
                # 4. æ‰§è¡ŒåŠ¨ä½œ
                # ----------------------------------------
                step_result = self.env.step(low_action)

                # è§£åŒ…ç»“æœ
                if len(step_result) == 5:
                    next_state, reward, done, truncated, step_info = step_result
                else:
                    next_state, reward, done, step_info = step_result
                    truncated = False

                # ----------------------------------------
                # 5. ğŸ”¥ æ£€æµ‹ need_high_level ä¿¡å·
                # ----------------------------------------
                if truncated and step_info.get('need_high_level', False):
                    error_type = step_info.get('error', 'unknown')
                    logger.info(f"âš ï¸ [Episode {episode_idx}] ä½å±‚æ£€æµ‹åˆ°é—®é¢˜: {error_type}")
                    logger.info(f"   â†’ è¿”å›é«˜å±‚é‡æ–°å†³ç­–ï¼ˆä¸ç»ˆæ­¢episodeï¼‰")

                    # è®°å½•å¥–åŠ±
                    episode_reward += reward

                    # é‡ç½®agentåˆ†æ”¯çŠ¶æ€ï¼ˆå¼ºåˆ¶è§¦å‘é«˜å±‚å†³ç­–ï¼‰
                    if hasattr(self.agent, 'current_branch_id'):
                        self.agent.current_branch_id = None
                    if hasattr(self.agent, 'subgoal_steps'):
                        self.agent.subgoal_steps = 999
                    if hasattr(self.agent, 'current_subgoal'):
                        self.agent.current_subgoal = None

                    # å­˜å‚¨ç»éªŒï¼ˆå¤±è´¥çš„å°è¯•ä¹Ÿè¦å­¦ä¹ ï¼‰
                    if action_info.get('high_level_decision', False):
                        goal = unconnected_dests[high_action] if unconnected_dests and high_action < len(
                            unconnected_dests) else -1
                        if goal != -1:
                            self.agent.store_transition_high(state, goal, reward, next_state, False)
                            stored_high_transitions += 1

                    self.agent.store_transition_low(state, low_action, reward, next_state, False)
                    stored_low_transitions += 1

                    # æ›´æ–°çŠ¶æ€
                    state = next_state
                    unconnected_dests = self._get_current_destinations()
                    steps += 1

                    # ç»§ç»­å¾ªç¯ï¼ˆä¸ç»ˆæ­¢episodeï¼‰
                    continue

                # ----------------------------------------
                # 6. æ›´æ–°æ—¶é—´æ§½ä¿¡æ¯
                # ----------------------------------------
                new_time_slot = step_info.get('time_slot', current_time_slot)
                new_decision_steps = step_info.get('decision_steps', decision_steps)

                if self.use_timeslot and new_time_slot != last_time_slot:
                    if self.log_timeslot_jumps:
                        logger.debug(f"â° [Ep {episode_idx}] Time Slot: {last_time_slot} â†’ {new_time_slot}")
                    self.timeslot_stats['timeslot_jumps'].append((last_time_slot, new_time_slot))
                    last_time_slot = new_time_slot

                current_time_slot = new_time_slot
                decision_steps = new_decision_steps

                # ----------------------------------------
                # 7. è®°å½•å¤±è´¥åŸå› ï¼ˆé»‘åå•å­¦ä¹ ï¼‰
                # ----------------------------------------
                if not step_info.get('success', True):
                    reason = step_info.get('message', 'unknown')
                    if "èµ„æºä¸è¶³" in reason or "è®¿é—®è¶…é™" in reason:
                        self.agent.record_failure(low_action, reason)

                # ----------------------------------------
                # 8. å­˜å‚¨ç»éªŒ
                # ----------------------------------------
                # High-Level Buffer
                if action_info.get('high_level_decision', False):
                    goal = unconnected_dests[high_action] if unconnected_dests and high_action < len(
                        unconnected_dests) else -1
                    if goal != -1:
                        self.agent.store_transition_high(state, goal, reward, next_state, done or truncated)
                        stored_high_transitions += 1

                # Low-Level Buffer
                self.agent.store_transition_low(state, low_action, reward, next_state, done or truncated)
                stored_low_transitions += 1

                # ----------------------------------------
                # 9. æ›´æ–°çŠ¶æ€
                # ----------------------------------------
                state = next_state
                action_mask = step_info.get('action_mask')
                blacklist_info = step_info.get('blacklist_info', {})
                unconnected_dests = self._get_current_destinations()
                episode_reward += reward
                steps += 1

                if truncated:
                    done = True

            # ============================================================
            # ğŸ”¥ å®šæœŸæ›´æ–°ç½‘ç»œï¼ˆé€‚ç”¨äºä¸¤ç§æ¨¡å¼ï¼‰
            # ============================================================
            if steps % self.update_frequency == 0:
                # ç¡®ä¿ç»éªŒç¼“å†²åŒºæœ‰è¶³å¤Ÿæ•°æ®
                has_enough_low_exp = len(self.agent.low_memory) >= self.agent.batch_size

                if has_enough_low_exp:
                    # è°ƒç”¨æ›´æ–°å¹¶è·å–è¯¦ç»†çš„æŸå¤±ä¿¡æ¯
                    loss_dict = self.agent.update_policies()

                    if loss_dict:
                        # è®°å½•å„ç§æŸå¤±
                        high_loss = loss_dict.get('high_loss', 0.0)
                        low_loss = loss_dict.get('low_loss', 0.0)
                        total_loss = loss_dict.get('total_loss', 0.0)

                        # åªè®°å½•éé›¶çš„æŸå¤±
                        if high_loss > 0:
                            episode_high_losses.append(high_loss)
                        if low_loss > 0:
                            episode_low_losses.append(low_loss)
                        if total_loss > 0:
                            episode_losses.append(total_loss)

                        self.total_updates += 1

                        # å®šæœŸæ‰“å°æ›´æ–°ä¿¡æ¯
                        if self.total_updates % 100 == 0:
                            logger.debug(
                                f"ğŸ”„ Update #{self.total_updates}: HighLoss={high_loss:.6f}, LowLoss={low_loss:.6f}")

                # å¦‚æœç»éªŒä¸è¶³ï¼Œæ‰“å°è­¦å‘Š
                elif self.total_updates < 10 and steps > 50:
                    logger.debug(f"âš ï¸ ç»éªŒä¸è¶³: High={len(self.agent.high_memory)}, Low={len(self.agent.low_memory)}")

        # ========================================
        # Episode ç»“æŸå¤„ç†
        # ========================================
        # åˆ¤æ–­æˆåŠŸä¸å¦
        is_success = step_info.get('request_success', None)
        if is_success is None:
            is_success = step_info.get('request_completed', False) or step_info.get('success', False)

        # æ£€æŸ¥ç¯å¢ƒæ˜¯å¦å·²å½’æ¡£
        env_already_archived = False
        if hasattr(self.env, 'current_request'):
            env_already_archived = (self.env.current_request is None)

        # å¦‚æœç¯å¢ƒæœªå½’æ¡£ï¼Œæ‰§è¡Œå½’æ¡£
        if not env_already_archived:
            if hasattr(self.env, 'current_request') and self.env.current_request:
                req_id = self.env.current_request.get('id', '?')
                if not is_success:
                    logger.info(f"ğŸ”„ [Episodeæ¸…ç†] è¯·æ±‚ {req_id} å¤±è´¥ï¼Œæ‰§è¡Œå›æ»š...")
                    self.env._archive_request(success=False)
                else:
                    logger.info(f"âœ… [Episodeæ¸…ç†] è¯·æ±‚ {req_id} æˆåŠŸï¼Œå½’æ¡£èµ„æº...")
                    self.env._archive_request(success=True)

                # æ¸…ç†ç¯å¢ƒçŠ¶æ€
                self.env.current_request = None
                self.env.current_branch_id = None
                self.env.current_tree = {}
                self.env.nodes_on_tree = set()
                self.env.branch_states = {}
                if hasattr(self.env, 'curr_ep_node_allocs'):
                    self.env.curr_ep_node_allocs = []
                if hasattr(self.env, 'curr_ep_link_allocs'):
                    self.env.curr_ep_link_allocs = []
        else:
            logger.info(f"â„¹ï¸ [Episodeæ¸…ç†] ç¯å¢ƒå·²å½’æ¡£ï¼Œè·³è¿‡Trainerå½’æ¡£")

        # ========================================
        # æ„å»º Episode Info
        # ========================================
        # è®¡ç®—å¹³å‡ Loss
        avg_loss = np.mean(episode_losses) if episode_losses else 0.0
        avg_high_loss = np.mean(episode_high_losses) if episode_high_losses else 0.0
        avg_low_loss = np.mean(episode_low_losses) if episode_low_losses else 0.0

        episode_info = {
            'steps': steps,
            'success': is_success,
            'blocking_rate': 0.0 if is_success else 1.0,
            'expert_usage': expert_steps / steps if steps > 0 else 0,
            'masked_expert': masked_expert_steps,
            'stored_high': stored_high_transitions,
            'stored_low': stored_low_transitions,
            'avg_loss': avg_loss,
            'avg_high_loss': avg_high_loss,
            'avg_low_loss': avg_low_loss,

            # æ—¶é—´æ§½ä¿¡æ¯
            'current_time_slot': current_time_slot,
            'initial_time_slot': initial_time_slot,
            'time_slots_covered': current_time_slot - initial_time_slot,
            'decision_steps': decision_steps,
            'request_id': request_id,
            'requests_processed': 1,

            # ğŸ”¥ æ–°å¢ï¼šæ ‡è®°ä½¿ç”¨çš„æ¨¡å¼
            'used_coordinator': use_coordinator
        }

        # æ›´æ–°æ—¶é—´æ§½ç»Ÿè®¡
        if self.use_timeslot:
            self.timeslot_stats['total_time_slots'] += (current_time_slot - initial_time_slot)
            self.timeslot_stats['total_decision_steps'] += decision_steps

        # ========================================
        # æ‰“å°æ—¥å¿—
        # ========================================
        status_icon = "âœ…" if is_success else "âŒ"
        mode_icon = "ğŸ¤–" if use_coordinator else "ğŸ”§"

        if is_success or episode_idx % 10 == 0:
            logger.info(
                f"{mode_icon} Ep {episode_idx} | {status_icon} | "
                f"Rw: {episode_reward:.1f} | "
                f"Steps: {steps} | "
                f"HiLoss: {avg_high_loss:.4f} | "
                f"LoLoss: {avg_low_loss:.4f} | "
                f"TS: {current_time_slot} | "
                f"DS: {decision_steps}"
            )

            # è°ƒè¯•ï¼šæ‰“å°ç»éªŒå­˜å‚¨æƒ…å†µ
            if stored_low_transitions == 0:
                logger.warning(f"âš ï¸ Episode {episode_idx}: æ²¡æœ‰å­˜å‚¨ä»»ä½•Low-Levelç»éªŒ!")

        return episode_reward, episode_info
    def _get_current_destinations(self):
        """è·å–å½“å‰æœªè¿æ¥çš„ç›®çš„åœ°åˆ—è¡¨"""
        if not hasattr(self.env, 'current_request') or self.env.current_request is None:
            return []
        all_dests = self.env.current_request.get('dest', [])
        connected = self.env.current_tree.get('connected_dests', set())
        return [d for d in all_dests if d not in connected]
    def _get_expert_action(self, state):
        """è·å–ä¸“å®¶åŠ¨ä½œ"""
        if not hasattr(self, 'agent') or not hasattr(self.agent, 'expert'):
            # å¦‚æœæ²¡æœ‰ Expert Wrapperï¼Œå°è¯•ç”¨ç¯å¢ƒé‡Œçš„
            if hasattr(self.env, 'expert') and self.env.expert:
                # è¿™é‡Œéœ€è¦ expert é€»è¾‘ï¼Œæš‚æ—¶éšæœºå…œåº•
                pass
        return random.randint(0, getattr(self.env, 'n', 28) - 1)

