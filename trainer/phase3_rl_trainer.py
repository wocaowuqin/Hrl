
# core/trainer/phase3_rl_trainer.py
# !/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Phase 3 RL Trainer - Goal-Conditioned HRL + DAgger + ğŸ”¥ æ—¶é—´æ§½ç³»ç»Ÿ
===============================================================================
ä¿®å¤å†…å®¹ï¼š
1. âœ… ç»Ÿè®¡é€»è¾‘ï¼šæ”¹ä¸º"å…¨å±€ç´¯è®¡å¹³å‡"ï¼Œä¿®å¤ Acc=1% çš„æ˜¾ç¤ºé—®é¢˜ã€‚
2. ğŸ›¡ï¸ å´©æºƒä¿æŠ¤ï¼šæ•è· Agent å†…éƒ¨é”™è¯¯ï¼Œé˜²æ­¢è®­ç»ƒä¸­æ–­ã€‚
3. ğŸ“Š è¿›åº¦æ¡ï¼šæ˜¾ç¤ºçœŸå®ç´¯è®¡ Acc (æ¥çº³ç‡) å’Œ Blk (é˜»å¡ç‡)ã€‚
4. ğŸ”¥ æ—¶é—´æ§½ç³»ç»Ÿï¼šæ”¯æŒç¦»æ•£æ—¶é—´æ¨¡æ‹Ÿã€æ‰¹é‡è¯·æ±‚å¤„ç†ã€èµ„æºè‡ªåŠ¨é‡Šæ”¾
5. ğŸ”§ ä¿®å¤Lossä¸º0çš„é—®é¢˜ï¼šç¡®ä¿ç½‘ç»œæ›´æ–°å’Œæ¢¯åº¦å›ä¼ æ­£å¸¸è¿›è¡Œ
===============================================================================
"""

import logging
import numpy as np
import random
import pickle
from pathlib import Path
from tqdm import tqdm
from torch.utils.tensorboard import SummaryWriter
import torch
from utils.visualizer import SFCVisualizer

logger = logging.getLogger(__name__)


class Phase3RLTrainer:
    """Phase 3: Goal-Conditioned RL Trainer with DAgger + Time Slot System"""

    def __init__(self, env, agent, output_dir, config):
        self.env = env
        self.agent = agent
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.cfg = config

        # ğŸ”¥ åˆå§‹åŒ–å¯è§†åŒ–å™¨
        self.visualizer = None
        if hasattr(env, 'topo'):
            try:
                self.visualizer = SFCVisualizer(env.topo, output_dir)
                logger.info("ğŸ¨ å¯è§†åŒ–å™¨å·²å°±ç»ª (plots å°†ä¿å­˜åœ¨ outputs/checkpoints/plots)")
            except Exception as e:
                logger.warning(f"âš ï¸ å¯è§†åŒ–å™¨åˆå§‹åŒ–å¤±è´¥: {e}")

        phase3_cfg = config.get("phase3", {})
        self.max_episodes = phase3_cfg.get("episodes", 1000)
        self.save_freq = phase3_cfg.get("save_every", 100)

        # ğŸ”§ æ–°å¢ï¼šè®­ç»ƒå‚æ•°
        self.max_steps_per_episode = phase3_cfg.get("max_steps", 600)
        self.update_frequency = phase3_cfg.get("update_frequency", 4)  # æ¯4æ­¥æ›´æ–°ä¸€æ¬¡
        self.warmup_steps = phase3_cfg.get("warmup_steps", 100)  # é¢„çƒ­æ­¥æ•°

        # 1. Epsilon é…ç½®
        epsilon_cfg = phase3_cfg.get("epsilon", {})
        self.epsilon_initial = epsilon_cfg.get("initial", 0.7)
        self.epsilon_final = epsilon_cfg.get("final", 0.01)
        self.epsilon_decay_steps = epsilon_cfg.get("decay_steps", 5000)

        # 2. DAgger é…ç½®
        dagger_cfg = phase3_cfg.get("dagger", {})
        self.use_dagger = dagger_cfg.get("enabled", True)
        self.beta = dagger_cfg.get("initial_beta", 0.8)
        self.beta_final = dagger_cfg.get("final_beta", 0.05)
        self.beta_decay_steps = dagger_cfg.get("decay_steps", 10000)

        # ğŸ”¥ æ–°å¢ï¼šæ—¶é—´æ§½ç³»ç»Ÿé…ç½®
        timeslot_cfg = phase3_cfg.get("timeslot", {})
        self.use_timeslot = timeslot_cfg.get("enabled", True)
        self.log_timeslot_info = timeslot_cfg.get("log_timeslot_info", True)
        self.log_timeslot_jumps = timeslot_cfg.get("log_jumps", True)

        # TensorBoard
        self.writer = SummaryWriter(log_dir=str(self.output_dir / "runs"))

        # ç»Ÿè®¡ä¿¡æ¯å®¹å™¨
        self.stats = {
            "rewards": [],
            "acceptance_rates": [],
            "blocking_rates": [],
            "resource_levels": [],
            "subgoal_completion_rate": [],
            "time_slots_covered": [],
            "decision_steps": [],
            "requests_per_episode": [],
            "losses": [],
            "high_losses": [],
            "low_losses": []

        }
        self.global_step = 0
        self.total_updates = 0  # ğŸ”§ æ–°å¢ï¼šæ€»æ›´æ–°æ¬¡æ•°

        # ğŸ”¥ æ–°å¢ï¼šæ—¶é—´æ§½ç›¸å…³ç»Ÿè®¡
        self.timeslot_stats = {
            'total_time_slots': 0,
            'total_decision_steps': 0,
            'avg_steps_per_request': 0,
            'timeslot_jumps': []
        }

    def _get_network_resource_level(self):
        """
        ğŸ”¥ [V10.17 ä¿®å¤ç‰ˆ] åŠ¨æ€è·å–çœŸå®å®¹é‡ï¼Œä¸å†å†™æ­» 100.0
        """
        try:
            rm = self.env.resource_mgr
            # è·å– DC èŠ‚ç‚¹åˆ—è¡¨
            dc_nodes = getattr(self.env, 'dc_nodes', [])

            if not dc_nodes:
                return 0.0

            total_dc_cpu = 0.0
            total_dc_cap = 0.0

            # 1. å°è¯•è·å–æ€»å®¹é‡åŸºå‡† (ä¼˜å…ˆç”¨ ResourceManager é‡Œçš„ C_cap)
            # è¿™æ˜¯ä¸€ä¸ªä¿é™©é€»è¾‘ï¼šçœ‹çœ‹ rm.C_cap æ˜¯æ•°ç»„è¿˜æ˜¯æ•°å­—
            c_cap_ref = getattr(rm, 'C_cap', 100.0)

            # éå†æ‰€æœ‰ DC èŠ‚ç‚¹
            for node in dc_nodes:
                # --- è·å–å½“å‰å‰©ä½™é‡ (åˆ†å­) ---
                current_cpu = 0.0
                if isinstance(rm.nodes, dict) and 'cpu' in rm.nodes:
                    if node < len(rm.nodes['cpu']):
                        current_cpu = rm.nodes['cpu'][node]
                elif isinstance(rm.nodes, list):
                    if node < len(rm.nodes):
                        current_cpu = rm.nodes[node].get('cpu', 0)

                # --- è·å–è¯¥èŠ‚ç‚¹æ€»å®¹é‡ (åˆ†æ¯) ---
                # ğŸ”¥ğŸ”¥ğŸ”¥ ä¹‹å‰è¿™é‡Œå†™æ­»æˆäº† total_dc_cap += 100.0ï¼Œè¿™å°±æ˜¯ 150% çš„ç½ªé­ç¥¸é¦–ï¼
                node_cap = 100.0  # é»˜è®¤å…œåº•

                if hasattr(c_cap_ref, '__getitem__'):  # å¦‚æœ C_cap æ˜¯æ•°ç»„ [30, 55, 40...]
                    if node < len(c_cap_ref):
                        node_cap = float(c_cap_ref[node])
                elif isinstance(c_cap_ref, (int, float)):  # å¦‚æœ C_cap æ˜¯æ ‡é‡ 100.0
                    node_cap = float(c_cap_ref)

                # ç´¯åŠ 
                total_dc_cpu += current_cpu
                total_dc_cap += node_cap

            # é˜²æ­¢é™¤ä»¥é›¶
            if total_dc_cap <= 0: return 0.0

            # è®¡ç®—ç™¾åˆ†æ¯”
            dc_res_pct = (total_dc_cpu / total_dc_cap) * 100.0

            # å†æ¬¡ä¿é™©ï¼šå¦‚æœç®—å‡ºæ¥å¤§äº 100ï¼Œå¼ºè¡Œä¿®æ­£ (è¯´æ˜ C_cap æ²¡å–å¯¹)
            if dc_res_pct > 100.0:
                # print(f"âš ï¸ èµ„æºæ˜¾ç¤ºå¼‚å¸¸: {dc_res_pct:.1f}% (åˆ†å­{total_dc_cpu}/åˆ†æ¯{total_dc_cap})")
                return 100.0

            return dc_res_pct

        except Exception as e:
            # print(f"èµ„æºç›‘æ§å‡ºé”™: {e}")
            return 0.0

    def load_timeslot_data(self):
        """
        ğŸ”¥ æ–°å¢ï¼šåŠ è½½æ—¶é—´æ§½æ•°æ®
        """
        if not self.use_timeslot:
            logger.info("âš ï¸ æ—¶é—´æ§½ç³»ç»Ÿæœªå¯ç”¨ï¼Œè·³è¿‡æ•°æ®åŠ è½½")
            return False

        try:
            # è·å–æ•°æ®è·¯å¾„
            path_cfg = self.cfg.get('path', {})
            input_dir = Path(path_cfg.get('input_dir', 'data/input_dir'))

            # æ–‡ä»¶å
            requests_file = input_dir / path_cfg.get('requests_file', 'phase3_requests.pkl')
            requests_by_slot_file = input_dir / path_cfg.get('requests_by_slot_file', 'phase3_requests_by_slot.pkl')

            logger.info(f"\n{'=' * 60}")
            logger.info(f"ğŸ”¥ åŠ è½½æ—¶é—´æ§½æ•°æ®")
            logger.info(f"{'=' * 60}")
            logger.info(f"è¯·æ±‚æ–‡ä»¶: {requests_file}")
            logger.info(f"æ—¶é—´æ§½æ–‡ä»¶: {requests_by_slot_file}")

            # åŠ è½½æ•°æ®
            with open(requests_file, 'rb') as f:
                requests = pickle.load(f)

            with open(requests_by_slot_file, 'rb') as f:
                requests_by_slot = pickle.load(f)

            # åŠ è½½åˆ°ç¯å¢ƒ
            if hasattr(self.env, 'load_requests'):
                self.env.load_requests(requests, requests_by_slot)
                logger.info(f"âœ… æ—¶é—´æ§½æ•°æ®åŠ è½½æˆåŠŸ")
                logger.info(f"   æ€»è¯·æ±‚æ•°: {len(requests)}")
                logger.info(f"   æ—¶é—´æ§½æ•°: {len(requests_by_slot)}")
                logger.info(f"{'=' * 60}\n")
                return True
            else:
                logger.warning("âš ï¸ ç¯å¢ƒä¸æ”¯æŒ load_requests() æ–¹æ³•")
                return False

        except FileNotFoundError as e:
            logger.error(f"âŒ æ—¶é—´æ§½æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {e}")
            logger.info("æç¤º: è¯·å…ˆè¿è¡Œæ•°æ®ç”Ÿæˆè„šæœ¬:")
            logger.info("  python main_generate_time_slot.py")
            logger.info("  python generate_event_time_slot.py")
            return False
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ—¶é—´æ§½æ•°æ®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

    def run(self):
        """è¿è¡Œè®­ç»ƒä¸»å¾ªç¯"""
        logger.info(f"ğŸš€ Starting Training: DAgger={self.use_dagger}, Beta={self.beta}")
        logger.info(
            f"ğŸ“Š è®­ç»ƒå‚æ•°: episodes={self.max_episodes}, warmup={self.warmup_steps}, update_freq={self.update_frequency}")

        # ğŸ”¥ åŠ è½½æ—¶é—´æ§½æ•°æ®
        if self.use_timeslot:
            if not self.load_timeslot_data():
                logger.error("âŒ æ—¶é—´æ§½æ•°æ®åŠ è½½å¤±è´¥ï¼Œé€€å‡ºè®­ç»ƒ")
                return

        # ============================================
        # ğŸ”¥ å…¨å±€ç´¯è®¡è®¡æ•°å™¨ (ä¿®å¤ Acc æ˜¾ç¤ºé—®é¢˜)
        # ============================================
        total_episodes = 0
        total_success = 0
        total_failed = 0

        pbar = tqdm(range(self.max_episodes), desc="RL Training")

        for ep in pbar:
            try:
                # è¿è¡Œä¸€ä¸ª Episode
                ep_reward, ep_info = self._run_episode(ep)

                # 1. è·å–èµ„æºæ°´å¹³
                curr_res_level = self._get_network_resource_level()

                # 2. âœ… æ›´æ–°å…¨å±€è®¡æ•°å™¨ (æ ¸å¿ƒä¿®å¤)
                total_episodes += 1

                # åˆ¤æ–­æˆåŠŸæ ‡å‡†ï¼šåªè¦ env è¯´æ˜¯ success æˆ– request_completed å°±ç®—æˆ
                is_success = ep_info.get('success', False)

                if is_success:
                    total_success += 1
                else:
                    total_failed += 1

                # 3. è®¡ç®—ç´¯è®¡æŒ‡æ ‡
                cum_acc = total_success / total_episodes if total_episodes > 0 else 0.0
                cum_blk = total_failed / total_episodes if total_episodes > 0 else 0.0

                # 4. è®°å½•åˆ° Stats (ç”¨äºç»˜å›¾)
                self.stats["rewards"].append(ep_reward)
                self.stats["acceptance_rates"].append(1.0 if is_success else 0.0)
                self.stats["blocking_rates"].append(0.0 if is_success else 1.0)
                self.stats["resource_levels"].append(curr_res_level)

                # ğŸ”¥ [æ–°å¢] è®°å½• Loss
                avg_loss = ep_info.get('avg_loss', 0.0)
                avg_high_loss = ep_info.get('avg_high_loss', 0.0)
                avg_low_loss = ep_info.get('avg_low_loss', 0.0)

                self.stats["losses"].append(avg_loss)
                self.stats["high_losses"].append(avg_high_loss)
                self.stats["low_losses"].append(avg_low_loss)

                # ğŸ”¥ æ–°å¢ï¼šæ—¶é—´æ§½ç»Ÿè®¡
                if self.use_timeslot:
                    self.stats["time_slots_covered"].append(ep_info.get('time_slots_covered', 0))
                    self.stats["decision_steps"].append(ep_info.get('decision_steps', 0))
                    self.stats["requests_per_episode"].append(ep_info.get('requests_processed', 1))

                # 5. TensorBoard (è®°å½•ç´¯è®¡å€¼æ›´å¹³æ»‘)
                self.writer.add_scalar("Train/Reward", ep_reward, ep)
                self.writer.add_scalar("Train/CumulativeAcc", cum_acc, ep)
                self.writer.add_scalar("Train/CumulativeBlk", cum_blk, ep)
                self.writer.add_scalar("Train/Resource", curr_res_level, ep)
                self.writer.add_scalar("Train/Loss", avg_loss, ep)
                self.writer.add_scalar("Train/HighLoss", avg_high_loss, ep)
                self.writer.add_scalar("Train/LowLoss", avg_low_loss, ep)

                # ğŸ”¥ æ–°å¢ï¼šæ—¶é—´æ§½æŒ‡æ ‡
                if self.use_timeslot:
                    self.writer.add_scalar("Train/TimeSlotsCovered", ep_info.get('time_slots_covered', 0), ep)
                    self.writer.add_scalar("Train/DecisionSteps", ep_info.get('decision_steps', 0), ep)
                    self.writer.add_scalar("Train/CurrentTimeSlot", ep_info.get('current_time_slot', 0), ep)

                if hasattr(self.agent, 'epsilon_low'):
                    self.writer.add_scalar("Train/Epsilon", self.agent.epsilon_low, ep)

                # 6. æ›´æ–°è¿›åº¦æ¡ (æ˜¾ç¤ºå…¨å±€ç´¯è®¡å€¼)
                expert_usage_pct = ep_info.get('expert_usage', 0) * 100

                # ğŸ”¥ æ„å»ºè¿›åº¦æ¡æ˜¾ç¤º
                postfix = {
                    "Rw": f"{ep_reward:.0f}",
                    "Exp": f"{expert_usage_pct:.0f}%",
                    "Acc": f"{cum_acc:.1%}",
                    "Blk": f"{cum_blk:.1%}",
                    "Res": f"{curr_res_level:.0f}%",
                    "Loss": f"{avg_loss:.4f}",
                    "HiLoss": f"{avg_high_loss:.4f}",
                    "LoLoss": f"{avg_low_loss:.4f}"
                }

                # ğŸ”¥ å¦‚æœå¯ç”¨æ—¶é—´æ§½ï¼Œæ·»åŠ æ—¶é—´æ§½ä¿¡æ¯
                if self.use_timeslot:
                    postfix["TS"] = ep_info.get('current_time_slot', 0)
                    postfix["DS"] = ep_info.get('decision_steps', 0)

                pbar.set_postfix(postfix)

                # 7. æ˜¾ç¤ºè®­ç»ƒçŠ¶æ€æ‘˜è¦
                if (ep + 1) % 50 == 0:
                    logger.info(f"\nğŸ“Š Episode {ep + 1} è®­ç»ƒçŠ¶æ€:")
                    logger.info(f"   ç´¯è®¡æ›´æ–°æ¬¡æ•°: {self.total_updates}")
                    logger.info(f"   ç»éªŒç¼“å†²åŒº: High={len(self.agent.high_memory)}, Low={len(self.agent.low_memory)}")
                    logger.info(f"   Loss: High={avg_high_loss:.6f}, Low={avg_low_loss:.6f}")

                # ä¿å­˜æ¨¡å‹
                if (ep + 1) % self.save_freq == 0:
                    self.agent.save(str(self.output_dir / f"rl_model_ep{ep + 1}.pth"))

                    # ğŸ”¥ æ‰“å°æ—¶é—´æ§½ç»Ÿè®¡
                    if self.use_timeslot and self.log_timeslot_info:
                        self._print_timeslot_stats(ep + 1)

            except Exception as e:
                # ğŸ›¡ï¸ å´©æºƒé˜²å¾¡ï¼šæ•è·æ‰€æœ‰å¼‚å¸¸ï¼Œä¸ä¸­æ–­è®­ç»ƒ
                logger.error(f"âŒ Episode {ep} CRASHED: {e}")
                import traceback
                traceback.print_exc()
                # å‘ç”Ÿå¼‚å¸¸ç®—ä½œå¤±è´¥
                total_episodes += 1
                total_failed += 1
                continue

        # è®­ç»ƒç»“æŸä¿å­˜
        self.agent.save(str(self.output_dir / "rl_model_final.pth"))
        logger.info(f"âœ… Training Complete. Final Acc: {total_success / total_episodes:.2%}")
        logger.info(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡: æ€»æ›´æ–°æ¬¡æ•°={self.total_updates}, å¹³å‡Loss={np.mean(self.stats['losses']):.6f}")

        # ğŸ”¥ æ‰“å°æœ€ç»ˆæ—¶é—´æ§½ç»Ÿè®¡
        if self.use_timeslot:
            self._print_final_timeslot_stats()

    def _run_episode(self, episode_idx: int):
        """è¿è¡Œä¸€ä¸ªepisodeï¼ˆé›†æˆé»‘åå• + DAgger + ğŸ”¥ æ—¶é—´æ§½ç³»ç»Ÿ + Lossç›‘æ§ï¼‰"""
        import numpy as np
        import random

        # ğŸ”§ æ–°å¢ï¼šé¢„çƒ­æ£€æŸ¥
        if self.agent.steps_done < self.warmup_steps:
            logger.debug(f"ğŸ”¥ é¢„çƒ­é˜¶æ®µ: {self.agent.steps_done}/{self.warmup_steps}")

        # è·å–æœ€å¤§æ­¥æ•°
        max_steps = self.max_steps_per_episode

        # âœ… é‡ç½®ç¯å¢ƒ
        reset_result = self.env.reset()
        if isinstance(reset_result, tuple) and len(reset_result) == 2:
            state, reset_info = reset_result
        else:
            state = reset_result
            reset_info = {}

        # ğŸ”¥ è·å–æ—¶é—´æ§½ä¿¡æ¯
        initial_time_slot = reset_info.get('time_slot', 0)
        current_time_slot = initial_time_slot
        request_id = reset_info.get('request_id')

        # ğŸ”¥ æ—¶é—´æ§½è·³è½¬æ£€æµ‹
        last_time_slot = current_time_slot

        # è·å– mask å’Œ info
        action_mask = reset_info.get('action_mask')
        blacklist_info = reset_info.get('blacklist_info', {})
        unconnected_dests = self._get_current_destinations()

        done = False
        steps = 0
        decision_steps = 0  # ğŸ”¥ å†³ç­–æ­¥æ•°ï¼ˆä¸æ˜¯æ—¶é—´ï¼ï¼‰
        episode_reward = 0

        # ğŸ”¥ Loss ç»Ÿè®¡å®¹å™¨
        episode_losses = []
        episode_high_losses = []
        episode_low_losses = []

        # DAgger ç»Ÿè®¡
        expert_steps = 0
        masked_expert_steps = 0

        # åˆå§‹åŒ– step_info
        step_info = {'success': False, 'request_completed': False}

        # ğŸ”§ æ–°å¢ï¼šç”¨äºç›‘æ§ç»éªŒå­˜å‚¨
        stored_high_transitions = 0
        stored_low_transitions = 0

        while not done and steps < max_steps:
            # DAgger é€»è¾‘
            beta = self.beta
            use_dagger = self.use_dagger
            use_expert = False
            expert_action = None

            # ğŸ”¥ğŸ”¥ğŸ”¥ å…³é”®ä¿®å¤ï¼šä» state ä¸­æå– action_mask ğŸ”¥ğŸ”¥ğŸ”¥
            action_mask = None

            # æ–¹å¼1: ä»PyG Dataå¯¹è±¡ä¸­æå–
            if hasattr(state, 'action_mask'):
                action_mask = state.action_mask
                if hasattr(action_mask, 'cpu'):
                    action_mask = action_mask.cpu().numpy()
                if action_mask.ndim > 1:
                    action_mask = action_mask.squeeze()

            # æ–¹å¼2: ä»step_infoä¸­æå–
            elif 'action_mask' in step_info:
                action_mask = step_info['action_mask']

            # æ–¹å¼3: ç›´æ¥è°ƒç”¨ç¯å¢ƒæ–¹æ³•
            if action_mask is None and hasattr(self.env, 'get_low_level_action_mask'):
                action_mask = self.env.get_low_level_action_mask()

            # ğŸ”¥ ç¡®ä¿maskæ˜¯numpyæ•°ç»„
            if action_mask is not None:
                if hasattr(action_mask, 'numpy'):
                    action_mask = action_mask.numpy()
                if isinstance(action_mask, list):
                    action_mask = np.array(action_mask)

            # ä¸“å®¶ä»‹å…¥åˆ¤æ–­
            if use_dagger and random.random() < beta:
                expert_suggestion = self._get_expert_action(state)
                if action_mask is None:
                    use_expert = True
                    expert_action = expert_suggestion
                else:
                    valid_actions = np.where(action_mask > 0)[0]
                    if expert_suggestion in valid_actions:
                        use_expert = True
                        expert_action = expert_suggestion
                        expert_steps += 1
                    else:
                        masked_expert_steps += 1

            # âœ… Agent é€‰æ‹©åŠ¨ä½œ
            high_action, low_action, action_info = self.agent.select_action(
                state=state,
                unconnected_dests=unconnected_dests,
                action_mask=action_mask,
                use_expert=use_expert,
                expert_action=expert_action,
                blacklist_info=blacklist_info
            )

            # ğŸ›¡ï¸ é˜²å¾¡ï¼šå¦‚æœ Agent è¿”å› -1 (æ— æ•ˆ)ï¼Œæ‰‹åŠ¨å¤„ç†
            if low_action == -1:
                logger.warning(f"âš ï¸ Agent returned -1 (No Valid Actions). Terminating Episode {episode_idx}.")
                return episode_reward, {
                    'success': False,
                    'blocking_rate': 1.0,
                    'message': 'no_valid_actions',
                    'time_slot': current_time_slot,
                    'decision_steps': decision_steps,
                    'time_slots_covered': current_time_slot - initial_time_slot,
                    'avg_loss': 0.0,
                    'avg_high_loss': 0.0,
                    'avg_low_loss': 0.0
                }

            # æ‰§è¡ŒåŠ¨ä½œ
            step_result = self.env.step(low_action)

            # è§£åŒ…ç»“æœ
            if len(step_result) == 5:
                next_state, reward, done, truncated, step_info = step_result
            else:
                next_state, reward, done, step_info = step_result
                truncated = False

            # ğŸ”¥ğŸ”¥ğŸ”¥ [V31.0 æ–°å¢] æ£€æµ‹ need_high_level ä¿¡å·
            # ============================================
            if truncated and step_info.get('need_high_level', False):
                error_type = step_info.get('error', 'unknown')
                logger.info(f"âš ï¸ [Episode {episode_idx}] ä½å±‚æ£€æµ‹åˆ°é—®é¢˜: {error_type}")
                logger.info(f"   â†’ è¿”å›é«˜å±‚é‡æ–°å†³ç­–ï¼ˆä¸ç»ˆæ­¢episodeï¼‰")

                # è®°å½•å¥–åŠ±
                episode_reward += reward

                # é‡ç½®agentåˆ†æ”¯çŠ¶æ€ï¼ˆå¼ºåˆ¶è§¦å‘é«˜å±‚å†³ç­–ï¼‰
                if hasattr(self.agent, 'current_branch_id'):
                    self.agent.current_branch_id = None
                if hasattr(self.agent, 'subgoal_steps'):
                    self.agent.subgoal_steps = 999
                if hasattr(self.agent, 'current_subgoal'):
                    self.agent.current_subgoal = None

                # å­˜å‚¨ç»éªŒï¼ˆå¤±è´¥çš„å°è¯•ä¹Ÿè¦å­¦ä¹ ï¼‰
                if action_info.get('high_level_decision', False):
                    goal = unconnected_dests[high_action] if unconnected_dests and high_action < len(
                        unconnected_dests) else -1
                    if goal != -1:
                        self.agent.store_transition_high(state, goal, reward, next_state, False)
                        stored_high_transitions += 1

                self.agent.store_transition_low(state, low_action, reward, next_state, False)
                stored_low_transitions += 1

                # æ›´æ–°çŠ¶æ€
                state = next_state
                unconnected_dests = self._get_current_destinations()
                steps += 1

                # ç»§ç»­å¾ªç¯ï¼ˆä¸ç»ˆæ­¢episodeï¼‰
                continue
            # ğŸ”¥ æ›´æ–°æ—¶é—´æ§½ä¿¡æ¯
            new_time_slot = step_info.get('time_slot', current_time_slot)
            new_decision_steps = step_info.get('decision_steps', decision_steps)

            # ğŸ”¥ æ£€æµ‹æ—¶é—´æ§½è·³è½¬
            if self.use_timeslot and new_time_slot != last_time_slot:
                if self.log_timeslot_jumps:
                    logger.debug(f"â° [Ep {episode_idx}] Time Slot: {last_time_slot} â†’ {new_time_slot}")
                self.timeslot_stats['timeslot_jumps'].append((last_time_slot, new_time_slot))
                last_time_slot = new_time_slot

            current_time_slot = new_time_slot
            decision_steps = new_decision_steps

            # è®°å½•å¤±è´¥åŸå› ç”¨äºé»‘åå•å­¦ä¹ 
            if not step_info.get('success', True):
                reason = step_info.get('message', 'unknown')
                if "èµ„æºä¸è¶³" in reason or "è®¿é—®è¶…é™" in reason:
                    self.agent.record_failure(low_action, reason)

            # ğŸ”§ ä¿®å¤ï¼šæ€»æ˜¯å­˜å‚¨ç»éªŒï¼Œæ— è®ºæ˜¯ä¸“å®¶è¿˜æ˜¯agentçš„é€‰æ‹©
            # High-Level Buffer
            if action_info.get('high_level_decision', False):
                goal = unconnected_dests[high_action] if unconnected_dests and high_action < len(
                    unconnected_dests) else -1
                if goal != -1:
                    self.agent.store_transition_high(state, goal, reward, next_state, done or truncated)
                    stored_high_transitions += 1

            # Low-Level Buffer
            self.agent.store_transition_low(state, low_action, reward, next_state, done or truncated)
            stored_low_transitions += 1

            # æ›´æ–°çŠ¶æ€
            state = next_state
            action_mask = step_info.get('action_mask')
            blacklist_info = step_info.get('blacklist_info', {})
            unconnected_dests = self._get_current_destinations()
            episode_reward += reward
            steps += 1

            # ğŸ”§ ä¿®å¤ï¼šå®šæœŸæ›´æ–°ç½‘ç»œï¼ˆç¡®ä¿æœ‰è¶³å¤Ÿç»éªŒï¼‰
            if steps % self.update_frequency == 0:
                # ğŸ”¥ ç¡®ä¿ç»éªŒç¼“å†²åŒºæœ‰è¶³å¤Ÿæ•°æ®
                has_enough_high_exp = len(self.agent.high_memory) >= self.agent.batch_size // 4
                has_enough_low_exp = len(self.agent.low_memory) >= self.agent.batch_size

                if has_enough_low_exp:
                    # ğŸ”§ è°ƒç”¨æ›´æ–°å¹¶è·å–è¯¦ç»†çš„æŸå¤±ä¿¡æ¯
                    loss_dict = self.agent.update_policies()

                    if loss_dict:
                        # è®°å½•å„ç§æŸå¤±
                        high_loss = loss_dict.get('high_loss', 0.0)
                        low_loss = loss_dict.get('low_loss', 0.0)
                        total_loss = loss_dict.get('total_loss', 0.0)

                        # åªè®°å½•éé›¶çš„æŸå¤±
                        if high_loss > 0:
                            episode_high_losses.append(high_loss)
                        if low_loss > 0:
                            episode_low_losses.append(low_loss)
                        if total_loss > 0:
                            episode_losses.append(total_loss)

                        self.total_updates += 1

                        # ğŸ”§ è°ƒè¯•ï¼šå®šæœŸæ‰“å°æ›´æ–°ä¿¡æ¯
                        if self.total_updates % 100 == 0:
                            logger.debug(
                                f"ğŸ”„ Update #{self.total_updates}: HighLoss={high_loss:.6f}, LowLoss={low_loss:.6f}")

                # ğŸ”§ å¦‚æœç»éªŒä¸è¶³ï¼Œæ‰“å°è­¦å‘Š
                elif self.total_updates < 10 and steps > 50:
                    logger.debug(f"âš ï¸ ç»éªŒä¸è¶³: High={len(self.agent.high_memory)}, Low={len(self.agent.low_memory)}")

            if truncated: done = True

        # ============================================================
        # ğŸ”¥ğŸ”¥ğŸ”¥ å…³é”®ä¿®å¤ï¼šEpisode ç»“æŸç»Ÿè®¡
        # ============================================================
        is_success = step_info.get('request_success', None)
        if is_success is None:
            is_success = step_info.get('request_completed', False) or step_info.get('success', False)

        env_already_archived = False
        if hasattr(self.env, 'current_request'):
            env_already_archived = (self.env.current_request is None)

        if not env_already_archived:
            if hasattr(self.env, 'current_request') and self.env.current_request:
                req_id = self.env.current_request.get('id', '?')
                if not is_success:
                    logger.info(f"ğŸ”„ [Episodeæ¸…ç†] è¯·æ±‚ {req_id} å¤±è´¥ï¼Œæ‰§è¡Œå›æ»š...")
                    self.env._archive_request(success=False)
                else:
                    logger.info(f"âœ… [Episodeæ¸…ç†] è¯·æ±‚ {req_id} æˆåŠŸï¼Œå½’æ¡£èµ„æº...")
                    self.env._archive_request(success=True)

                self.env.current_request = None
                self.env.current_branch_id = None
                self.env.current_tree = {}
                self.env.nodes_on_tree = set()
                self.env.branch_states = {}
                if hasattr(self.env, 'curr_ep_node_allocs'): self.env.curr_ep_node_allocs = []
                if hasattr(self.env, 'curr_ep_link_allocs'): self.env.curr_ep_link_allocs = []
        else:
            logger.info(f"â„¹ï¸ [Episodeæ¸…ç†] ç¯å¢ƒå·²å½’æ¡£ï¼Œè·³è¿‡Trainerå½’æ¡£")

        # ============================================================
        # ğŸ”¥ æ„å»ºå®Œæ•´çš„ episode_infoï¼ˆåŒ…å«æ—¶é—´æ§½ä¿¡æ¯å’ŒLossï¼‰
        # ============================================================

        # è®¡ç®—å¹³å‡ Loss
        avg_loss = np.mean(episode_losses) if episode_losses else 0.0
        avg_high_loss = np.mean(episode_high_losses) if episode_high_losses else 0.0
        avg_low_loss = np.mean(episode_low_losses) if episode_low_losses else 0.0

        episode_info = {
            'steps': steps,
            'success': is_success,
            'blocking_rate': 0.0 if is_success else 1.0,
            'expert_usage': expert_steps / steps if steps > 0 else 0,
            'masked_expert': masked_expert_steps,
            'stored_high': stored_high_transitions,
            'stored_low': stored_low_transitions,
            'avg_loss': avg_loss,
            'avg_high_loss': avg_high_loss,
            'avg_low_loss': avg_low_loss,

            # ğŸ”¥ æ—¶é—´æ§½ä¿¡æ¯
            'current_time_slot': current_time_slot,
            'initial_time_slot': initial_time_slot,
            'time_slots_covered': current_time_slot - initial_time_slot,
            'decision_steps': decision_steps,
            'request_id': request_id,
            'requests_processed': 1
        }

        # ğŸ”¥ æ›´æ–°æ—¶é—´æ§½ç»Ÿè®¡
        if self.use_timeslot:
            self.timeslot_stats['total_time_slots'] += (current_time_slot - initial_time_slot)
            self.timeslot_stats['total_decision_steps'] += decision_steps

        # ç®€å•æ—¥å¿—
        status_icon = "âœ…" if is_success else "âŒ"
        if is_success or episode_idx % 10 == 0:
            logger.info(
                f"Ep {episode_idx} | {status_icon} | "
                f"Rw: {episode_reward:.1f} | "
                f"Steps: {steps} | "
                f"HiLoss: {avg_high_loss:.4f} | "
                f"LoLoss: {avg_low_loss:.4f} | "
                f"TS: {current_time_slot} | "
                f"DS: {decision_steps}"
            )

            # ğŸ”§ è°ƒè¯•ï¼šæ‰“å°ç»éªŒå­˜å‚¨æƒ…å†µ
            if stored_low_transitions == 0:
                logger.warning(f"âš ï¸ Episode {episode_idx}: æ²¡æœ‰å­˜å‚¨ä»»ä½•Low-Levelç»éªŒ!")

        return episode_reward, episode_info

    def _get_current_destinations(self):
        """è·å–å½“å‰æœªè¿æ¥çš„ç›®çš„åœ°åˆ—è¡¨"""
        if not hasattr(self.env, 'current_request') or self.env.current_request is None:
            return []
        all_dests = self.env.current_request.get('dest', [])
        connected = self.env.current_tree.get('connected_dests', set())
        return [d for d in all_dests if d not in connected]

    def _get_expert_action(self, state):
        """è·å–ä¸“å®¶åŠ¨ä½œ"""
        if not hasattr(self, 'agent') or not hasattr(self.agent, 'expert'):
            # å¦‚æœæ²¡æœ‰ Expert Wrapperï¼Œå°è¯•ç”¨ç¯å¢ƒé‡Œçš„
            if hasattr(self.env, 'expert') and self.env.expert:
                # è¿™é‡Œéœ€è¦ expert é€»è¾‘ï¼Œæš‚æ—¶éšæœºå…œåº•
                pass
        return random.randint(0, getattr(self.env, 'n', 28) - 1)

    def _print_timeslot_stats(self, episode):
        """
        ğŸ”¥ æ–°å¢ï¼šæ‰“å°æ—¶é—´æ§½ç»Ÿè®¡ä¿¡æ¯
        """
        logger.info(f"\n{'=' * 60}")
        logger.info(f"â° æ—¶é—´æ§½ç»Ÿè®¡ @ Episode {episode}")
        logger.info(f"{'=' * 60}")

        if self.timeslot_stats['total_decision_steps'] > 0:
            avg_steps = (self.timeslot_stats['total_decision_steps'] /
                         max(1, len(self.stats['decision_steps'])))
            logger.info(f"å¹³å‡å†³ç­–æ­¥æ•°: {avg_steps:.1f}")

        if len(self.stats['time_slots_covered']) > 0:
            avg_slots = np.mean(self.stats['time_slots_covered'][-100:])
            logger.info(f"å¹³å‡æ—¶é—´æ§½è·¨åº¦: {avg_slots:.1f}")

        if len(self.timeslot_stats['timeslot_jumps']) > 0:
            logger.info(f"æ—¶é—´æ§½è·³è½¬æ¬¡æ•°: {len(self.timeslot_stats['timeslot_jumps'])}")

        logger.info(f"{'=' * 60}\n")

    def _print_final_timeslot_stats(self):
        """
        ğŸ”¥ æ–°å¢ï¼šæ‰“å°æœ€ç»ˆæ—¶é—´æ§½ç»Ÿè®¡
        """
        logger.info(f"\n{'=' * 60}")
        logger.info(f"ğŸ‰ æœ€ç»ˆæ—¶é—´æ§½ç»Ÿè®¡")
        logger.info(f"{'=' * 60}")

        total_episodes = len(self.stats['decision_steps'])

        if total_episodes > 0:
            avg_decision_steps = np.mean(self.stats['decision_steps'])
            avg_time_slots = np.mean(self.stats['time_slots_covered'])

            logger.info(f"æ€»Episodes: {total_episodes}")
            logger.info(f"å¹³å‡å†³ç­–æ­¥æ•°: {avg_decision_steps:.1f}")
            logger.info(f"å¹³å‡æ—¶é—´æ§½è·¨åº¦: {avg_time_slots:.1f}")
            logger.info(f"æ€»æ—¶é—´æ§½è·³è½¬: {len(self.timeslot_stats['timeslot_jumps'])}")

            if self.timeslot_stats['total_decision_steps'] > 0:
                efficiency = (self.timeslot_stats['total_time_slots'] /
                              self.timeslot_stats['total_decision_steps'])
                logger.info(f"æ—¶é—´æ§½æ•ˆç‡: {efficiency:.2f} (æ—¶é—´æ§½/å†³ç­–æ­¥)")

        logger.info(f"{'=' * 60}\n")

