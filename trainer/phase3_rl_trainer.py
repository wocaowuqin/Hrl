
# core/trainer/phase3_rl_trainer.py
# !/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Phase 3 RL Trainer - Goal-Conditioned HRL + DAgger + ğŸ”¥ æ—¶é—´æ§½ç³»ç»Ÿ
===============================================================================
ä¿®å¤å†…å®¹ï¼š
1. âœ… ç»Ÿè®¡é€»è¾‘ï¼šæ”¹ä¸º"å…¨å±€ç´¯è®¡å¹³å‡"ï¼Œä¿®å¤ Acc=1% çš„æ˜¾ç¤ºé—®é¢˜ã€‚
2. ğŸ›¡ï¸ å´©æºƒä¿æŠ¤ï¼šæ•è· Agent å†…éƒ¨é”™è¯¯ï¼Œé˜²æ­¢è®­ç»ƒä¸­æ–­ã€‚
3. ğŸ“Š è¿›åº¦æ¡ï¼šæ˜¾ç¤ºçœŸå®ç´¯è®¡ Acc (æ¥çº³ç‡) å’Œ Blk (é˜»å¡ç‡)ã€‚
4. ğŸ”¥ æ—¶é—´æ§½ç³»ç»Ÿï¼šæ”¯æŒç¦»æ•£æ—¶é—´æ¨¡æ‹Ÿã€æ‰¹é‡è¯·æ±‚å¤„ç†ã€èµ„æºè‡ªåŠ¨é‡Šæ”¾
5. ğŸ”§ ä¿®å¤Lossä¸º0çš„é—®é¢˜ï¼šç¡®ä¿ç½‘ç»œæ›´æ–°å’Œæ¢¯åº¦å›ä¼ æ­£å¸¸è¿›è¡Œ
===============================================================================
"""

import logging
import numpy as np
import random
import pickle
from pathlib import Path
from tqdm import tqdm
from torch.utils.tensorboard import SummaryWriter
import torch
from utils.visualizer import SFCVisualizer

logger = logging.getLogger(__name__)


class Phase3RLTrainer:
    """Phase 3: Goal-Conditioned RL Trainer with DAgger + Time Slot System"""

    def __init__(self, env, agent, output_dir, config, coordinator=None):
        self.env = env
        self.agent = agent
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.cfg = config
        self.coordinator = coordinator

        # ğŸ”¥ åˆå§‹åŒ–å¯è§†åŒ–å™¨
        self.visualizer = None
        if hasattr(env, 'topo'):
            try:
                self.visualizer = SFCVisualizer(env.topo, output_dir)
                logger.info("ğŸ¨ å¯è§†åŒ–å™¨å·²å°±ç»ª (plots å°†ä¿å­˜åœ¨ outputs/checkpoints/plots)")
            except Exception as e:
                logger.warning(f"âš ï¸ å¯è§†åŒ–å™¨åˆå§‹åŒ–å¤±è´¥: {e}")

        phase3_cfg = config.get("phase3", {})
        self.max_episodes = phase3_cfg.get("episodes", 1000)
        self.save_freq = phase3_cfg.get("save_every", 100)

        # ğŸ”§ æ–°å¢ï¼šè®­ç»ƒå‚æ•°
        self.max_steps_per_episode = phase3_cfg.get("max_steps", 600)
        self.update_frequency = phase3_cfg.get("update_frequency", 4)  # æ¯4æ­¥æ›´æ–°ä¸€æ¬¡
        self.warmup_steps = phase3_cfg.get("warmup_steps", 100)  # é¢„çƒ­æ­¥æ•°

        # 1. Epsilon é…ç½®
        epsilon_cfg = phase3_cfg.get("epsilon", {})
        self.epsilon_initial = epsilon_cfg.get("initial", 0.7)
        self.epsilon_final = epsilon_cfg.get("final", 0.01)
        self.epsilon_decay_steps = epsilon_cfg.get("decay_steps", 5000)

        # 2. DAgger é…ç½®
        dagger_cfg = phase3_cfg.get("dagger", {})
        self.use_dagger = dagger_cfg.get("enabled", True)
        self.beta = dagger_cfg.get("initial_beta", 0.8)
        self.beta_final = dagger_cfg.get("final_beta", 0.05)
        self.beta_decay_steps = dagger_cfg.get("decay_steps", 10000)

        # ğŸ”¥ æ–°å¢ï¼šæ—¶é—´æ§½ç³»ç»Ÿé…ç½®
        timeslot_cfg = phase3_cfg.get("timeslot", {})
        self.use_timeslot = timeslot_cfg.get("enabled", True)
        self.log_timeslot_info = timeslot_cfg.get("log_timeslot_info", True)
        self.log_timeslot_jumps = timeslot_cfg.get("log_jumps", True)

        # TensorBoard
        self.writer = SummaryWriter(log_dir=str(self.output_dir / "runs"))

        # ç»Ÿè®¡ä¿¡æ¯å®¹å™¨
        self.stats = {
            "rewards": [],
            "acceptance_rates": [],
            "blocking_rates": [],
            "resource_levels": [],
            "subgoal_completion_rate": [],
            "time_slots_covered": [],
            "decision_steps": [],
            "requests_per_episode": [],
            "losses": [],
            "high_losses": [],
            "low_losses": []

        }
        self.global_step = 0
        self.total_updates = 0  # ğŸ”§ æ–°å¢ï¼šæ€»æ›´æ–°æ¬¡æ•°

        # ğŸ”¥ æ–°å¢ï¼šæ—¶é—´æ§½ç›¸å…³ç»Ÿè®¡
        self.timeslot_stats = {
            'total_time_slots': 0,
            'total_decision_steps': 0,
            'avg_steps_per_request': 0,
            'timeslot_jumps': []
        }

    def _get_network_resource_level(self):
        """
        ğŸ”¥ [V10.17 ä¿®å¤ç‰ˆ] åŠ¨æ€è·å–çœŸå®å®¹é‡ï¼Œä¸å†å†™æ­» 100.0
        """
        try:
            rm = self.env.resource_mgr
            # è·å– DC èŠ‚ç‚¹åˆ—è¡¨
            dc_nodes = getattr(self.env, 'dc_nodes', [])

            if not dc_nodes:
                return 0.0

            total_dc_cpu = 0.0
            total_dc_cap = 0.0

            # 1. å°è¯•è·å–æ€»å®¹é‡åŸºå‡† (ä¼˜å…ˆç”¨ ResourceManager é‡Œçš„ C_cap)
            # è¿™æ˜¯ä¸€ä¸ªä¿é™©é€»è¾‘ï¼šçœ‹çœ‹ rm.C_cap æ˜¯æ•°ç»„è¿˜æ˜¯æ•°å­—
            c_cap_ref = getattr(rm, 'C_cap', 100.0)

            # éå†æ‰€æœ‰ DC èŠ‚ç‚¹
            for node in dc_nodes:
                # --- è·å–å½“å‰å‰©ä½™é‡ (åˆ†å­) ---
                current_cpu = 0.0
                if isinstance(rm.nodes, dict) and 'cpu' in rm.nodes:
                    if node < len(rm.nodes['cpu']):
                        current_cpu = rm.nodes['cpu'][node]
                elif isinstance(rm.nodes, list):
                    if node < len(rm.nodes):
                        current_cpu = rm.nodes[node].get('cpu', 0)

                # --- è·å–è¯¥èŠ‚ç‚¹æ€»å®¹é‡ (åˆ†æ¯) ---
                # ğŸ”¥ğŸ”¥ğŸ”¥ ä¹‹å‰è¿™é‡Œå†™æ­»æˆäº† total_dc_cap += 100.0ï¼Œè¿™å°±æ˜¯ 150% çš„ç½ªé­ç¥¸é¦–ï¼
                node_cap = 100.0  # é»˜è®¤å…œåº•

                if hasattr(c_cap_ref, '__getitem__'):  # å¦‚æœ C_cap æ˜¯æ•°ç»„ [30, 55, 40...]
                    if node < len(c_cap_ref):
                        node_cap = float(c_cap_ref[node])
                elif isinstance(c_cap_ref, (int, float)):  # å¦‚æœ C_cap æ˜¯æ ‡é‡ 100.0
                    node_cap = float(c_cap_ref)

                # ç´¯åŠ 
                total_dc_cpu += current_cpu
                total_dc_cap += node_cap

            # é˜²æ­¢é™¤ä»¥é›¶
            if total_dc_cap <= 0: return 0.0

            # è®¡ç®—ç™¾åˆ†æ¯”
            dc_res_pct = (total_dc_cpu / total_dc_cap) * 100.0

            # å†æ¬¡ä¿é™©ï¼šå¦‚æœç®—å‡ºæ¥å¤§äº 100ï¼Œå¼ºè¡Œä¿®æ­£ (è¯´æ˜ C_cap æ²¡å–å¯¹)
            if dc_res_pct > 100.0:
                # print(f"âš ï¸ èµ„æºæ˜¾ç¤ºå¼‚å¸¸: {dc_res_pct:.1f}% (åˆ†å­{total_dc_cpu}/åˆ†æ¯{total_dc_cap})")
                return 100.0

            return dc_res_pct

        except Exception as e:
            # print(f"èµ„æºç›‘æ§å‡ºé”™: {e}")
            return 0.0

    def load_timeslot_data(self):
        """
        ğŸ”¥ æ–°å¢ï¼šåŠ è½½æ—¶é—´æ§½æ•°æ®
        """
        if not self.use_timeslot:
            logger.info("âš ï¸ æ—¶é—´æ§½ç³»ç»Ÿæœªå¯ç”¨ï¼Œè·³è¿‡æ•°æ®åŠ è½½")
            return False

        try:
            # è·å–æ•°æ®è·¯å¾„
            path_cfg = self.cfg.get('path', {})
            input_dir = Path(path_cfg.get('input_dir', 'data/input_dir'))

            # æ–‡ä»¶å
            requests_file = input_dir / path_cfg.get('requests_file', 'phase3_requests.pkl')
            requests_by_slot_file = input_dir / path_cfg.get('requests_by_slot_file', 'phase3_requests_by_slot.pkl')

            logger.info(f"\n{'=' * 60}")
            logger.info(f"ğŸ”¥ åŠ è½½æ—¶é—´æ§½æ•°æ®")
            logger.info(f"{'=' * 60}")
            logger.info(f"è¯·æ±‚æ–‡ä»¶: {requests_file}")
            logger.info(f"æ—¶é—´æ§½æ–‡ä»¶: {requests_by_slot_file}")

            # åŠ è½½æ•°æ®
            with open(requests_file, 'rb') as f:
                requests = pickle.load(f)

            with open(requests_by_slot_file, 'rb') as f:
                requests_by_slot = pickle.load(f)

            # åŠ è½½åˆ°ç¯å¢ƒ
            if hasattr(self.env, 'load_requests'):
                self.env.load_requests(requests, requests_by_slot)
                logger.info(f"âœ… æ—¶é—´æ§½æ•°æ®åŠ è½½æˆåŠŸ")
                logger.info(f"   æ€»è¯·æ±‚æ•°: {len(requests)}")
                logger.info(f"   æ—¶é—´æ§½æ•°: {len(requests_by_slot)}")
                logger.info(f"{'=' * 60}\n")
                return True
            else:
                logger.warning("âš ï¸ ç¯å¢ƒä¸æ”¯æŒ load_requests() æ–¹æ³•")
                return False

        except FileNotFoundError as e:
            logger.error(f"âŒ æ—¶é—´æ§½æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {e}")
            logger.info("æç¤º: è¯·å…ˆè¿è¡Œæ•°æ®ç”Ÿæˆè„šæœ¬:")
            logger.info("  python main_generate_time_slot.py")
            logger.info("  python generate_event_time_slot.py")
            return False
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ—¶é—´æ§½æ•°æ®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

    def run(self):
        """è¿è¡Œè®­ç»ƒä¸»å¾ªç¯"""
        logger.info(f"ğŸš€ Starting Training: DAgger={self.use_dagger}, Beta={self.beta}")
        logger.info(
            f"ğŸ“Š è®­ç»ƒå‚æ•°: episodes={self.max_episodes}, warmup={self.warmup_steps}, update_freq={self.update_frequency}")

        # ğŸ”¥ åŠ è½½æ—¶é—´æ§½æ•°æ®
        if self.use_timeslot:
            if not self.load_timeslot_data():
                logger.error("âŒ æ—¶é—´æ§½æ•°æ®åŠ è½½å¤±è´¥ï¼Œé€€å‡ºè®­ç»ƒ")
                return

        # ============================================
        # ğŸ”¥ å…¨å±€ç´¯è®¡è®¡æ•°å™¨ (ä¿®å¤ Acc æ˜¾ç¤ºé—®é¢˜)
        # ============================================
        total_episodes = 0
        total_success = 0
        total_failed = 0

        pbar = tqdm(range(self.max_episodes), desc="RL Training")

        for ep in pbar:
            try:
                # è¿è¡Œä¸€ä¸ª Episode
                ep_reward, ep_info = self._run_episode(ep)

                # 1. è·å–èµ„æºæ°´å¹³
                curr_res_level = self._get_network_resource_level()

                # 2. âœ… æ›´æ–°å…¨å±€è®¡æ•°å™¨ (æ ¸å¿ƒä¿®å¤)
                total_episodes += 1

                # åˆ¤æ–­æˆåŠŸæ ‡å‡†ï¼šåªè¦ env è¯´æ˜¯ success æˆ– request_completed å°±ç®—æˆ
                is_success = ep_info.get('success', False)

                if is_success:
                    total_success += 1
                else:
                    total_failed += 1

                # 3. è®¡ç®—ç´¯è®¡æŒ‡æ ‡
                cum_acc = total_success / total_episodes if total_episodes > 0 else 0.0
                cum_blk = total_failed / total_episodes if total_episodes > 0 else 0.0

                # 4. è®°å½•åˆ° Stats (ç”¨äºç»˜å›¾)
                self.stats["rewards"].append(ep_reward)
                self.stats["acceptance_rates"].append(1.0 if is_success else 0.0)
                self.stats["blocking_rates"].append(0.0 if is_success else 1.0)
                self.stats["resource_levels"].append(curr_res_level)

                # ğŸ”¥ [æ–°å¢] è®°å½• Loss
                avg_loss = ep_info.get('avg_loss', 0.0)
                avg_high_loss = ep_info.get('avg_high_loss', 0.0)
                avg_low_loss = ep_info.get('avg_low_loss', 0.0)

                self.stats["losses"].append(avg_loss)
                self.stats["high_losses"].append(avg_high_loss)
                self.stats["low_losses"].append(avg_low_loss)

                # ğŸ”¥ æ–°å¢ï¼šæ—¶é—´æ§½ç»Ÿè®¡
                if self.use_timeslot:
                    self.stats["time_slots_covered"].append(ep_info.get('time_slots_covered', 0))
                    self.stats["decision_steps"].append(ep_info.get('decision_steps', 0))
                    self.stats["requests_per_episode"].append(ep_info.get('requests_processed', 1))

                # 5. TensorBoard (è®°å½•ç´¯è®¡å€¼æ›´å¹³æ»‘)
                self.writer.add_scalar("Train/Reward", ep_reward, ep)
                self.writer.add_scalar("Train/CumulativeAcc", cum_acc, ep)
                self.writer.add_scalar("Train/CumulativeBlk", cum_blk, ep)
                self.writer.add_scalar("Train/Resource", curr_res_level, ep)
                self.writer.add_scalar("Train/Loss", avg_loss, ep)
                self.writer.add_scalar("Train/HighLoss", avg_high_loss, ep)
                self.writer.add_scalar("Train/LowLoss", avg_low_loss, ep)

                # ğŸ”¥ æ–°å¢ï¼šæ—¶é—´æ§½æŒ‡æ ‡
                if self.use_timeslot:
                    self.writer.add_scalar("Train/TimeSlotsCovered", ep_info.get('time_slots_covered', 0), ep)
                    self.writer.add_scalar("Train/DecisionSteps", ep_info.get('decision_steps', 0), ep)
                    self.writer.add_scalar("Train/CurrentTimeSlot", ep_info.get('current_time_slot', 0), ep)

                if hasattr(self.agent, 'epsilon_low'):
                    self.writer.add_scalar("Train/Epsilon", self.agent.epsilon_low, ep)

                # 6. æ›´æ–°è¿›åº¦æ¡ (æ˜¾ç¤ºå…¨å±€ç´¯è®¡å€¼)
                expert_usage_pct = ep_info.get('expert_usage', 0) * 100

                # ğŸ”¥ æ„å»ºè¿›åº¦æ¡æ˜¾ç¤º
                postfix = {
                    "Rw": f"{ep_reward:.0f}",
                    "Exp": f"{expert_usage_pct:.0f}%",
                    "Acc": f"{cum_acc:.1%}",
                    "Blk": f"{cum_blk:.1%}",
                    "Res": f"{curr_res_level:.0f}%",
                    "Loss": f"{avg_loss:.4f}",
                    "HiLoss": f"{avg_high_loss:.4f}",
                    "LoLoss": f"{avg_low_loss:.4f}"
                }

                # ğŸ”¥ å¦‚æœå¯ç”¨æ—¶é—´æ§½ï¼Œæ·»åŠ æ—¶é—´æ§½ä¿¡æ¯
                if self.use_timeslot:
                    postfix["TS"] = ep_info.get('current_time_slot', 0)
                    postfix["DS"] = ep_info.get('decision_steps', 0)

                pbar.set_postfix(postfix)

                # 7. æ˜¾ç¤ºè®­ç»ƒçŠ¶æ€æ‘˜è¦
                if (ep + 1) % 50 == 0:
                    logger.info(f"\nğŸ“Š Episode {ep + 1} è®­ç»ƒçŠ¶æ€:")
                    logger.info(f"   ç´¯è®¡æ›´æ–°æ¬¡æ•°: {self.total_updates}")
                    logger.info(f"   ç»éªŒç¼“å†²åŒº: High={len(self.agent.high_memory)}, Low={len(self.agent.low_memory)}")
                    logger.info(f"   Loss: High={avg_high_loss:.6f}, Low={avg_low_loss:.6f}")

                # ä¿å­˜æ¨¡å‹
                if (ep + 1) % self.save_freq == 0:
                    self.agent.save(str(self.output_dir / f"rl_model_ep{ep + 1}.pth"))

                    # ğŸ”¥ æ‰“å°æ—¶é—´æ§½ç»Ÿè®¡
                    if self.use_timeslot and self.log_timeslot_info:
                        self._print_timeslot_stats(ep + 1)

            except Exception as e:
                # ğŸ›¡ï¸ å´©æºƒé˜²å¾¡ï¼šæ•è·æ‰€æœ‰å¼‚å¸¸ï¼Œä¸ä¸­æ–­è®­ç»ƒ
                logger.error(f"âŒ Episode {ep} CRASHED: {e}")
                import traceback
                traceback.print_exc()
                # å‘ç”Ÿå¼‚å¸¸ç®—ä½œå¤±è´¥
                total_episodes += 1
                total_failed += 1
                continue

        # è®­ç»ƒç»“æŸä¿å­˜
        self.agent.save(str(self.output_dir / "rl_model_final.pth"))
        logger.info(f"âœ… Training Complete. Final Acc: {total_success / total_episodes:.2%}")
        logger.info(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡: æ€»æ›´æ–°æ¬¡æ•°={self.total_updates}, å¹³å‡Loss={np.mean(self.stats['losses']):.6f}")

        # ğŸ”¥ æ‰“å°æœ€ç»ˆæ—¶é—´æ§½ç»Ÿè®¡
        if self.use_timeslot:
            self._print_final_timeslot_stats()

    def _run_episode(self, episode_idx: int):
        """
        ğŸ”¥ [V32.0 HRL Coordinator é›†æˆç‰ˆ]

        è¿è¡Œä¸€ä¸ªepisodeï¼ˆé›†æˆ Coordinator + é»‘åå• + DAgger + æ—¶é—´æ§½ç³»ç»Ÿ + Lossç›‘æ§ï¼‰

        æ ¸å¿ƒé€»è¾‘:
        1. ä¼˜å…ˆä½¿ç”¨ HRL Coordinatorï¼ˆå¦‚æœå¯ç”¨ï¼‰
        2. Coordinator è‡ªåŠ¨ç®¡ç†é«˜ä½å±‚äº¤äº’
        3. å›é€€åˆ°ç›´æ¥è°ƒç”¨ env.stepï¼ˆå…¼å®¹æ¨¡å¼ï¼‰
        """
        import numpy as np
        import random

        # ========================================
        # åˆå§‹åŒ–
        # ========================================
        # ğŸ”§ é¢„çƒ­æ£€æŸ¥
        if self.agent.steps_done < self.warmup_steps:
            logger.debug(f"ğŸ”¥ é¢„çƒ­é˜¶æ®µ: {self.agent.steps_done}/{self.warmup_steps}")

        max_steps = self.max_steps_per_episode

        # âœ… é‡ç½®ç¯å¢ƒ
        reset_result = self.env.reset()
        if isinstance(reset_result, tuple) and len(reset_result) == 2:
            state, reset_info = reset_result
        else:
            state = reset_result
            reset_info = {}

        # ğŸ”¥ è·å–æ—¶é—´æ§½ä¿¡æ¯
        initial_time_slot = reset_info.get('time_slot', 0)
        current_time_slot = initial_time_slot
        request_id = reset_info.get('request_id')
        last_time_slot = current_time_slot

        # è·å–åˆå§‹ mask å’Œ info
        action_mask = reset_info.get('action_mask')
        blacklist_info = reset_info.get('blacklist_info', {})
        unconnected_dests = self._get_current_destinations()

        # Episode çŠ¶æ€
        done = False
        steps = 0
        decision_steps = 0
        episode_reward = 0

        # ğŸ”¥ Loss ç»Ÿè®¡å®¹å™¨
        episode_losses = []
        episode_high_losses = []
        episode_low_losses = []

        # DAgger ç»Ÿè®¡
        expert_steps = 0
        masked_expert_steps = 0

        # ç»éªŒå­˜å‚¨ç»Ÿè®¡
        stored_high_transitions = 0
        stored_low_transitions = 0

        # åˆå§‹åŒ– step_info
        step_info = {'success': False, 'request_completed': False}

        # ========================================
        # ğŸ”¥ æ£€æµ‹æ˜¯å¦ä½¿ç”¨ Coordinator
        # ========================================
        use_coordinator = (self.coordinator is not None)

        if use_coordinator:
            logger.debug(f"âœ… Episode {episode_idx}: ä½¿ç”¨ HRL Coordinator æ¨¡å¼")
        else:
            logger.debug(f"âš ï¸ Episode {episode_idx}: ä½¿ç”¨å›é€€æ¨¡å¼ï¼ˆç›´æ¥è°ƒç”¨ env.stepï¼‰")

        # ========================================
        # ä¸»å¾ªç¯
        # ========================================
        while not done and steps < max_steps:

            # ============================================================
            # ğŸ”¥ğŸ”¥ğŸ”¥ æ–¹æ¡ˆ A: ä½¿ç”¨ HRL Coordinator
            # ============================================================
            if use_coordinator:
                try:
                    # Coordinator è‡ªåŠ¨ç®¡ç†é«˜ä½å±‚äº¤äº’
                    next_state, reward, done, truncated, step_info = self.coordinator.step()

                    # ä» Coordinator è·å–æ‰§è¡Œçš„åŠ¨ä½œä¿¡æ¯
                    if hasattr(self.coordinator, 'last_transition'):
                        transition = self.coordinator.last_transition
                        if transition and len(transition) == 5:
                            trans_state, low_action, trans_reward, trans_next_state, trans_done = transition

                            # å­˜å‚¨ä½å±‚ç»éªŒ
                            self.agent.store_transition_low(
                                trans_state, low_action, trans_reward, trans_next_state, trans_done
                            )
                            stored_low_transitions += 1

                    # å¦‚æœ Coordinator è§¦å‘äº†é«˜å±‚å†³ç­–ï¼Œå¯èƒ½éœ€è¦å•ç‹¬å­˜å‚¨
                    if hasattr(self.coordinator, 'last_high_action'):
                        high_action = self.coordinator.last_high_action
                        if high_action is not None and unconnected_dests:
                            goal = unconnected_dests[high_action] if high_action < len(unconnected_dests) else -1
                            if goal != -1:
                                self.agent.store_transition_high(
                                    state, goal, reward, next_state, done or truncated
                                )
                                stored_high_transitions += 1

                    # æ›´æ–°çŠ¶æ€
                    state = next_state
                    episode_reward += reward
                    steps += 1

                    # æ›´æ–°æ—¶é—´æ§½ä¿¡æ¯
                    new_time_slot = step_info.get('time_slot', current_time_slot)
                    new_decision_steps = step_info.get('decision_steps', decision_steps)

                    if self.use_timeslot and new_time_slot != last_time_slot:
                        if self.log_timeslot_jumps:
                            logger.debug(f"â° [Ep {episode_idx}] Time Slot: {last_time_slot} â†’ {new_time_slot}")
                        self.timeslot_stats['timeslot_jumps'].append((last_time_slot, new_time_slot))
                        last_time_slot = new_time_slot

                    current_time_slot = new_time_slot
                    decision_steps = new_decision_steps

                    # æ›´æ–°ç›®æ ‡ä¿¡æ¯
                    unconnected_dests = self._get_current_destinations()

                except Exception as e:
                    logger.error(f"âŒ Coordinator.step å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
                    # å‘ç”Ÿé”™è¯¯æ—¶ç»ˆæ­¢ episode
                    break

            # ============================================================
            # ğŸ”¥ğŸ”¥ğŸ”¥ æ–¹æ¡ˆ B: å›é€€æ¨¡å¼ï¼ˆæ—  Coordinatorï¼‰
            # ============================================================
            else:
                # ----------------------------------------
                # 1. æå– Action Mask
                # ----------------------------------------
                action_mask = None

                # æ–¹å¼1: ä»PyG Dataå¯¹è±¡ä¸­æå–
                if hasattr(state, 'action_mask'):
                    action_mask = state.action_mask
                    if hasattr(action_mask, 'cpu'):
                        action_mask = action_mask.cpu().numpy()
                    if action_mask.ndim > 1:
                        action_mask = action_mask.squeeze()

                # æ–¹å¼2: ä»step_infoä¸­æå–
                elif 'action_mask' in step_info:
                    action_mask = step_info['action_mask']

                # æ–¹å¼3: ç›´æ¥è°ƒç”¨ç¯å¢ƒæ–¹æ³•
                if action_mask is None and hasattr(self.env, 'get_low_level_action_mask'):
                    action_mask = self.env.get_low_level_action_mask()

                # ğŸ”¥ ç¡®ä¿maskæ˜¯numpyæ•°ç»„
                if action_mask is not None:
                    if hasattr(action_mask, 'numpy'):
                        action_mask = action_mask.numpy()
                    if isinstance(action_mask, list):
                        action_mask = np.array(action_mask)

                # ----------------------------------------
                # 2. DAgger é€»è¾‘
                # ----------------------------------------
                beta = self.beta
                use_dagger = self.use_dagger
                use_expert = False
                expert_action = None

                if use_dagger and random.random() < beta:
                    expert_suggestion = self._get_expert_action(state)
                    if action_mask is None:
                        use_expert = True
                        expert_action = expert_suggestion
                    else:
                        valid_actions = np.where(action_mask > 0)[0]
                        if expert_suggestion in valid_actions:
                            use_expert = True
                            expert_action = expert_suggestion
                            expert_steps += 1
                        else:
                            masked_expert_steps += 1

                # ----------------------------------------
                # 3. Agent é€‰æ‹©åŠ¨ä½œ
                # ----------------------------------------
                high_action, low_action, action_info = self.agent.select_action(
                    state=state,
                    unconnected_dests=unconnected_dests,
                    action_mask=action_mask,
                    use_expert=use_expert,
                    expert_action=expert_action,
                    blacklist_info=blacklist_info
                )

                # ğŸ›¡ï¸ é˜²å¾¡ï¼šå¦‚æœ Agent è¿”å› -1 (æ— æ•ˆ)ï¼Œç»ˆæ­¢ episode
                if low_action == -1:
                    logger.warning(f"âš ï¸ Agent returned -1 (No Valid Actions). Terminating Episode {episode_idx}.")
                    return episode_reward, {
                        'success': False,
                        'blocking_rate': 1.0,
                        'message': 'no_valid_actions',
                        'time_slot': current_time_slot,
                        'decision_steps': decision_steps,
                        'time_slots_covered': current_time_slot - initial_time_slot,
                        'avg_loss': 0.0,
                        'avg_high_loss': 0.0,
                        'avg_low_loss': 0.0
                    }

                # ----------------------------------------
                # 4. æ‰§è¡ŒåŠ¨ä½œ
                # ----------------------------------------
                step_result = self.env.step(low_action)

                # è§£åŒ…ç»“æœ
                if len(step_result) == 5:
                    next_state, reward, done, truncated, step_info = step_result
                else:
                    next_state, reward, done, step_info = step_result
                    truncated = False

                # ----------------------------------------
                # 5. ğŸ”¥ æ£€æµ‹ need_high_level ä¿¡å·
                # ----------------------------------------
                if truncated and step_info.get('need_high_level', False):
                    error_type = step_info.get('error', 'unknown')
                    logger.info(f"âš ï¸ [Episode {episode_idx}] ä½å±‚æ£€æµ‹åˆ°é—®é¢˜: {error_type}")
                    logger.info(f"   â†’ è¿”å›é«˜å±‚é‡æ–°å†³ç­–ï¼ˆä¸ç»ˆæ­¢episodeï¼‰")

                    # è®°å½•å¥–åŠ±
                    episode_reward += reward

                    # é‡ç½®agentåˆ†æ”¯çŠ¶æ€ï¼ˆå¼ºåˆ¶è§¦å‘é«˜å±‚å†³ç­–ï¼‰
                    if hasattr(self.agent, 'current_branch_id'):
                        self.agent.current_branch_id = None
                    if hasattr(self.agent, 'subgoal_steps'):
                        self.agent.subgoal_steps = 999
                    if hasattr(self.agent, 'current_subgoal'):
                        self.agent.current_subgoal = None

                    # å­˜å‚¨ç»éªŒï¼ˆå¤±è´¥çš„å°è¯•ä¹Ÿè¦å­¦ä¹ ï¼‰
                    if action_info.get('high_level_decision', False):
                        goal = unconnected_dests[high_action] if unconnected_dests and high_action < len(
                            unconnected_dests) else -1
                        if goal != -1:
                            self.agent.store_transition_high(state, goal, reward, next_state, False)
                            stored_high_transitions += 1

                    self.agent.store_transition_low(state, low_action, reward, next_state, False)
                    stored_low_transitions += 1

                    # æ›´æ–°çŠ¶æ€
                    state = next_state
                    unconnected_dests = self._get_current_destinations()
                    steps += 1

                    # ç»§ç»­å¾ªç¯ï¼ˆä¸ç»ˆæ­¢episodeï¼‰
                    continue

                # ----------------------------------------
                # 6. æ›´æ–°æ—¶é—´æ§½ä¿¡æ¯
                # ----------------------------------------
                new_time_slot = step_info.get('time_slot', current_time_slot)
                new_decision_steps = step_info.get('decision_steps', decision_steps)

                if self.use_timeslot and new_time_slot != last_time_slot:
                    if self.log_timeslot_jumps:
                        logger.debug(f"â° [Ep {episode_idx}] Time Slot: {last_time_slot} â†’ {new_time_slot}")
                    self.timeslot_stats['timeslot_jumps'].append((last_time_slot, new_time_slot))
                    last_time_slot = new_time_slot

                current_time_slot = new_time_slot
                decision_steps = new_decision_steps

                # ----------------------------------------
                # 7. è®°å½•å¤±è´¥åŸå› ï¼ˆé»‘åå•å­¦ä¹ ï¼‰
                # ----------------------------------------
                if not step_info.get('success', True):
                    reason = step_info.get('message', 'unknown')
                    if "èµ„æºä¸è¶³" in reason or "è®¿é—®è¶…é™" in reason:
                        self.agent.record_failure(low_action, reason)

                # ----------------------------------------
                # 8. å­˜å‚¨ç»éªŒ
                # ----------------------------------------
                # High-Level Buffer
                if action_info.get('high_level_decision', False):
                    goal = unconnected_dests[high_action] if unconnected_dests and high_action < len(
                        unconnected_dests) else -1
                    if goal != -1:
                        self.agent.store_transition_high(state, goal, reward, next_state, done or truncated)
                        stored_high_transitions += 1

                # Low-Level Buffer
                self.agent.store_transition_low(state, low_action, reward, next_state, done or truncated)
                stored_low_transitions += 1

                # ----------------------------------------
                # 9. æ›´æ–°çŠ¶æ€
                # ----------------------------------------
                state = next_state
                action_mask = step_info.get('action_mask')
                blacklist_info = step_info.get('blacklist_info', {})
                unconnected_dests = self._get_current_destinations()
                episode_reward += reward
                steps += 1

                if truncated:
                    done = True

            # ============================================================
            # ğŸ”¥ å®šæœŸæ›´æ–°ç½‘ç»œï¼ˆé€‚ç”¨äºä¸¤ç§æ¨¡å¼ï¼‰
            # ============================================================
            if steps % self.update_frequency == 0:
                # ç¡®ä¿ç»éªŒç¼“å†²åŒºæœ‰è¶³å¤Ÿæ•°æ®
                has_enough_low_exp = len(self.agent.low_memory) >= self.agent.batch_size

                if has_enough_low_exp:
                    # è°ƒç”¨æ›´æ–°å¹¶è·å–è¯¦ç»†çš„æŸå¤±ä¿¡æ¯
                    loss_dict = self.agent.update_policies()

                    if loss_dict:
                        # è®°å½•å„ç§æŸå¤±
                        high_loss = loss_dict.get('high_loss', 0.0)
                        low_loss = loss_dict.get('low_loss', 0.0)
                        total_loss = loss_dict.get('total_loss', 0.0)

                        # åªè®°å½•éé›¶çš„æŸå¤±
                        if high_loss > 0:
                            episode_high_losses.append(high_loss)
                        if low_loss > 0:
                            episode_low_losses.append(low_loss)
                        if total_loss > 0:
                            episode_losses.append(total_loss)

                        self.total_updates += 1

                        # å®šæœŸæ‰“å°æ›´æ–°ä¿¡æ¯
                        if self.total_updates % 100 == 0:
                            logger.debug(
                                f"ğŸ”„ Update #{self.total_updates}: HighLoss={high_loss:.6f}, LowLoss={low_loss:.6f}")

                # å¦‚æœç»éªŒä¸è¶³ï¼Œæ‰“å°è­¦å‘Š
                elif self.total_updates < 10 and steps > 50:
                    logger.debug(f"âš ï¸ ç»éªŒä¸è¶³: High={len(self.agent.high_memory)}, Low={len(self.agent.low_memory)}")

        # ========================================
        # Episode ç»“æŸå¤„ç†
        # ========================================
        # åˆ¤æ–­æˆåŠŸä¸å¦
        is_success = step_info.get('request_success', None)
        if is_success is None:
            is_success = step_info.get('request_completed', False) or step_info.get('success', False)

        # æ£€æŸ¥ç¯å¢ƒæ˜¯å¦å·²å½’æ¡£
        env_already_archived = False
        if hasattr(self.env, 'current_request'):
            env_already_archived = (self.env.current_request is None)

        # å¦‚æœç¯å¢ƒæœªå½’æ¡£ï¼Œæ‰§è¡Œå½’æ¡£
        if not env_already_archived:
            if hasattr(self.env, 'current_request') and self.env.current_request:
                req_id = self.env.current_request.get('id', '?')
                if not is_success:
                    logger.info(f"ğŸ”„ [Episodeæ¸…ç†] è¯·æ±‚ {req_id} å¤±è´¥ï¼Œæ‰§è¡Œå›æ»š...")
                    self.env._archive_request(success=False)
                else:
                    logger.info(f"âœ… [Episodeæ¸…ç†] è¯·æ±‚ {req_id} æˆåŠŸï¼Œå½’æ¡£èµ„æº...")
                    self.env._archive_request(success=True)

                # æ¸…ç†ç¯å¢ƒçŠ¶æ€
                self.env.current_request = None
                self.env.current_branch_id = None
                self.env.current_tree = {}
                self.env.nodes_on_tree = set()
                self.env.branch_states = {}
                if hasattr(self.env, 'curr_ep_node_allocs'):
                    self.env.curr_ep_node_allocs = []
                if hasattr(self.env, 'curr_ep_link_allocs'):
                    self.env.curr_ep_link_allocs = []
        else:
            logger.info(f"â„¹ï¸ [Episodeæ¸…ç†] ç¯å¢ƒå·²å½’æ¡£ï¼Œè·³è¿‡Trainerå½’æ¡£")

        # ========================================
        # æ„å»º Episode Info
        # ========================================
        # è®¡ç®—å¹³å‡ Loss
        avg_loss = np.mean(episode_losses) if episode_losses else 0.0
        avg_high_loss = np.mean(episode_high_losses) if episode_high_losses else 0.0
        avg_low_loss = np.mean(episode_low_losses) if episode_low_losses else 0.0

        episode_info = {
            'steps': steps,
            'success': is_success,
            'blocking_rate': 0.0 if is_success else 1.0,
            'expert_usage': expert_steps / steps if steps > 0 else 0,
            'masked_expert': masked_expert_steps,
            'stored_high': stored_high_transitions,
            'stored_low': stored_low_transitions,
            'avg_loss': avg_loss,
            'avg_high_loss': avg_high_loss,
            'avg_low_loss': avg_low_loss,

            # æ—¶é—´æ§½ä¿¡æ¯
            'current_time_slot': current_time_slot,
            'initial_time_slot': initial_time_slot,
            'time_slots_covered': current_time_slot - initial_time_slot,
            'decision_steps': decision_steps,
            'request_id': request_id,
            'requests_processed': 1,

            # ğŸ”¥ æ–°å¢ï¼šæ ‡è®°ä½¿ç”¨çš„æ¨¡å¼
            'used_coordinator': use_coordinator
        }

        # æ›´æ–°æ—¶é—´æ§½ç»Ÿè®¡
        if self.use_timeslot:
            self.timeslot_stats['total_time_slots'] += (current_time_slot - initial_time_slot)
            self.timeslot_stats['total_decision_steps'] += decision_steps

        # ========================================
        # æ‰“å°æ—¥å¿—
        # ========================================
        status_icon = "âœ…" if is_success else "âŒ"
        mode_icon = "ğŸ¤–" if use_coordinator else "ğŸ”§"

        if is_success or episode_idx % 10 == 0:
            logger.info(
                f"{mode_icon} Ep {episode_idx} | {status_icon} | "
                f"Rw: {episode_reward:.1f} | "
                f"Steps: {steps} | "
                f"HiLoss: {avg_high_loss:.4f} | "
                f"LoLoss: {avg_low_loss:.4f} | "
                f"TS: {current_time_slot} | "
                f"DS: {decision_steps}"
            )

            # è°ƒè¯•ï¼šæ‰“å°ç»éªŒå­˜å‚¨æƒ…å†µ
            if stored_low_transitions == 0:
                logger.warning(f"âš ï¸ Episode {episode_idx}: æ²¡æœ‰å­˜å‚¨ä»»ä½•Low-Levelç»éªŒ!")

        return episode_reward, episode_info
    def _get_current_destinations(self):
        """è·å–å½“å‰æœªè¿æ¥çš„ç›®çš„åœ°åˆ—è¡¨"""
        if not hasattr(self.env, 'current_request') or self.env.current_request is None:
            return []
        all_dests = self.env.current_request.get('dest', [])
        connected = self.env.current_tree.get('connected_dests', set())
        return [d for d in all_dests if d not in connected]

    def _get_expert_action(self, state):
        """è·å–ä¸“å®¶åŠ¨ä½œ"""
        if not hasattr(self, 'agent') or not hasattr(self.agent, 'expert'):
            # å¦‚æœæ²¡æœ‰ Expert Wrapperï¼Œå°è¯•ç”¨ç¯å¢ƒé‡Œçš„
            if hasattr(self.env, 'expert') and self.env.expert:
                # è¿™é‡Œéœ€è¦ expert é€»è¾‘ï¼Œæš‚æ—¶éšæœºå…œåº•
                pass
        return random.randint(0, getattr(self.env, 'n', 28) - 1)

    def _print_timeslot_stats(self, episode):
        """
        ğŸ”¥ æ–°å¢ï¼šæ‰“å°æ—¶é—´æ§½ç»Ÿè®¡ä¿¡æ¯
        """
        logger.info(f"\n{'=' * 60}")
        logger.info(f"â° æ—¶é—´æ§½ç»Ÿè®¡ @ Episode {episode}")
        logger.info(f"{'=' * 60}")

        if self.timeslot_stats['total_decision_steps'] > 0:
            avg_steps = (self.timeslot_stats['total_decision_steps'] /
                         max(1, len(self.stats['decision_steps'])))
            logger.info(f"å¹³å‡å†³ç­–æ­¥æ•°: {avg_steps:.1f}")

        if len(self.stats['time_slots_covered']) > 0:
            avg_slots = np.mean(self.stats['time_slots_covered'][-100:])
            logger.info(f"å¹³å‡æ—¶é—´æ§½è·¨åº¦: {avg_slots:.1f}")

        if len(self.timeslot_stats['timeslot_jumps']) > 0:
            logger.info(f"æ—¶é—´æ§½è·³è½¬æ¬¡æ•°: {len(self.timeslot_stats['timeslot_jumps'])}")

        logger.info(f"{'=' * 60}\n")

    def _print_final_timeslot_stats(self):
        """
        ğŸ”¥ æ–°å¢ï¼šæ‰“å°æœ€ç»ˆæ—¶é—´æ§½ç»Ÿè®¡
        """
        logger.info(f"\n{'=' * 60}")
        logger.info(f"ğŸ‰ æœ€ç»ˆæ—¶é—´æ§½ç»Ÿè®¡")
        logger.info(f"{'=' * 60}")

        total_episodes = len(self.stats['decision_steps'])

        if total_episodes > 0:
            avg_decision_steps = np.mean(self.stats['decision_steps'])
            avg_time_slots = np.mean(self.stats['time_slots_covered'])

            logger.info(f"æ€»Episodes: {total_episodes}")
            logger.info(f"å¹³å‡å†³ç­–æ­¥æ•°: {avg_decision_steps:.1f}")
            logger.info(f"å¹³å‡æ—¶é—´æ§½è·¨åº¦: {avg_time_slots:.1f}")
            logger.info(f"æ€»æ—¶é—´æ§½è·³è½¬: {len(self.timeslot_stats['timeslot_jumps'])}")

            if self.timeslot_stats['total_decision_steps'] > 0:
                efficiency = (self.timeslot_stats['total_time_slots'] /
                              self.timeslot_stats['total_decision_steps'])
                logger.info(f"æ—¶é—´æ§½æ•ˆç‡: {efficiency:.2f} (æ—¶é—´æ§½/å†³ç­–æ­¥)")

        logger.info(f"{'=' * 60}\n")

