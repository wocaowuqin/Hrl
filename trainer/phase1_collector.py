# trainer/phase1_collector.py - æ—¶é—´æ§½ç‰ˆæœ¬ï¼ˆä¿®å¤æ ·æœ¬æ”¶é›†é€»è¾‘ï¼‰
import os
import pickle
import logging
from tqdm import tqdm
from typing import Dict, List, Any
import numpy as np
from torch_geometric.data import Data
import torch
from pathlib import Path

logger = logging.getLogger(__name__)


class Phase1ExpertCollector:
    """
    Phase 1 ä¸“å®¶æ•°æ®æ”¶é›†å™¨ï¼ˆæ—¶é—´æ§½ç‰ˆæœ¬ - ä¿®å¤ç‰ˆï¼‰

    ğŸ”¥ ä¿®å¤é—®é¢˜ï¼š
    åŸæ¥: åªä¿å­˜äº†60ä¸ªæ ·æœ¬ï¼ˆè¾¾åˆ°max_episodes=5000ååœæ­¢ï¼Œä½†5000æ˜¯è¯·æ±‚æ•°ä¸æ˜¯æ ·æœ¬æ•°ï¼‰
    ç°åœ¨: æ­£ç¡®æ”¶é›†5000ä¸ªæ ·æœ¬ï¼ˆæ¯ä¸ªæˆåŠŸè¯·æ±‚å¯èƒ½äº§ç”Ÿå¤šä¸ªè·¯å¾„æ ·æœ¬ï¼‰

    å…³é”®ä¿®å¤ï¼š
    1. max_episodes ç°åœ¨æŒ‡çš„æ˜¯"æˆåŠŸæ ·æœ¬æ•°"è€Œä¸æ˜¯"å¤„ç†çš„è¯·æ±‚æ•°"
    2. æ­£ç¡®ç»Ÿè®¡å’Œä¿å­˜æ‰€æœ‰æˆåŠŸçš„è·¯å¾„æ ·æœ¬
    3. åœ¨è¾¾åˆ°ç›®æ ‡æ ·æœ¬æ•°æ—¶åœæ­¢ï¼Œè€Œä¸æ˜¯å¤„ç†å®Œæ‰€æœ‰è¯·æ±‚
    """

    def __init__(self, env, expert_solver, output_dir: str, max_episodes: int = 5000,
                 save_every: int = 500, use_timeslot: bool = True):
        self.env = env
        self.expert = expert_solver
        self.output_dir = output_dir
        os.makedirs(self.output_dir, exist_ok=True)

        # ğŸ”¥ ä¿®å¤ï¼šmax_requests â†’ max_success_samplesï¼Œæ˜ç¡®å«ä¹‰
        self.max_success_samples = max_episodes
        self.save_every = save_every

        self.success_samples = []
        self.fail_contexts = []
        # ğŸ”¥ æ·»åŠ paths_collectedç»Ÿè®¡
        self.stats = {"requests": 0, "success": 0, "fail": 0, "paths_collected": 0}

        # æ—¶é—´æ§½ç³»ç»Ÿé…ç½®
        self.use_timeslot = use_timeslot
        self.timeslot_stats = {
            'total_time_slots': 0,
            'requests_per_slot': [],
            'current_time_slot': 0
        }

    def _estimate_load(self):
        rm = self.env.resource_mgr
        bw_util = 1.0 - rm.B.mean() / max(1.0, rm.B_cap)
        cpu_util = 1.0 - rm.C.mean() / max(1.0, rm.C_cap)
        return 0.5 * bw_util + 0.5 * cpu_util

    def _sanitize_request(self, req):
        """å°† Request å¯¹è±¡è½¬æ¢ä¸ºæ™®é€šå­—å…¸ï¼Œé˜²æ­¢ Pickle æŠ¥é”™"""
        if isinstance(req, dict):
            return req.copy()
        if hasattr(req, '__dict__'):
            return req.__dict__.copy()
        try:
            return dict(req)
        except:
            return {
                'id': getattr(req, 'id', -1),
                'source': getattr(req, 'source', 0),
                'dest': getattr(req, 'dest', []),
                'vnf': getattr(req, 'vnf', []),
                'bandwidth': getattr(req, 'bandwidth', 1.0),
                'ttl': getattr(req, 'ttl', 100),
                'time_slot': getattr(req, 'time_slot', 0),
                'duration': getattr(req, 'duration', 100),
                'leave_time_slot': getattr(req, 'leave_time_slot', 100)
            }

    def _convert_request_indices(self, raw_req):
        """è½¬æ¢è¯·æ±‚ç´¢å¼•ï¼š1-based â†’ 0-based"""
        req = self._sanitize_request(raw_req)

        # Source: 1-based â†’ 0-based
        src = req.get("source", 0)
        if isinstance(src, (list, np.ndarray)):
            src = src.item()
        if src > 0:
            src = src - 1
        req['source'] = int(src)

        # Dest: 1-based â†’ 0-based
        new_dests = []
        raw_dests = req.get("dest", [])
        if hasattr(raw_dests, 'flatten'):
            raw_dests = raw_dests.flatten()
        for d in raw_dests:
            d_val = int(d)
            if d_val > 0:
                d_val = d_val - 1
            new_dests.append(d_val)
        req['dest'] = new_dests

        # VNF: 1-based â†’ 0-based
        new_vnfs = req.get('vnf', [])
        if hasattr(new_vnfs, 'flatten'):
            new_vnfs = new_vnfs.flatten()
        vnf_list = []
        for v in new_vnfs:
            v_val = int(v)
            if v_val > 0:
                v_val = v_val - 1
            vnf_list.append(v_val)
        req['vnf'] = vnf_list

        # ğŸ”¥ ä¿®å¤ï¼šå¤„ç†å¸¦å®½å­—æ®µ
        if 'bandwidth' not in req or req['bandwidth'] is None:
            # ä¼˜å…ˆä½¿ç”¨ bw_origin
            req['bandwidth'] = req.get('bw_origin', 3.0)

        # ğŸ”¥ ä¿®å¤ï¼šå¤„ç†CPUå’Œå†…å­˜å­—æ®µ
        if 'cpu' not in req or req['cpu'] is None:
            req['cpu'] = req.get('cpu_origin', [1.0] * len(vnf_list))

        if 'memory' not in req or req['memory'] is None:
            req['memory'] = req.get('memory_origin', [1.0] * len(vnf_list))

        return req

    def _try_auto_load_timeslot_data(self):
        """è‡ªåŠ¨å°è¯•åŠ è½½æ—¶é—´æ§½æ•°æ®"""
        logger.info("ğŸ” å°è¯•è‡ªåŠ¨åŠ è½½æ—¶é—´æ§½æ•°æ®...")

        if hasattr(self.env, 'config'):
            config = self.env.config
            data_dir = Path(config.get('paths', {}).get('input_dir', 'data/input_dir'))
        else:
            data_dir = Path('data/input_dir')

        requests_file = data_dir / 'phase1_requests.pkl'
        requests_by_slot_file = data_dir / 'phase1_requests_by_slot.pkl'

        logger.info(f"   æ£€æŸ¥æ–‡ä»¶: {requests_file}")
        logger.info(f"   æ£€æŸ¥æ–‡ä»¶: {requests_by_slot_file}")

        if not requests_file.exists() or not requests_by_slot_file.exists():
            return False

        try:
            with open(requests_file, 'rb') as f:
                requests = pickle.load(f)
            with open(requests_by_slot_file, 'rb') as f:
                requests_by_slot = pickle.load(f)

            logger.info(f"   âœ… æ–‡ä»¶åŠ è½½æˆåŠŸ: {len(requests)} è¯·æ±‚, {len(requests_by_slot)} æ—¶é—´æ§½")

            if hasattr(self.env, 'load_requests'):
                self.env.load_requests(requests, requests_by_slot)
            else:
                self.env.all_requests = requests
                self.env.requests_by_slot = requests_by_slot

            return True
        except Exception as e:
            logger.error(f"   âŒ è‡ªåŠ¨åŠ è½½å¤±è´¥: {e}")
            return False

    def load_timeslot_data(self):
        """åŠ è½½æ—¶é—´æ§½æ•°æ®"""
        if not self.use_timeslot:
            return False

        try:
            if (hasattr(self.env, 'all_requests') and self.env.all_requests and
                    hasattr(self.env, 'requests_by_slot') and self.env.requests_by_slot):
                logger.info(f"âœ… ç¯å¢ƒå·²åŠ è½½æ—¶é—´æ§½æ•°æ®: {len(self.env.all_requests)} è¯·æ±‚")
                return True
            else:
                if self._try_auto_load_timeslot_data():
                    return True
                else:
                    self.use_timeslot = False
                    return False
        except Exception as e:
            self.use_timeslot = False
            return False

    def collect(self):
        """ä¸»æ”¶é›†æ–¹æ³•"""
        logger.info("ğŸš€ Starting Phase 1: Expert Data Collection")
        logger.info(f"   ç›®æ ‡æ ·æœ¬æ•°: {self.max_success_samples}")  # ğŸ”¥ æ˜ç¡®æ˜¯æ ·æœ¬æ•°

        self.load_timeslot_data()
        self.env.reset()

        # è·å–æ•°æ®
        requests = None
        events = None

        if hasattr(self.env, 'all_requests') and self.env.all_requests:
            requests = self.env.all_requests
        elif hasattr(self.env, 'data_loader'):
            if hasattr(self.env.data_loader, 'requests'):
                requests = self.env.data_loader.requests
            if hasattr(self.env.data_loader, 'events'):
                events = self.env.data_loader.events

        if not requests:
            logger.error("âŒ No requests found!")
            return self.stats

        # é€‰æ‹©æ”¶é›†ç­–ç•¥
        if events is not None and not self.use_timeslot:
            return self._collect_from_events(events, requests)
        else:
            return self._collect_from_requests(requests)

    def _collect_from_events(self, events, requests):
        """Event-basedæ”¶é›†"""
        pbar = tqdm(desc="Collecting HRL Data", ncols=120)

        for t, event in enumerate(events):
            leave_list = event.get("leave", event.get("leave_event", []))
            for leave_req_id in leave_list:
                try:
                    self.env.event_handler.unregister_service(leave_req_id)
                except:
                    pass

            arrive_list = event.get("arrive", event.get("arrive_event", []))
            for req_id in arrive_list:
                if req_id <= 0 or req_id > len(requests):
                    continue

                self._process_single_request(requests[req_id - 1], pbar)

                # ğŸ”¥ ä¿®å¤ï¼šæ£€æŸ¥æ ·æœ¬æ•°è€Œä¸æ˜¯è¯·æ±‚æ•°
                if len(self.success_samples) >= self.max_success_samples:
                    logger.info(f"\nâœ… è¾¾åˆ°ç›®æ ‡æ ·æœ¬æ•°: {len(self.success_samples)}")
                    break

            if len(self.success_samples) >= self.max_success_samples:
                break

        pbar.close()
        self._save_final()
        return self.stats

    def _collect_from_requests(self, requests):
        """æ—¶é—´æ§½æ”¶é›†"""
        pbar = tqdm(desc="Collecting HRL Data (Time Slot)", ncols=120)

        for raw_req in requests:
            self._process_single_request(raw_req, pbar)

            # ğŸ”¥ ä¿®å¤ï¼šæ£€æŸ¥æ ·æœ¬æ•°è€Œä¸æ˜¯è¯·æ±‚æ•°
            if len(self.success_samples) >= self.max_success_samples:
                logger.info(f"\nâœ… è¾¾åˆ°ç›®æ ‡æ ·æœ¬æ•°: {len(self.success_samples)}")
                break

        pbar.close()
        self._save_final()
        return self.stats

    def _process_single_request(self, raw_req, pbar):
        """å¤„ç†å•ä¸ªè¯·æ±‚"""
        self.stats["requests"] += 1
        pbar.update(1)

        # è°ƒè¯•è¾“å‡ºï¼ˆå¯é€‰ï¼Œæ”¶é›†å®Œæˆåå¯ä»¥åˆ é™¤ï¼‰
        if self.stats["requests"] <= 5:
            print(f"\n{'=' * 60}")
            print(f"DEBUG è¯·æ±‚ {self.stats['requests']}")
            print(f"raw_reqç±»å‹: {type(raw_req)}")
            print(f"raw_reqå†…å®¹: {raw_req}")
            print(f"{'=' * 60}\n")

        req = self._convert_request_indices(raw_req)

        if self.stats["requests"] <= 5:
            print(f"è½¬æ¢åreq: {req}")
            print(f"å¸¦å®½: {req.get('bandwidth')}")

        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå¤„ç†æ—¶é—´æ§½å˜åŒ–å¹¶é‡Šæ”¾è¿‡æœŸèµ„æº
        if self.use_timeslot:
            current_slot = req.get('time_slot', 0)
            if current_slot != self.timeslot_stats['current_time_slot']:
                self.timeslot_stats['current_time_slot'] = current_slot
                self.timeslot_stats['total_time_slots'] += 1

                # ğŸ”¥ é‡Šæ”¾è¿‡æœŸè¯·æ±‚çš„èµ„æº
                try:
                    if hasattr(self.env, 'event_handler') and hasattr(self.env.event_handler, 'services'):
                        expired_services = []
                        for service_id, service_info in list(self.env.event_handler.services.items()):
                            service_req = service_info.get('req', {})
                            leave_slot = service_req.get('leave_time_slot', float('inf'))

                            # å¦‚æœè¯·æ±‚å·²è¿‡æœŸï¼Œæ ‡è®°ä¸ºé‡Šæ”¾
                            if leave_slot <= current_slot:
                                expired_services.append(service_id)

                        # é‡Šæ”¾è¿‡æœŸæœåŠ¡
                        for service_id in expired_services:
                            try:
                                self.env.event_handler.unregister_service(service_id)
                            except Exception as e:
                                # å¿½ç•¥é‡Šæ”¾å¤±è´¥
                                pass

                        # æ‰“å°é‡Šæ”¾ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
                        if expired_services and self.stats["requests"] % 100 == 0:
                            print(f"\n[æ—¶é—´æ§½ {current_slot}] é‡Šæ”¾äº† {len(expired_services)} ä¸ªè¿‡æœŸæœåŠ¡")

                except Exception as e:
                    # å¦‚æœé‡Šæ”¾å¤±è´¥ï¼Œè®°å½•ä½†ä¸ä¸­æ–­
                    pass

        # ä¸“å®¶æ±‚è§£
        network_state = self.env.resource_mgr.get_network_state_dict(req)
        expert_result = self.expert.solve_request_for_expert(req, network_state)

        success = False
        if expert_result is not None:
            tree_data, expert_traj = expert_result

            if tree_data is not None:
                dict_tree = {}
                paths_map = tree_data.get('paths_map', {})

                # ğŸ”¥ æ£€æŸ¥ paths_map æ˜¯å¦ä¸ºç©º
                if not paths_map:
                    self.stats["fail"] += 1
                    # æ›´æ–°è¿›åº¦æ¡
                    br = self.stats["fail"] / max(1, self.stats["requests"])
                    pbar.set_postfix({
                        "reqs": self.stats["requests"],
                        "succ": self.stats["success"],
                        "samples": len(self.success_samples),
                        "BR": f"{br:.1%}"
                    })
                    return  # ç›´æ¥è¿”å›

                deployment_plan = {'hvt': tree_data['hvt'], 'tree': dict_tree}

                for dest, path_nodes in paths_map.items():
                    path_0 = [n - 1 if n > 0 else 0 for n in path_nodes]
                    for i in range(len(path_0) - 1):
                        u, v = path_0[i], path_0[i + 1]
                        dict_tree[(u, v)] = 1.0
                        dict_tree[(v, u)] = 1.0

                if self.env.resource_mgr.apply_tree_deployment(deployment_plan, req):
                    success = True
                    self.stats["success"] += 1

                    req_id = req.get('id', self.stats["requests"])
                    self.env.event_handler.services[req_id] = {
                        'req': req, 'tree': deployment_plan, 'hvt': tree_data['hvt']
                    }

                    # æ„é€ æ ·æœ¬
                    self.env.current_request = req
                    dummy_tree = {'tree': {}, 'hvt': np.zeros((self.env.n, 8))}
                    x, edge_index, edge_attr, req_vec = self.env.resource_mgr.get_graph_state(
                        current_request=req, nodes_on_tree={req['source']},
                        current_tree=dummy_tree, served_dest_count=0,
                        sharing_strategy=0, nb_high_goals=10
                    )
                    state_to_save = Data(
                        x=x.cpu(), edge_index=edge_index.cpu(),
                        edge_attr=edge_attr.cpu(), req_vec=req_vec.cpu()
                    )

                    clean_req = req.copy()

                    # ä¸ºæ¯ä¸ªè·¯å¾„åˆ›å»ºæ ·æœ¬
                    for dest, path_nodes in paths_map.items():
                        path_0 = [int(n - 1 if n > 0 else 0) for n in path_nodes]

                        sample_data = {
                            "state": state_to_save,
                            "request": clean_req,
                            "action": {"path": path_0},
                            "cost": 0.0,
                            "load": self._estimate_load(),
                            "hrl_info": {
                                "subgoal": int(path_0[-1]),
                                "full_path": path_0
                            }
                        }

                        if self.use_timeslot:
                            sample_data["timeslot_info"] = {
                                "time_slot": req.get('time_slot', 0),
                                "duration": req.get('duration', 100),
                                "leave_time_slot": req.get('leave_time_slot', 100)
                            }

                        self.success_samples.append(sample_data)
                        self.stats["paths_collected"] += 1

        if not success:
            self.stats["fail"] += 1

        # æ›´æ–°è¿›åº¦æ¡
        br = self.stats["fail"] / max(1, self.stats["requests"])
        pbar.set_postfix({
            "reqs": self.stats["requests"],
            "succ": self.stats["success"],
            "samples": len(self.success_samples),
            "BR": f"{br:.1%}"
        })
    def _save_final(self):
        """ä¿å­˜æ•°æ®"""
        path = os.path.join(self.output_dir, "expert_data_final.pkl")
        try:
            data_to_save = {
                "success": self.success_samples,
                "stats": self.stats
            }

            if self.use_timeslot:
                data_to_save["timeslot_stats"] = self.timeslot_stats

            with open(path, "wb") as f:
                pickle.dump(data_to_save, f)

            logger.info(f"âœ… Saved {len(self.success_samples)} expert samples to {path}")
            logger.info(f"   æˆåŠŸè¯·æ±‚: {self.stats['success']} ä¸ª")
            logger.info(f"   æ”¶é›†è·¯å¾„: {self.stats['paths_collected']} æ¡")
            logger.info(f"   æ ·æœ¬æ€»æ•°: {len(self.success_samples)} ä¸ª")

            if self.use_timeslot:
                logger.info(f"â° æ—¶é—´æ§½ç»Ÿè®¡:")
                logger.info(f"   æ€»æ—¶é—´æ§½: {self.timeslot_stats['total_time_slots']}")
        except Exception as e:
            logger.error(f"âŒ Save failed: {e}")
            import traceback
            traceback.print_exc()