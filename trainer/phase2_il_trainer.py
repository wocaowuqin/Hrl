#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Phase 2 Imitation Learning Trainer (HRL é€‚é…ç‰ˆ)
====================================================
åŠŸèƒ½å‡çº§ï¼š
1. âœ… æ”¯æŒ HRLAgent åŒç­–ç•¥æ¶æ„ (High + Low)
2. âœ… è‡ªåŠ¨ä»ä¸“å®¶è·¯å¾„ä¸­æå– Subgoal Index (High-Level Label)
3. âœ… åŒé‡ Loss è”åˆè®­ç»ƒ (High-Level é¢„æµ‹ç›®æ ‡ç´¢å¼• + Low-Level é¢„æµ‹è·¯å¾„)
====================================================
"""

import os
import pickle
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset, random_split
from torch_geometric.data import Batch
from pathlib import Path
import logging
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from tqdm import tqdm
import platform
from torch_geometric.loader import DataLoader as PyGDataLoader
logger = logging.getLogger(__name__)


class EarlyStopping:
    def __init__(self, patience: int = 20, min_delta: float = 0.0001):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
        self.early_stop = False

    def __call__(self, val_loss: float) -> bool:
        if self.best_loss is None:
            self.best_loss = val_loss
            return False

        if val_loss > self.best_loss - self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
                return True
        else:
            self.best_loss = val_loss
            self.counter = 0

        return False


class ExpertDataset(Dataset):
    def __init__(self, expert_data_path: str):
        self.samples = []
        self._load_and_convert(expert_data_path)

    def _load_and_convert(self, data_path: str):
        logger.info(f"ğŸ“‚ åŠ è½½ä¸“å®¶æ•°æ®: {data_path}")

        if not os.path.exists(data_path):
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")

        with open(data_path, 'rb') as f:
            raw_data = pickle.load(f)

        if isinstance(raw_data, dict):
            transitions = raw_data.get('success', raw_data.get('data', []))
        elif isinstance(raw_data, list):
            transitions = raw_data
        else:
            raise ValueError(f"æœªçŸ¥çš„æ•°æ®æ ¼å¼: {type(raw_data)}")

        if len(transitions) == 0:
            logger.error("âŒ æ²¡æœ‰å¯ç”¨çš„è®­ç»ƒæ•°æ®ï¼")
            return

        converted = 0
        skipped = 0

        for i, trans in enumerate(transitions):
            try:
                # æ£€æŸ¥æ˜¯å¦åŒ…å« path ä¿¡æ¯ (è¿™æ˜¯ HRL è®­ç»ƒå¿…éœ€çš„)
                action_data = trans.get('action')
                if isinstance(action_data, dict) and 'path' in action_data:
                    converted_samples = self._convert_path_to_steps(trans)
                    self.samples.extend(converted_samples)
                    converted += len(converted_samples)
                else:
                    # å¦‚æœåªæ˜¯ç®€å•çš„ state->action å¯¹ï¼Œæ— æ³•æ¨æ–­ subgoalï¼Œè·³è¿‡
                    skipped += 1
            except Exception as e:
                skipped += 1

        logger.info(f"âœ… æ•°æ®è½¬æ¢å®Œæˆ:")
        logger.info(f"  - ç”Ÿæˆæ ·æœ¬æ•°: {converted} (Stepçº§åˆ«)")
        logger.info(f"  - è·³è¿‡æ ·æœ¬æ•°: {skipped} (æ ¼å¼ä¸ç¬¦)")
        logger.info(f"  - æ€»è®­ç»ƒæ ·æœ¬: {len(self.samples)}")

    def _convert_path_to_steps(self, trans: Dict) -> List[Dict]:
        """
        å°†ä¸€æ¡å®Œæ•´è·¯å¾„æ‹†è§£ä¸ºå¤šä¸ªè®­ç»ƒæ ·æœ¬ï¼š
        1. High-Level Label: è¿™æ¡è·¯çš„ç»ˆç‚¹æ˜¯ç¬¬å‡ ä¸ªç›®çš„åœ°ï¼Ÿ
        2. Low-Level Label: å½“å‰èŠ‚ç‚¹çš„ä¸‹ä¸€è·³æ˜¯è°ï¼Ÿ

        ğŸ”¥ ä¼˜åŒ–ï¼šåªä¿ç•™ç»ˆç‚¹åœ¨destä¸­çš„è·¯å¾„ï¼Œè¿‡æ»¤Hubè·¯å¾„
        """
        path = trans['action']['path']
        req = trans.get('request', {})  # Phase 1 å¿…é¡»ä¿å­˜ request ä¿¡æ¯

        if not path or len(path) < 2:
            return []

        steps = []

        # ğŸ”¥ [ä¼˜åŒ–] ç¡®å®š Subgoal å¹¶è¿‡æ»¤
        subgoal_node = int(path[-1])
        if subgoal_node >= 28: subgoal_node %= 28

        # è·å–ç›®çš„åœ°åˆ—è¡¨
        dest_list = req.get('dest', [])

        # ğŸ”¥ [å…³é”®ä¼˜åŒ–] åªä¿ç•™ç»ˆç‚¹åœ¨destä¸­çš„è·¯å¾„
        if subgoal_node not in dest_list:
            # è¿™æ˜¯åˆ°Hubçš„ä¸­é—´è·¯å¾„ï¼Œè·³è¿‡
            return []

        # ç¡®å®š High Action Index
        try:
            high_action_idx = dest_list.index(subgoal_node)
            if high_action_idx >= 10:
                high_action_idx = 0
        except ValueError:
            # ç†è®ºä¸Šä¸åº”è¯¥åˆ°è¿™é‡Œï¼Œå› ä¸ºå·²ç»æ£€æŸ¥è¿‡äº†
            return []

        # 3. ç”Ÿæˆæ¯ä¸€æ­¥çš„æ ·æœ¬
        for step_idx in range(len(path) - 1):
            curr_node = int(path[step_idx])
            next_node = int(path[step_idx + 1])  # Low-Level Label

            # èŠ‚ç‚¹ ID ä¿®æ­£
            if curr_node >= 28: curr_node %= 28
            if next_node >= 28: next_node %= 28

            step_trans = {
                'state': trans.get('state'),  # GNN Data Object
                'high_label': high_action_idx,  # High-Level ç›‘ç£ä¿¡å·
                'low_label': next_node  # Low-Level ç›‘ç£ä¿¡å·
            }
            steps.append(step_trans)

        return steps

    def __len__(self):
        return len(self.samples)

    # ä¿®æ”¹ phase2_il_trainer.py ä¸­çš„ ExpertDataset.__getitem__
    def __getitem__(self, index):
        sample = self.samples[index]
        state = sample['state']

        # ğŸ”¥ [æ ¸å¿ƒä¿®å¤] å¦‚æœæ—§çš„ä¸“å®¶æ•°æ®é‡Œæ²¡å­˜ maskï¼Œå®æ—¶è¡¥ä¸€ä¸ªå…¨ 1 çš„æ©ç é˜²æ­¢å´©æºƒ
        # æˆ–è€…å¦‚æœä½ èƒ½è®¿é—® envï¼Œå¯ä»¥è°ƒç”¨ env.get_low_level_action_mask()
        if not hasattr(state, 'action_mask'):
            # è¿™é‡Œçš„ 28 æ˜¯ä½ çš„åŠ¨ä½œç©ºé—´ç»´åº¦ï¼ˆæ ¹æ®æ—¥å¿—æç¤ºï¼‰
            state.action_mask = torch.ones((1, 28), dtype=torch.float32)

        return {
            'state': state,
            'high_label': torch.tensor(sample['high_label'], dtype=torch.long),
            'low_label': torch.tensor(sample['low_label'], dtype=torch.long)
        }


class Phase2ILTrainer:
    def __init__(self, env, agent, expert_data_path: str, output_dir: str, config: dict):
        self.env = env
        self.agent = agent
        self.cfg = config
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self._loss_high_sum = 0.0
        self._loss_low_sum = 0.0
        self._loss_count = 0
        # è®­ç»ƒå‚æ•°
        phase2_cfg = config.get('phase2', {})
        self.epochs = phase2_cfg.get('epochs', 200)
        self.batch_size = phase2_cfg.get('batch_size', 64)
        self.validation_split = phase2_cfg.get('validation_split', 0.1)
        self.device = agent.device

        # ğŸ”¥ [å…³é”®] æ£€æµ‹ Agent ç±»å‹å¹¶è·å– High/Low Policy
        self.is_hrl = hasattr(agent, 'high_policy') and hasattr(agent, 'low_policy')

        if self.is_hrl:
            logger.info("âœ… Phase 2: æ£€æµ‹åˆ° HRL Agentï¼Œå‡†å¤‡è¿›è¡ŒåŒå±‚ç­–ç•¥è®­ç»ƒ")
            self.model_high = agent.high_policy
            self.model_low = agent.low_policy
            self.optimizer_high = agent.optimizer_high
            self.optimizer_low = agent.optimizer_low

            # ğŸ”¥ [ä¼˜åŒ–] æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆç§»é™¤verboseå‚æ•°ä»¥å…¼å®¹æ—§ç‰ˆPyTorchï¼‰
            self.scheduler_high = torch.optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer_high, mode='min', factor=0.5, patience=10, min_lr=1e-6
            )
            self.scheduler_low = torch.optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer_low, mode='min', factor=0.5, patience=10, min_lr=1e-6
            )
        else:
            logger.warning("âš ï¸ Phase 2: æ£€æµ‹åˆ°æ—§ç‰ˆ Agentï¼Œä»…è®­ç»ƒ PolicyNet")
            self.model = agent.policy_net
            self.optimizer = agent.optimizer
            self.scheduler = None

        self.criterion = nn.CrossEntropyLoss()

        # æ•°æ®åŠ è½½
        self.num_workers = 0 if platform.system() == 'Windows' else 4
        self._prepare_data(expert_data_path)

        # æ—©åœ
        self.early_stopping = EarlyStopping(patience=20)

    def _prepare_data(self, data_path):
        full_dataset = ExpertDataset(data_path)
        if len(full_dataset) == 0:
            self.train_loader = None
            return

        val_size = int(len(full_dataset) * self.validation_split)
        train_size = len(full_dataset) - val_size

        train_dataset, val_dataset = random_split(
            full_dataset, [train_size, val_size],
            generator=torch.Generator().manual_seed(42)
        )

        self.train_loader = DataLoader(
            train_dataset, batch_size=self.batch_size, shuffle=True,
            num_workers=self.num_workers, collate_fn=self._collate_fn,
            drop_last=True  # é¿å…æœ€åä¸€ä¸ª Batch åªæœ‰ 1 ä¸ªæ ·æœ¬å¯¼è‡´ BatchNorm æŠ¥é”™
        )
        self.val_loader = DataLoader(
            val_dataset, batch_size=self.batch_size, shuffle=False,
            num_workers=self.num_workers, collate_fn=self._collate_fn,
            drop_last=True
        )

    def _collate_fn(self, batch):
        states = []
        high_labels = []
        low_labels = []

        for item in batch:
            state = item.get('state')
            if state is None: continue

            states.append(state)
            high_labels.append(item['high_label'])
            low_labels.append(item['low_label'])

        if not states: return None

        graph_batch = Batch.from_data_list(states)
        high_labels = torch.tensor(high_labels, dtype=torch.long)
        low_labels = torch.tensor(low_labels, dtype=torch.long)

        return graph_batch, high_labels, low_labels

    def run(self):
        """ä¸»è¿è¡Œå¾ªç¯"""
        if not self.train_loader:
            logger.error("âŒ æ•°æ®æœªå°±ç»ªï¼Œåœæ­¢è®­ç»ƒ")
            return

        logger.info("ğŸš€ å¼€å§‹ Phase 2 æ¨¡ä»¿å­¦ä¹  (HRL Mode)...")
        for epoch in range(1, self.epochs + 1):
            train_loss = self._train_epoch(epoch)

            # æ—¥å¿—ç»Ÿè®¡
            if self._loss_count > 0 and epoch % 10 == 0:
                avg_h = self._loss_high_sum / self._loss_count
                avg_l = self._loss_low_sum / self._loss_count
                logger.info(f"ğŸ“Š Epoch {epoch} åˆ†ç¦»Loss: High={avg_h:.4f}, Low={avg_l:.4f}")

            if epoch % 10 == 0:
                self._save_checkpoint(epoch)

        self._save_checkpoint("final")
        logger.info("âœ… Phase 2 å®Œæˆ")

    def _train_epoch(self, epoch):
        """
        ğŸ”¥ [IL V3.1 ç»ˆæè‡ªæ„ˆç‰ˆ]
        1. ä¿®å¤ Encoder å±æ€§ç¼ºå¤±ï¼šæ”¹ç”¨ Agent å°è£…çš„ _get_graph_embedding æ¥å£
        2. ä¿®å¤æ©ç ç¼ºå¤±ï¼šåŠ å…¥ hasattr æ£€æŸ¥
        3. å¼ºåŒ– Lossï¼šä¿ç•™ 15.0 å€éæ³•åŠ¨ä½œæƒ©ç½š
        """
        self.model_high.train()
        self.model_low.train()

        # åˆå§‹åŒ–ç»Ÿè®¡å˜é‡ (è§£å†³ _loss_count æŠ¥é”™)
        self._loss_high_sum = 0.0
        self._loss_low_sum = 0.0
        self._loss_count = 0
        total_loss = 0

        pbar = tqdm(self.train_loader, desc=f"Epoch {epoch}")
        for batch_data in pbar:
            # 1. è‡ªåŠ¨é€‚é… dict/tuple æ•°æ®è§£åŒ…
            if isinstance(batch_data, dict):
                states = batch_data['state'].to(self.device)
                high_labels = batch_data['high_label'].to(self.device)
                low_labels = batch_data['low_label'].to(self.device)
            else:
                states, high_labels, low_labels = batch_data
                states = states.to(self.device)
                high_labels = high_labels.to(self.device)
                low_labels = low_labels.to(self.device)

            self.optimizer_high.zero_grad()
            self.optimizer_low.zero_grad()

            # 2. ğŸ”¥ã€æ ¸å¿ƒä¿®å¤ã€‘è°ƒç”¨ Agent å†…éƒ¨ç¨³å®šçš„åµŒå…¥æ¥å£
            # agent._get_graph_embedding å†…éƒ¨å¤„ç†äº†å¯¹ self.encoder çš„è°ƒç”¨å’Œ None æ£€æŸ¥
            graph_emb = self.agent._get_graph_embedding(states)

            # 3. å‰å‘é¢„æµ‹
            high_logits, subgoal_emb, _ = self.model_high(graph_emb, return_subgoal=True)
            low_logits, _ = self.model_low(graph_emb, subgoal_emb)

            # 4. åŠ¨ä½œæ©ç å¤„ç† (æ”¯æŒæ—§æ•°æ®å…¼å®¹)
            if hasattr(states, 'action_mask'):
                action_masks = states.action_mask.float()
            else:
                # å¦‚æœ states é‡Œæ²¡æœ‰æ©ç ï¼Œä½¿ç”¨å…¨ 1 æ©ç ï¼ˆä¸æƒ©ç½šä½†ä¿è¯é€»è¾‘è¿è¡Œï¼‰
                action_masks = torch.ones((states.num_graphs, low_logits.size(-1)), device=self.device)

            # 5. æŸå¤±è®¡ç®—
            loss_high = self.criterion(high_logits, high_labels)
            loss_low_bc = self.criterion(low_logits, low_labels)

            # éæ³•åŠ¨ä½œæŠ‘åˆ¶ï¼šæƒ©ç½šæ¨¡å‹åœ¨ Masked ä¸º 0 çš„èŠ‚ç‚¹ä¸Šåˆ†é…çš„æ¦‚ç‡
            low_probs = torch.softmax(low_logits, dim=-1)
            illegal_penalty = (low_probs * (1.0 - action_masks)).sum(dim=-1).mean()

            # ç»¼åˆæŸå¤± (ä¿ç•™ 15.0x éæ³•æƒ©ç½šæƒé‡)
            loss = loss_high * 0.5 + loss_low_bc + 15.0 * illegal_penalty

            # 6. åå‘ä¼ æ’­ä¸ä¼˜åŒ–
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model_high.parameters(), 1.0)
            torch.nn.utils.clip_grad_norm_(self.model_low.parameters(), 1.0)
            self.optimizer_high.step()
            self.optimizer_low.step()

            # è®°å½•ç»Ÿè®¡
            self._loss_high_sum += loss_high.item()
            self._loss_low_sum += loss_low_bc.item()
            self._loss_count += 1
            total_loss += loss.item()

            pbar.set_postfix({'L': f"{loss.item():.3f}", 'P': f"{illegal_penalty.item():.3f}"})

        return total_loss / max(1, self._loss_count)
    def _validate_epoch(self, epoch):
        # ğŸ”¥ [è°ƒè¯•] æ‰“å°åˆ†ç¦»çš„Loss
        if hasattr(self, '_loss_count') and self._loss_count > 0:
            avg_high = self._loss_high_sum / self._loss_count
            avg_low = self._loss_low_sum / self._loss_count
            if epoch % 10 == 0:  # æ¯10ä¸ªepochæ‰“å°
                logger.info(f"   ğŸ“Š åˆ†ç¦»Loss: High={avg_high:.4f}, Low={avg_low:.4f}")
            # é‡ç½®
            self._loss_high_sum = 0
            self._loss_low_sum = 0
            self._loss_count = 0

        total_loss = 0
        count = 0

        if self.is_hrl:
            self.model_high.eval()
            self.model_low.eval()

        with torch.no_grad():
            for batch in self.val_loader:
                if not batch: continue
                states, high_labels, low_labels = batch
                states = states.to(self.device)
                high_labels = high_labels.to(self.device)
                low_labels = low_labels.to(self.device)

                if self.is_hrl:
                    if self.agent.encoder:
                        graph_emb = self.agent.encoder(states.x, states.edge_index, states.batch)
                    else:
                        graph_emb = self.agent._get_graph_embedding(states)

                    high_logits, subgoal_emb, _ = self.model_high(graph_emb, return_subgoal=True)
                    low_logits, _ = self.model_low(graph_emb, subgoal_emb)

                    loss = self.criterion(high_logits, high_labels) * 0.5 + \
                           self.criterion(low_logits, low_labels)
                else:
                    loss = torch.tensor(0.0)

                total_loss += loss.item()
                count += 1

        if self.is_hrl:
            self.model_high.train()
            self.model_low.train()

        return total_loss / max(1, count)

    def _save_checkpoint(self, tag):
        path = self.output_dir / f"il_model_{tag}.pth"

        save_dict = {
            'config': self.cfg,
        }

        if self.is_hrl:
            save_dict.update({
                'high_policy': self.model_high.state_dict(),
                'low_policy': self.model_low.state_dict(),
                'optimizer_high': self.optimizer_high.state_dict(),
                'optimizer_low': self.optimizer_low.state_dict(),
            })
        else:
            save_dict['model_state_dict'] = self.model.state_dict()

        torch.save(save_dict, path)
        logger.info(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {path}")