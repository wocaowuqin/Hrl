import numpy as np
import torch
from collections import defaultdict, deque
import time
import logging

logger = logging.getLogger(__name__)


class HRL_Coordinator:
    """
    ğŸ® HRLæ—¶åºåè°ƒå™¨ - è§£å†³æ—¶é—´å°ºåº¦ä¸åŒ¹é…é—®é¢˜

    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. ç®¡ç†é«˜å±‚å’Œä½å±‚çš„æ‰§è¡Œæ—¶åº
    2. ç¡®ä¿ State(t) â†’ High Action â†’ Low Execution â†’ State(t+1) é—­ç¯
    3. åè°ƒé«˜å±‚å’Œä½å±‚çš„çŠ¶æ€åŒæ­¥
    4. ç»Ÿè®¡å’Œç›‘æ§è®­ç»ƒè¿‡ç¨‹
    """

    def __init__(self, env, high_agent, low_agent, config=None):
        """
        åˆå§‹åŒ–åè°ƒå™¨

        Args:
            env: SFC_HIRL_Env ç¯å¢ƒå®ä¾‹
            high_agent: é«˜å±‚Agent
            low_agent: ä½å±‚Agent
            config: é…ç½®å­—å…¸
        """
        self.env = env
        self.high_agent = high_agent
        self.low_agent = low_agent

        # é…ç½®å‚æ•°
        self.config = config or {}
        self.max_low_steps = config.get('max_low_steps', 100)
        self.max_high_steps = config.get('max_high_steps', 100)
        self.use_masking = config.get('use_masking', True)

        # çŠ¶æ€åŒæ­¥ç¼“å­˜
        self.current_high_state = None
        self.current_low_state = None
        self.last_high_action = None
        self.last_high_reward = 0.0

        # æ‰§è¡Œç»Ÿè®¡
        self.stats = defaultdict(int)
        self.episode_stats = {
            'total_reward': 0.0,
            'high_steps': 0,
            'low_steps': 0,
            'vnf_deployments': 0,
            'dest_connections': 0,
            'failures': 0,
            'success': False
        }

        # å†å²è®°å½•ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        self.history = {
            'high_actions': [],
            'low_actions': [],
            'rewards': [],
            'states': []
        }

        self.current_goal = None  # å½“å‰é«˜å±‚ç›®æ ‡
        self.current_subgoal = None  # å½“å‰å­ç›®æ ‡
        self._low_step_count = 0  # ä½å±‚æ­¥æ•°è®¡æ•°å™¨
        self._last_phase = None  # ä¸Šä¸€æ¬¡çš„é˜¶æ®µ
        self.last_transition = None  # æœ€è¿‘çš„ transitionï¼ˆä¾› Trainer ä½¿ç”¨ï¼‰
        self.last_high_action = None  # æœ€è¿‘çš„é«˜å±‚åŠ¨ä½œ
        logger.info("âœ… HRLæ—¶åºåè°ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"é…ç½®: max_low_steps={self.max_low_steps}, max_high_steps={self.max_high_steps}")

    def reset(self):
        """é‡ç½®åè°ƒå™¨çŠ¶æ€"""
        self.current_high_state = None
        self.current_low_state = None
        self.last_high_action = None
        self.last_high_reward = 0.0

        # é‡ç½®episodeç»Ÿè®¡
        self.episode_stats = {
            'total_reward': 0.0,
            'high_steps': 0,
            'low_steps': 0,
            'vnf_deployments': 0,
            'dest_connections': 0,
            'failures': 0,
            'success': False
        }

        # æ¸…ç©ºå†å²è®°å½•
        self.history = {
            'high_actions': [],
            'low_actions': [],
            'rewards': [],
            'states': []
        }

        logger.debug("åè°ƒå™¨çŠ¶æ€å·²é‡ç½®")

    def run_high_low_cycle(self, training_mode=True):
        """
        æ‰§è¡Œä¸€ä¸ªå®Œæ•´çš„é«˜å±‚-ä½å±‚å¾ªç¯

        Returns:
            high_state: é«˜å±‚å½“å‰çŠ¶æ€
            high_action: é«˜å±‚åŠ¨ä½œ
            high_reward: é«˜å±‚å¥–åŠ±
            next_high_state: é«˜å±‚ä¸‹ä¸€ä¸ªçŠ¶æ€ï¼ˆåŒæ­¥åï¼‰
            high_done: é«˜å±‚æ˜¯å¦å®Œæˆ
            info: é™„åŠ ä¿¡æ¯
        """
        # ========================================
        # 1. æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›è¡Œé«˜å±‚å†³ç­–
        # ========================================
        need_high_decision = self._check_need_high_decision()

        if need_high_decision:
            # ========================================
            # 2. é«˜å±‚å†³ç­–é˜¶æ®µ
            # ========================================
            high_state = self._get_synchronized_high_state()

            # è·å–é«˜å±‚åŠ¨ä½œæ©ç 
            high_mask = None
            if self.use_masking:
                try:
                    high_mask = self.env.get_high_level_action_mask()
                except AttributeError:
                    logger.warning("ç¯å¢ƒä¸æ”¯æŒget_high_level_action_maskï¼Œå°†ä½¿ç”¨å…¨æ©ç ")
                    high_mask = np.ones(self.env.high_action_space, dtype=bool)

            # é«˜å±‚Agenté€‰æ‹©åŠ¨ä½œ
            high_action = self.high_agent.select_action(
                high_state,
                mask=high_mask,
                training=training_mode
            )

            self.last_high_action = high_action
            self.history['high_actions'].append(high_action)

            # ========================================
            # 3. æ‰§è¡Œé«˜å±‚åŠ¨ä½œï¼ˆåªè®¾å®šç›®æ ‡ï¼‰
            # ========================================
            start_time = time.time()

            _, _, high_done, _, high_info = self.env.step_high_level(high_action)

            high_decision_time = time.time() - start_time

            # è®°å½•é«˜å±‚å†³ç­–
            self.stats['high_decisions'] += 1
            self.episode_stats['high_steps'] += 1

            logger.info(
                f"ğŸ” [é«˜å±‚å†³ç­–] æ­¥éª¤{self.stats['high_decisions']}ï¼ŒåŠ¨ä½œ={high_action}ï¼Œç”¨æ—¶={high_decision_time:.3f}s")

            # å¦‚æœé«˜å±‚ä»»åŠ¡å®Œæˆï¼ˆæ‰€æœ‰ç›®çš„åœ°å·²è¿æ¥ï¼‰
            if high_done:
                logger.info("âœ… é«˜å±‚ä»»åŠ¡å®Œæˆï¼šæ‰€æœ‰ç›®çš„åœ°å·²è¿æ¥")

                # è·å–æœ€ç»ˆçŠ¶æ€
                final_state = self.env.get_high_level_state_graph()

                return (
                    high_state,
                    high_action,
                    100.0,  # å®Œæˆå¥–åŠ±
                    final_state,
                    True,
                    {
                        'episode_complete': True,
                        'high_done': True,
                        'message': 'æ‰€æœ‰ç›®çš„åœ°è¿æ¥å®Œæˆ'
                    }
                )

            # ========================================
            # 4. ä½å±‚æ‰§è¡Œå¾ªç¯ï¼ˆåŒæ­¥å…³é”®ï¼‰
            # ========================================
            low_execution_result = self._execute_low_level_loop(training_mode)

            # æ›´æ–°ç»Ÿè®¡
            self.episode_stats['low_steps'] += low_execution_result['steps']
            self.episode_stats['total_reward'] += low_execution_result['total_reward']

            # ========================================
            # 5. è·å–åŒæ­¥åçš„ä¸‹ä¸€ä¸ªé«˜å±‚çŠ¶æ€
            # ========================================
            # ğŸ”¥ å…³é”®ï¼šä½å±‚æ‰§è¡Œå®Œåï¼ŒçŠ¶æ€å·²æ›´æ–°ï¼Œç°åœ¨è·å–åŒæ­¥åçš„é«˜å±‚çŠ¶æ€
            next_high_state = self._get_synchronized_high_state()

            # ========================================
            # 6. è®¡ç®—é«˜å±‚å¥–åŠ±
            # ========================================
            high_reward = self._calculate_high_reward(
                low_execution_result['total_reward'],
                low_execution_result['info'],
                low_execution_result['steps']
            )

            self.last_high_reward = high_reward
            self.history['rewards'].append(high_reward)

            # ========================================
            # 7. è¿”å›ç»“æœ
            # ========================================
            return (
                high_state,
                high_action,
                high_reward,
                next_high_state,
                False,  # high_done
                {
                    **high_info,
                    **low_execution_result['info'],
                    'low_steps': low_execution_result['steps'],
                    'low_total_reward': low_execution_result['total_reward'],
                    'high_reward': high_reward,
                    'execution_time': high_decision_time + low_execution_result['execution_time']
                }
            )
        else:
            # ä¸éœ€è¦é«˜å±‚å†³ç­–ï¼Œç›´æ¥è¿”å›å½“å‰çŠ¶æ€
            current_state = self._get_synchronized_high_state()
            return (
                current_state,
                None,
                0.0,
                current_state,
                False,
                {'message': 'ç­‰å¾…ä½å±‚æ‰§è¡Œå®Œæˆ'}
            )

    def _execute_low_level_loop(self, training_mode=True):
        """
        æ‰§è¡Œä½å±‚å¾ªç¯ï¼Œç›´åˆ°å­ä»»åŠ¡å®Œæˆæˆ–è¾¾åˆ°æœ€å¤§æ­¥æ•°

        Returns:
            dict: åŒ…å«æ€»å¥–åŠ±ã€æ­¥æ•°ã€æ‰§è¡Œæ—¶é—´ç­‰ä¿¡æ¯
        """
        logger.info("âš™ï¸ [ä½å±‚æ‰§è¡Œ] å¼€å§‹æ‰§è¡Œé«˜å±‚æŒ‡ä»¤...")

        start_time = time.time()
        total_reward = 0.0
        step_count = 0
        low_done = False
        last_info = {}

        # ä½å±‚æ‰§è¡Œå¾ªç¯
        while not low_done and step_count < self.max_low_steps:
            step_count += 1

            # ========================================
            # a. è·å–ä½å±‚çŠ¶æ€
            # ========================================
            low_state = self.env.get_state()

            # ========================================
            # b. è·å–ä½å±‚åŠ¨ä½œæ©ç 
            # ========================================
            low_mask = None
            if self.use_masking:
                try:
                    low_mask = self.env.get_low_level_action_mask()
                except AttributeError:
                    logger.warning("ç¯å¢ƒä¸æ”¯æŒget_low_level_action_maskï¼Œå°†ä½¿ç”¨å…¨æ©ç ")
                    low_mask = np.ones(self.env.n, dtype=bool)

            # ========================================
            # c. ä½å±‚Agenté€‰æ‹©åŠ¨ä½œ
            # ========================================
            low_action = self.low_agent.select_action(
                low_state,
                mask=low_mask,
                training=training_mode
            )

            self.history['low_actions'].append(low_action)

            # ========================================
            # d. æ‰§è¡Œä½å±‚åŠ¨ä½œ
            # ========================================
            _, low_reward, low_terminated, low_truncated, low_info = \
                self.env.step_low_level(low_action)

            total_reward += low_reward

            # ========================================
            # e. æ£€æŸ¥ä½å±‚ä»»åŠ¡æ˜¯å¦å®Œæˆ
            # ========================================
            last_info = low_info

            # æˆåŠŸå®Œæˆéƒ¨ç½²
            if low_info.get('deploy_success', False):
                self.episode_stats['vnf_deployments'] += 1
                self.stats['successful_deployments'] += 1
                low_done = True
                logger.info(f"âœ… ä½å±‚ä»»åŠ¡å®Œæˆ: VNFéƒ¨ç½²æˆåŠŸ (æ­¥éª¤{step_count})")

            # æˆåŠŸè¿æ¥ç›®çš„åœ°
            elif low_info.get('connection_success', False):
                self.episode_stats['dest_connections'] += 1
                self.stats['successful_connections'] += 1
                low_done = True
                logger.info(f"âœ… ä½å±‚ä»»åŠ¡å®Œæˆ: ç›®çš„åœ°è¿æ¥æˆåŠŸ (æ­¥éª¤{step_count})")

            # å¤±è´¥æƒ…å†µ
            elif low_info.get('deploy_fail', False):
                self.episode_stats['failures'] += 1
                self.stats['failed_deployments'] += 1
                low_done = True
                logger.warning(f"âŒ ä½å±‚ä»»åŠ¡å¤±è´¥: VNFéƒ¨ç½²å¤±è´¥")

            elif low_info.get('connection_fail', False):
                self.episode_stats['failures'] += 1
                self.stats['failed_connections'] += 1
                low_done = True
                logger.warning(f"âŒ ä½å±‚ä»»åŠ¡å¤±è´¥: ç›®çš„åœ°è¿æ¥å¤±è´¥")

            elif low_info.get('path_fail', False):
                self.episode_stats['failures'] += 1
                self.stats['failed_paths'] += 1
                low_done = True
                logger.warning(f"âŒ ä½å±‚ä»»åŠ¡å¤±è´¥: è·¯å¾„å»ºç«‹å¤±è´¥")

            # è¶…æ—¶æˆ–éæ³•åŠ¨ä½œ
            elif low_info.get('timeout', False) or low_info.get('invalid', False):
                self.episode_stats['failures'] += 1
                self.stats['timeouts'] += 1
                low_done = True
                logger.warning(f"â° ä½å±‚ä»»åŠ¡è¶…æ—¶æˆ–éæ³•åŠ¨ä½œ")

            # ä½å±‚è‡ªå·±çš„ç»ˆæ­¢æ¡ä»¶
            elif low_terminated or low_truncated:
                low_done = True
                logger.info(f"ğŸ›‘ ä½å±‚ä»»åŠ¡ç»ˆæ­¢")

            # æ˜¾ç¤ºè¿›åº¦
            if step_count % 20 == 0:
                logger.debug(f"  ä½å±‚æ‰§è¡Œä¸­... æ­¥æ•°: {step_count}, ç´¯è®¡å¥–åŠ±: {total_reward:.2f}")

        execution_time = time.time() - start_time

        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§æ­¥æ•°
        if step_count >= self.max_low_steps and not low_done:
            logger.warning(f"âš ï¸ ä½å±‚æ‰§è¡Œè¾¾åˆ°æœ€å¤§æ­¥æ•°é™åˆ¶ ({self.max_low_steps})")
            self.episode_stats['failures'] += 1
            self.stats['max_steps_exceeded'] += 1

        logger.info(f"âš™ï¸ [ä½å±‚æ‰§è¡Œ] å®Œæˆï¼Œæ­¥æ•°={step_count}, æ€»å¥–åŠ±={total_reward:.2f}, ç”¨æ—¶={execution_time:.3f}s")

        return {
            'total_reward': total_reward,
            'steps': step_count,
            'execution_time': execution_time,
            'done': low_done,
            'info': last_info
        }

    def _check_need_high_decision(self):
        """
        æ£€æŸ¥å½“å‰æ˜¯å¦éœ€è¦é«˜å±‚å†³ç­–

        éœ€è¦é«˜å±‚å†³ç­–çš„æ¡ä»¶ï¼š
        1. å½“å‰æ²¡æœ‰æ­£åœ¨æ‰§è¡Œçš„ä½å±‚ä»»åŠ¡
        2. ä¸Šä¸€ä¸ªä½å±‚ä»»åŠ¡å·²å®Œæˆ
        3. å½“å‰é˜¶æ®µéœ€è¦æ–°çš„ç›®æ ‡
        """
        # è·å–å½“å‰ç¯å¢ƒçŠ¶æ€
        current_phase = getattr(self.env, 'current_phase', None)
        current_target = getattr(self.env, 'current_target_node', None)
        current_deployment = getattr(self.env, 'current_deployment_target', None)

        # å¦‚æœæ²¡æœ‰å½“å‰é˜¶æ®µï¼Œéœ€è¦é«˜å±‚å†³ç­–
        if current_phase is None:
            return True

        # æ£€æŸ¥VNFéƒ¨ç½²è¿›åº¦
        if hasattr(self.env, '_get_total_vnf_progress'):
            vnf_progress = self.env._get_total_vnf_progress()
            vnf_list = self.env.current_request.get('vnf', [])

            # VNFæœªå®Œæˆä¸”æ²¡æœ‰éƒ¨ç½²ç›®æ ‡ï¼Œéœ€è¦é«˜å±‚å†³ç­–
            if vnf_progress < len(vnf_list) and current_deployment is None:
                return True

            # VNFå·²å®Œæˆä¸”æ²¡æœ‰è¿æ¥ç›®æ ‡ï¼Œéœ€è¦é«˜å±‚å†³ç­–
            elif vnf_progress >= len(vnf_list) and current_target is None:
                return True

        # é»˜è®¤æƒ…å†µ
        return False

    def _get_synchronized_high_state(self):
        """è·å–åŒæ­¥åçš„é«˜å±‚çŠ¶æ€"""
        try:
            return self.env.get_high_level_state_graph()
        except Exception as e:
            logger.error(f"è·å–é«˜å±‚çŠ¶æ€å¤±è´¥: {e}")
            # è¿”å›ä¸€ä¸ªé»˜è®¤çŠ¶æ€
            return self._create_default_state()

    def _calculate_high_reward(self, low_total_reward, low_info, low_steps):
        """
        è®¡ç®—é«˜å±‚å¥–åŠ±

        ç­–ç•¥ï¼š
        1. åŸºç¡€å¥–åŠ± = ä½å±‚ç´¯è®¡å¥–åŠ±
        2. æ ¹æ®ä»»åŠ¡å®Œæˆæƒ…å†µç»™äºˆé¢å¤–å¥–åŠ±/æƒ©ç½š
        3. è€ƒè™‘æ•ˆç‡å› ç´ ï¼ˆæ­¥æ•°è¶Šå°‘è¶Šå¥½ï¼‰
        """
        base_reward = low_total_reward

        # ä»»åŠ¡æˆåŠŸå¥–åŠ±
        if low_info.get('deploy_success', False):
            base_reward += 30.0  # VNFéƒ¨ç½²æˆåŠŸé¢å¤–å¥–åŠ±
        elif low_info.get('connection_success', False):
            base_reward += 50.0  # ç›®çš„åœ°è¿æ¥æˆåŠŸé¢å¤–å¥–åŠ±

        # ä»»åŠ¡å¤±è´¥æƒ©ç½š
        if low_info.get('deploy_fail', False):
            base_reward -= 40.0  # VNFéƒ¨ç½²å¤±è´¥æƒ©ç½š
        elif low_info.get('connection_fail', False):
            base_reward -= 50.0  # ç›®çš„åœ°è¿æ¥å¤±è´¥æƒ©ç½š
        elif low_info.get('path_fail', False):
            base_reward -= 60.0  # è·¯å¾„å»ºç«‹å¤±è´¥æƒ©ç½š

        # æ•ˆç‡å¥–åŠ±ï¼šæ­¥æ•°è¶Šå°‘ï¼Œå¥–åŠ±è¶Šé«˜
        efficiency_factor = max(0, 1.0 - (low_steps / 50.0))  # 50æ­¥ä¸ºåŸºå‡†
        base_reward += 10.0 * efficiency_factor

        return base_reward

    def _create_default_state(self):
        """åˆ›å»ºé»˜è®¤çŠ¶æ€ï¼ˆå®¹é”™å¤„ç†ï¼‰"""
        import torch
        from torch_geometric.data import Data

        n = getattr(self.env, 'n', 14)

        return Data(
            x=torch.zeros((n, 13), dtype=torch.float32),
            edge_index=torch.zeros((2, 0), dtype=torch.long),
            edge_attr=torch.zeros((0, 2), dtype=torch.float32),
            global_attr=torch.zeros((1, 5), dtype=torch.float32)
        )

    def run_full_episode(self, training_mode=True):
        """
        è¿è¡Œå®Œæ•´çš„Episode

        Returns:
            dict: Episodeç»“æœç»Ÿè®¡
        """
        logger.info("=" * 60)
        logger.info("ğŸ¬ å¼€å§‹è¿è¡Œå®Œæ•´Episode")
        logger.info("=" * 60)

        # é‡ç½®ç¯å¢ƒ
        initial_state = self.env.reset()
        self.reset()

        episode_done = False
        total_high_steps = 0
        total_low_steps = 0

        # Episodeä¸»å¾ªç¯
        while not episode_done and total_high_steps < self.max_high_steps:
            # æ‰§è¡Œé«˜å±‚-ä½å±‚å¾ªç¯
            result = self.run_high_low_cycle(training_mode)

            high_state, high_action, high_reward, next_high_state, high_done, info = result

            # æ›´æ–°ç»Ÿè®¡
            total_high_steps += 1
            total_low_steps += info.get('low_steps', 0)

            # æ£€æŸ¥æ˜¯å¦å®Œæˆ
            if high_done:
                episode_done = True
                self.episode_stats['success'] = True
                logger.info("ğŸ‰ Episodeå®Œæˆï¼šæ‰€æœ‰ä»»åŠ¡æˆåŠŸå®Œæˆ")
                break

            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§æ­¥æ•°
            if total_high_steps >= self.max_high_steps:
                logger.warning(f"âš ï¸ Episodeè¾¾åˆ°æœ€å¤§é«˜å±‚æ­¥æ•°é™åˆ¶ ({self.max_high_steps})")
                episode_done = True
                break

        # æœ€ç»ˆç»Ÿè®¡
        self.episode_stats['high_steps'] = total_high_steps
        self.episode_stats['low_steps'] = total_low_steps

        # æ‰“å°æ€»ç»“
        self._print_episode_summary()

        return self.episode_stats.copy()

    def _print_episode_summary(self):
        """æ‰“å°Episodeæ€»ç»“"""
        stats = self.episode_stats

        logger.info("=" * 60)
        logger.info("ğŸ“Š Episodeæ€»ç»“æŠ¥å‘Š")
        logger.info("=" * 60)
        logger.info(f"   æ€»å¥–åŠ±: {stats['total_reward']:.2f}")
        logger.info(f"   é«˜å±‚å†³ç­–æ­¥æ•°: {stats['high_steps']}")
        logger.info(f"   ä½å±‚æ‰§è¡Œæ­¥æ•°: {stats['low_steps']}")
        logger.info(f"   VNFéƒ¨ç½²æ¬¡æ•°: {stats['vnf_deployments']}")
        logger.info(f"   ç›®çš„åœ°è¿æ¥æ•°: {stats['dest_connections']}")
        logger.info(f"   å¤±è´¥æ¬¡æ•°: {stats['failures']}")
        logger.info(f"   æ˜¯å¦æˆåŠŸ: {'âœ…' if stats['success'] else 'âŒ'}")
        logger.info("=" * 60)

    def get_training_data(self):
        """
        è·å–è®­ç»ƒæ•°æ®

        Returns:
            dict: åŒ…å«é«˜å±‚å’Œä½å±‚è®­ç»ƒæ•°æ®
        """
        return {
            'high_agent': {
                'states': self.history.get('high_states', []),
                'actions': self.history['high_actions'],
                'rewards': self.history['rewards'],
                'next_states': self.history.get('next_high_states', [])
            },
            'low_agent': {
                'states': self.history.get('low_states', []),
                'actions': self.history['low_actions'],
                'rewards': self.history.get('low_rewards', [])
            },
            'stats': dict(self.stats),
            'episode_stats': self.episode_stats
        }

    def update_agents(self, high_data=None, low_data=None):
        """
        æ›´æ–°Agent

        Args:
            high_data: é«˜å±‚è®­ç»ƒæ•°æ®
            low_data: ä½å±‚è®­ç»ƒæ•°æ®
        """
        if high_data and hasattr(self.high_agent, 'update'):
            try:
                self.high_agent.update(high_data)
                logger.debug("é«˜å±‚Agentå·²æ›´æ–°")
            except Exception as e:
                logger.error(f"é«˜å±‚Agentæ›´æ–°å¤±è´¥: {e}")

        if low_data and hasattr(self.low_agent, 'update'):
            try:
                self.low_agent.update(low_data)
                logger.debug("ä½å±‚Agentå·²æ›´æ–°")
            except Exception as e:
                logger.error(f"ä½å±‚Agentæ›´æ–°å¤±è´¥: {e}")

    # ============================================================
    # ğŸ”¥ [æ–°å¢] æ ¸å¿ƒæ‰§è¡Œæ–¹æ³•
    # ============================================================

    def step(self):
        """
        ğŸ¯ æ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„ HRL å†³ç­–å¾ªç¯

        æ ¸å¿ƒé€»è¾‘:
        1. æ£€æŸ¥æ˜¯å¦éœ€è¦é«˜å±‚å†³ç­–ï¼ˆé‡æ–°è§„åˆ’å­ç›®æ ‡ï¼‰
        2. å¦‚æœéœ€è¦ï¼Œæ‰§è¡Œé«˜å±‚å†³ç­–
        3. æ‰§è¡Œä½å±‚åŠ¨ä½œ
        4. è¿”å›æ ‡å‡† Gym æ¥å£

        Returns:
            (next_obs, reward, done, truncated, info)
        """
        # ========================================
        # Phase 1: é«˜å±‚å†³ç­–ï¼ˆæ¡ä»¶è§¦å‘ï¼‰
        # ========================================
        if self.current_goal is None or self._should_replan():
            logger.info("ğŸ¯ [Coordinator] è§¦å‘é«˜å±‚å†³ç­–")

            # è·å–é«˜å±‚çŠ¶æ€
            high_obs = self.env.get_high_level_state_graph()

            # é«˜å±‚é€‰æ‹©å­ç›®æ ‡
            high_action, _, high_info = self.high_agent.select_action(high_obs)

            logger.info(f"ğŸ¯ [é«˜å±‚] é€‰æ‹©ç›®æ ‡: {high_action}")

            # è®¾ç½®ç›®æ ‡
            self._set_goal(high_action)

            # æ‰§è¡Œé«˜å±‚ stepï¼ˆè®¾ç½®ç¯å¢ƒçš„ç›®æ ‡çŠ¶æ€ï¼‰
            _, high_reward, done, truncated, info = self.env.step_high_level(high_action)

            # å¦‚æœé«˜å±‚å†³ç­–åå°±ç»“æŸäº†ï¼ˆä¾‹å¦‚æ‰€æœ‰ç›®æ ‡å·²è¿æ¥ï¼‰ï¼Œç›´æ¥è¿”å›
            if done:
                return self.env.get_state(), high_reward, done, truncated, info

        # ========================================
        # Phase 2: ä½å±‚æ‰§è¡Œï¼ˆå‘ç›®æ ‡ç§»åŠ¨/éƒ¨ç½²ï¼‰
        # ========================================
        low_obs = self.env.get_state()
        low_mask = self.env.get_low_level_action_mask()

        # ä½å±‚é€‰æ‹©åŠ¨ä½œ
        _, low_action, low_info = self.low_agent.select_action(
            low_obs,
            action_mask=low_mask
        )

        logger.debug(f"ğŸš¶ [ä½å±‚] æ‰§è¡ŒåŠ¨ä½œ: {low_action}")

        # æ‰§è¡Œä½å±‚åŠ¨ä½œ
        next_obs, reward, done, truncated, info = self.env.step_low_level(low_action)

        # ========================================
        # Phase 3: çŠ¶æ€æ›´æ–°
        # ========================================
        # å¦‚æœå­ç›®æ ‡å®Œæˆï¼ˆtruncated=Trueï¼‰ï¼Œæ¸…ç©ºç›®æ ‡ä»¥ä¾¿ä¸‹æ¬¡é‡æ–°è§„åˆ’
        if truncated:
            logger.info("âœ… [Coordinator] å­ç›®æ ‡å®Œæˆï¼Œæ¸…ç©ºç›®æ ‡")
            self.current_goal = None

        # å­˜å‚¨æœ€è¿‘çš„ transitionï¼ˆä¾› Trainer ä½¿ç”¨ï¼‰
        self.last_transition = (low_obs, low_action, reward, next_obs, done)

        return next_obs, reward, done, truncated, info

    def _should_replan(self):
        """
        åˆ¤æ–­æ˜¯å¦éœ€è¦é‡æ–°è§„åˆ’é«˜å±‚ç›®æ ‡

        è§¦å‘æ¡ä»¶:
        1. æ­¥æ•°è¾¾åˆ°é˜ˆå€¼ï¼ˆé˜²æ­¢å¡æ­»ï¼‰
        2. å½“å‰é˜¶æ®µæ”¹å˜ï¼ˆVNFéƒ¨ç½² -> ç›®çš„åœ°è¿æ¥ï¼‰
        """
        if not hasattr(self, '_low_step_count'):
            self._low_step_count = 0

        self._low_step_count += 1

        # ç­–ç•¥1: æ¯ 10 æ­¥å¼ºåˆ¶é‡æ–°è§„åˆ’
        if self._low_step_count >= 10:
            logger.debug(f"â° [Coordinator] æ­¥æ•°è¾¾åˆ° {self._low_step_count}ï¼Œè§¦å‘é‡æ–°è§„åˆ’")
            return True

        # ç­–ç•¥2: é˜¶æ®µæ”¹å˜æ—¶é‡æ–°è§„åˆ’
        current_phase = getattr(self.env, 'current_phase', None)
        if hasattr(self, '_last_phase') and current_phase != self._last_phase:
            logger.debug(f"ğŸ”„ [Coordinator] é˜¶æ®µæ”¹å˜ {self._last_phase} -> {current_phase}")
            self._last_phase = current_phase
            return True

        self._last_phase = current_phase

        return False

    def _set_goal(self, high_action):
        """è®¾ç½®å½“å‰é«˜å±‚ç›®æ ‡"""
        self.current_goal = high_action
        self._low_step_count = 0
        logger.debug(f"ğŸ“Œ [Coordinator] è®¾ç½®ç›®æ ‡: {high_action}")