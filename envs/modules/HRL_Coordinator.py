import numpy as np
import torch
from collections import defaultdict, deque
import time
import logging

logger = logging.getLogger(__name__)


class HRL_Coordinator:
    """
    ğŸ® HRLæ—¶åºåè°ƒå™¨ - è§£å†³æ—¶é—´å°ºåº¦ä¸åŒ¹é…é—®é¢˜

    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. ç®¡ç†é«˜å±‚å’Œä½å±‚çš„æ‰§è¡Œæ—¶åº
    2. ç¡®ä¿ State(t) â†’ High Action â†’ Low Execution â†’ State(t+1) é—­ç¯
    3. åè°ƒé«˜å±‚å’Œä½å±‚çš„çŠ¶æ€åŒæ­¥
    4. ç»Ÿè®¡å’Œç›‘æ§è®­ç»ƒè¿‡ç¨‹
    """

    def __init__(self, env, high_agent, low_agent, config=None):
        """
        åˆå§‹åŒ–åè°ƒå™¨

        Args:
            env: SFC_HIRL_Env ç¯å¢ƒå®ä¾‹
            high_agent: é«˜å±‚Agent
            low_agent: ä½å±‚Agent
            config: é…ç½®å­—å…¸
        """
        self.env = env
        self.high_agent = high_agent
        self.low_agent = low_agent

        # é…ç½®å‚æ•°
        self.config = config or {}
        self.max_low_steps = config.get('max_low_steps', 100)
        self.max_high_steps = config.get('max_high_steps', 100)
        self.use_masking = config.get('use_masking', True)

        # çŠ¶æ€åŒæ­¥ç¼“å­˜
        self.current_high_state = None
        self.current_low_state = None
        self.last_high_action = None
        self.last_high_reward = 0.0

        # æ‰§è¡Œç»Ÿè®¡
        self.stats = defaultdict(int)
        self.episode_stats = {
            'total_reward': 0.0,
            'high_steps': 0,
            'low_steps': 0,
            'vnf_deployments': 0,
            'dest_connections': 0,
            'failures': 0,
            'success': False
        }

        # å†å²è®°å½•ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        self.history = {
            'high_actions': [],
            'low_actions': [],
            'rewards': [],
            'states': []
        }

        self.current_goal = None  # å½“å‰é«˜å±‚ç›®æ ‡
        self.current_subgoal = None  # å½“å‰å­ç›®æ ‡
        self._low_step_count = 0  # ä½å±‚æ­¥æ•°è®¡æ•°å™¨
        self._last_phase = None  # ä¸Šä¸€æ¬¡çš„é˜¶æ®µ
        self.last_transition = None  # æœ€è¿‘çš„ transitionï¼ˆä¾› Trainer ä½¿ç”¨ï¼‰
        self.last_high_action = None  # æœ€è¿‘çš„é«˜å±‚åŠ¨ä½œ

        self.last_vnf_progress = 0
        self.last_connected_count = 0

        logger.info("âœ… HRLæ—¶åºåè°ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"é…ç½®: max_low_steps={self.max_low_steps}, max_high_steps={self.max_high_steps}")

    def reset(self):
        """é‡ç½®åè°ƒå™¨çŠ¶æ€"""
        self.current_high_state = None
        self.current_low_state = None
        self.last_high_action = None
        self.last_high_reward = 0.0

        # é‡ç½®episodeç»Ÿè®¡
        self.episode_stats = {
            'total_reward': 0.0,
            'high_steps': 0,
            'low_steps': 0,
            'vnf_deployments': 0,
            'dest_connections': 0,
            'failures': 0,
            'success': False
        }

        # æ¸…ç©ºå†å²è®°å½•
        self.history = {
            'high_actions': [],
            'low_actions': [],
            'rewards': [],
            'states': []
        }

        logger.debug("åè°ƒå™¨çŠ¶æ€å·²é‡ç½®")

    def run_high_low_cycle(self, training_mode=True):
        """
        æ‰§è¡Œä¸€ä¸ªå®Œæ•´çš„é«˜å±‚-ä½å±‚å¾ªç¯

        ä¿®å¤ï¼šæ­£ç¡®å¤„ç†é«˜å±‚è¿”å›çš„ truncated ä¿¡å·ï¼Œç¡®ä¿é«˜å±‚-ä½å±‚ä¸¥æ ¼åˆ‡æ¢

        Returns:
            high_state: é«˜å±‚å½“å‰çŠ¶æ€
            high_action: é«˜å±‚åŠ¨ä½œ
            high_reward: é«˜å±‚å¥–åŠ±
            next_high_state: é«˜å±‚ä¸‹ä¸€ä¸ªçŠ¶æ€ï¼ˆåŒæ­¥åï¼‰
            high_done: é«˜å±‚æ˜¯å¦å®Œæˆ
            info: é™„åŠ ä¿¡æ¯
        """
        # ========================================
        # 1. æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›è¡Œé«˜å±‚å†³ç­–
        # ========================================
        need_high_decision = self._check_need_high_decision()

        if need_high_decision:
            # ========================================
            # 2. é«˜å±‚å†³ç­–é˜¶æ®µ
            # ========================================
            high_state = self._get_synchronized_high_state()

            # è·å–é«˜å±‚åŠ¨ä½œæ©ç 
            high_mask = None
            if self.use_masking:
                try:
                    high_mask = self.env.get_high_level_action_mask()
                except AttributeError:
                    logger.warning("ç¯å¢ƒä¸æ”¯æŒget_high_level_action_maskï¼Œå°†ä½¿ç”¨å…¨æ©ç ")
                    high_mask = np.ones(self.env.high_action_space, dtype=bool)

            # é«˜å±‚Agenté€‰æ‹©åŠ¨ä½œ
            high_action = self.high_agent.select_action(
                high_state,
                mask=high_mask,
                training=training_mode
            )

            self.last_high_action = high_action
            self.history['high_actions'].append(high_action)

            # ========================================
            # 3. æ‰§è¡Œé«˜å±‚åŠ¨ä½œï¼ˆåªè®¾å®šç›®æ ‡ï¼‰
            # ========================================
            start_time = time.time()

            # ğŸ”¥ ä¿®å¤ï¼šæ­£ç¡®è¯»å– truncated ä¿¡å·
            _, _, high_done, high_truncated, high_info = self.env.step_high_level(high_action)

            high_decision_time = time.time() - start_time

            # è®°å½•é«˜å±‚å†³ç­–
            self.stats['high_decisions'] += 1
            self.episode_stats['high_steps'] += 1

            logger.info(
                f"ğŸ” [é«˜å±‚å†³ç­–] æ­¥éª¤{self.stats['high_decisions']}ï¼ŒåŠ¨ä½œ={high_action}ï¼Œç”¨æ—¶={high_decision_time:.3f}sï¼Œtruncated={high_truncated}")

            # ========================================
            # 4. æ£€æŸ¥é«˜å±‚è¿”å›çŠ¶æ€
            # ========================================
            # æƒ…å†µ1ï¼šé«˜å±‚ä»»åŠ¡å®Œæˆï¼ˆæ‰€æœ‰ç›®çš„åœ°å·²è¿æ¥ï¼‰
            if high_done:
                logger.info("âœ… é«˜å±‚ä»»åŠ¡å®Œæˆï¼šæ‰€æœ‰ç›®çš„åœ°å·²è¿æ¥")

                # è·å–æœ€ç»ˆçŠ¶æ€
                final_state = self.env.get_high_level_state_graph()

                return (
                    high_state,
                    high_action,
                    100.0,  # å®Œæˆå¥–åŠ±
                    final_state,
                    True,
                    {
                        'episode_complete': True,
                        'high_done': True,
                        'message': 'æ‰€æœ‰ç›®çš„åœ°è¿æ¥å®Œæˆ'
                    }
                )

            # æƒ…å†µ2ï¼šé«˜å±‚å†³ç­–ç»“æŸï¼Œè¿›å…¥ä½å±‚æ‰§è¡Œé˜¶æ®µ
            elif high_truncated:
                logger.info("ğŸ”„ é«˜å±‚å†³ç­–ç»“æŸï¼Œè¿›å…¥ä½å±‚æ‰§è¡Œé˜¶æ®µ")

                # ========================================
                # 5. ä½å±‚æ‰§è¡Œå¾ªç¯ï¼ˆåŒæ­¥å…³é”®ï¼‰
                # ========================================
                low_execution_result = self._execute_low_level_loop(training_mode)

                # æ›´æ–°ç»Ÿè®¡
                self.episode_stats['low_steps'] += low_execution_result['steps']
                self.episode_stats['total_reward'] += low_execution_result['total_reward']

                # ========================================
                # 6. è·å–åŒæ­¥åçš„ä¸‹ä¸€ä¸ªé«˜å±‚çŠ¶æ€
                # ========================================
                # ğŸ”¥ å…³é”®ï¼šä½å±‚æ‰§è¡Œå®Œåï¼ŒçŠ¶æ€å·²æ›´æ–°ï¼Œç°åœ¨è·å–åŒæ­¥åçš„é«˜å±‚çŠ¶æ€
                next_high_state = self._get_synchronized_high_state()

                # ========================================
                # 7. è®¡ç®—é«˜å±‚å¥–åŠ±
                # ========================================
                high_reward = self._calculate_high_reward(
                    low_execution_result['total_reward'],
                    low_execution_result['info'],
                    low_execution_result['steps']
                )

                self.last_high_reward = high_reward
                self.history['rewards'].append(high_reward)

                # ========================================
                # 8. è¿”å›ç»“æœ
                # ========================================
                return (
                    high_state,
                    high_action,
                    high_reward,
                    next_high_state,
                    False,  # high_done
                    {
                        **high_info,
                        **low_execution_result['info'],
                        'low_steps': low_execution_result['steps'],
                        'low_total_reward': low_execution_result['total_reward'],
                        'high_reward': high_reward,
                        'execution_time': high_decision_time + low_execution_result['execution_time'],
                        'high_truncated': high_truncated,
                        'phase': getattr(self.env, 'current_phase', 'unknown')
                    }
                )

            # æƒ…å†µ3ï¼šé«˜å±‚æ—¢æ²¡å®Œæˆä¹Ÿæ²¡ç»“æŸï¼Œç»§ç»­ç­‰å¾…ï¼ˆç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼‰
            else:
                logger.warning("âš ï¸ é«˜å±‚å†³ç­–æ—¢æœªå®Œæˆä¹Ÿæœªç»“æŸï¼Œå¯èƒ½é€»è¾‘é”™è¯¯")
                return (
                    high_state,
                    high_action,
                    0.0,
                    high_state,
                    False,
                    {
                        **high_info,
                        'message': 'é«˜å±‚å†³ç­–çŠ¶æ€å¼‚å¸¸',
                        'high_done': high_done,
                        'high_truncated': high_truncated
                    }
                )
        else:
            # ä¸éœ€è¦é«˜å±‚å†³ç­–ï¼Œç›´æ¥è¿”å›å½“å‰çŠ¶æ€
            current_state = self._get_synchronized_high_state()
            return (
                current_state,
                None,
                0.0,
                current_state,
                False,
                {
                    'message': 'ç­‰å¾…ä½å±‚æ‰§è¡Œå®Œæˆ',
                    'phase': getattr(self.env, 'current_phase', 'unknown')
                }
            )

    def _execute_low_level_step(self):
        """
        æ‰§è¡Œä½å±‚åŠ¨ä½œ
        ğŸ”¥ [ä¿®å¤] ç§»é™¤ high_action æ±‡æŠ¥ï¼Œé˜²æ­¢ Trainer è¯¯åˆ¤ä¸ºé«˜å±‚æ­»å¾ªç¯
        """
        low_obs = self.env.get_state()
        low_mask = self.env.get_low_level_action_mask()

        # 1. é€‰åŠ¨ä½œ
        try:
            _, low_action, low_info = self.low_agent.select_action(
                low_obs,
                action_mask=low_mask
            )
        except Exception as e:
            logger.error(f"âŒ [Coordinator] ä½å±‚é€‰åŠ¨ä½œå¤±è´¥: {e}")
            return low_obs, -5.0, False, True, {'error': 'low_select_fail'}

        # 2. æ‰§è¡ŒåŠ¨ä½œ
        try:
            next_obs, reward, done, truncated, info = self.env.step_low_level(low_action)
        except Exception as e:
            logger.error(f"âŒ [Coordinator] ä½å±‚æ‰§è¡Œå¤±è´¥: {e}")
            return low_obs, -10.0, False, True, {'error': 'low_step_fail'}

        # 3. çŠ¶æ€æ›´æ–°
        if truncated:
            logger.info("âœ… [Coordinator] å­ç›®æ ‡è¾¾æˆï¼Œå‡†å¤‡é‡è§„åˆ’")
            self.current_goal = None
            self.stats['subgoals_completed'] += 1

        self.last_transition = (low_obs, low_action, reward, next_obs, done)
        self.stats['total_low_actions'] += 1

        if not isinstance(info, dict): info = {}

        # ğŸ”¥ğŸ”¥ğŸ”¥ [å…³é”®ä¿®æ”¹] ä¸è¦åœ¨è¿™é‡Œæ±‡æŠ¥ high_actionï¼ğŸ”¥ğŸ”¥ğŸ”¥
        # info['high_action'] = getattr(self, 'last_high_action', None)  <-- åˆ æ‰æˆ–æ³¨é‡Šæ‰è¿™è¡Œ
        # è®© Trainer çœ‹åˆ° Noneï¼Œå®ƒå°±çŸ¥é“ "å“¦ï¼Œè¿™ä¸€æ­¥ä¸æ˜¯é«˜å±‚å†³ç­–"
        info['high_action'] = None

        return next_obs, reward, done, truncated, info

    def _enhance_low_state(self, low_state):
        """
        å¢å¼ºä½å±‚çŠ¶æ€ä¿¡æ¯ï¼Œå¸®åŠ©Agentç†è§£ä½•æ—¶åº”è¯¥æ‰§è¡Œéƒ¨ç½²/è¿æ¥

        Args:
            low_state: åŸå§‹ä½å±‚çŠ¶æ€

        Returns:
            å¢å¼ºåçš„çŠ¶æ€
        """
        # è·å–å½“å‰ç¯å¢ƒä¿¡æ¯
        current_phase = getattr(self.env, 'current_phase', None)
        current_target = getattr(self.env, 'current_target_node', None)
        current_node = getattr(self.env, 'current_node', None)

        # åˆ›å»ºå¢å¼ºä¿¡æ¯
        enhanced_info = {
            'phase': current_phase,
            'target_node': current_target,
            'current_node': current_node,
            'at_target': current_node == current_target if current_node and current_target else False
        }

        # æ ¹æ®ç¯å¢ƒç±»å‹è¿”å›å¢å¼ºçŠ¶æ€
        if hasattr(low_state, 'enhanced_info'):
            low_state.enhanced_info = enhanced_info
        elif isinstance(low_state, dict):
            low_state['enhanced_info'] = enhanced_info
        elif hasattr(low_state, '__dict__'):
            low_state.enhanced_info = enhanced_info

        return low_state
    def _check_need_high_decision(self):
        """
        æ£€æŸ¥å½“å‰æ˜¯å¦éœ€è¦é«˜å±‚å†³ç­–

        éœ€è¦é«˜å±‚å†³ç­–çš„æ¡ä»¶ï¼š
        1. å½“å‰æ²¡æœ‰æ­£åœ¨æ‰§è¡Œçš„ä½å±‚ä»»åŠ¡
        2. ä¸Šä¸€ä¸ªä½å±‚ä»»åŠ¡å·²å®Œæˆ
        3. å½“å‰é˜¶æ®µéœ€è¦æ–°çš„ç›®æ ‡
        """
        # è·å–å½“å‰ç¯å¢ƒçŠ¶æ€
        current_phase = getattr(self.env, 'current_phase', None)
        current_target = getattr(self.env, 'current_target_node', None)
        current_deployment = getattr(self.env, 'current_deployment_target', None)

        # å¦‚æœæ²¡æœ‰å½“å‰é˜¶æ®µï¼Œéœ€è¦é«˜å±‚å†³ç­–
        if current_phase is None:
            return True

        # æ£€æŸ¥VNFéƒ¨ç½²è¿›åº¦
        if hasattr(self.env, '_get_total_vnf_progress'):
            vnf_progress = self.env._get_total_vnf_progress()
            vnf_list = self.env.current_request.get('vnf', [])

            # VNFæœªå®Œæˆä¸”æ²¡æœ‰éƒ¨ç½²ç›®æ ‡ï¼Œéœ€è¦é«˜å±‚å†³ç­–
            if vnf_progress < len(vnf_list) and current_deployment is None:
                return True

            # VNFå·²å®Œæˆä¸”æ²¡æœ‰è¿æ¥ç›®æ ‡ï¼Œéœ€è¦é«˜å±‚å†³ç­–
            elif vnf_progress >= len(vnf_list) and current_target is None:
                return True

        # é»˜è®¤æƒ…å†µ
        return False

    def _get_synchronized_high_state(self):
        """è·å–åŒæ­¥åçš„é«˜å±‚çŠ¶æ€"""
        try:
            return self.env.get_high_level_state_graph()
        except Exception as e:
            logger.error(f"è·å–é«˜å±‚çŠ¶æ€å¤±è´¥: {e}")
            # è¿”å›ä¸€ä¸ªé»˜è®¤çŠ¶æ€
            return self._create_default_state()

    def _calculate_high_reward(self, low_total_reward, low_info, low_steps):
        """
        è®¡ç®—é«˜å±‚å¥–åŠ±

        ç­–ç•¥ï¼š
        1. åŸºç¡€å¥–åŠ± = ä½å±‚ç´¯è®¡å¥–åŠ±
        2. æ ¹æ®ä»»åŠ¡å®Œæˆæƒ…å†µç»™äºˆé¢å¤–å¥–åŠ±/æƒ©ç½š
        3. è€ƒè™‘æ•ˆç‡å› ç´ ï¼ˆæ­¥æ•°è¶Šå°‘è¶Šå¥½ï¼‰
        """
        base_reward = low_total_reward

        # ä»»åŠ¡æˆåŠŸå¥–åŠ±
        if low_info.get('deploy_success', False):
            base_reward += 30.0  # VNFéƒ¨ç½²æˆåŠŸé¢å¤–å¥–åŠ±
        elif low_info.get('connection_success', False):
            base_reward += 50.0  # ç›®çš„åœ°è¿æ¥æˆåŠŸé¢å¤–å¥–åŠ±

        # ä»»åŠ¡å¤±è´¥æƒ©ç½š
        if low_info.get('deploy_fail', False):
            base_reward -= 40.0  # VNFéƒ¨ç½²å¤±è´¥æƒ©ç½š
        elif low_info.get('connection_fail', False):
            base_reward -= 50.0  # ç›®çš„åœ°è¿æ¥å¤±è´¥æƒ©ç½š
        elif low_info.get('path_fail', False):
            base_reward -= 60.0  # è·¯å¾„å»ºç«‹å¤±è´¥æƒ©ç½š

        # æ•ˆç‡å¥–åŠ±ï¼šæ­¥æ•°è¶Šå°‘ï¼Œå¥–åŠ±è¶Šé«˜
        efficiency_factor = max(0, 1.0 - (low_steps / 50.0))  # 50æ­¥ä¸ºåŸºå‡†
        base_reward += 10.0 * efficiency_factor

        return base_reward

    def _create_default_state(self):
        """åˆ›å»ºé»˜è®¤çŠ¶æ€ï¼ˆå®¹é”™å¤„ç†ï¼‰"""
        import torch
        from torch_geometric.data import Data

        n = getattr(self.env, 'n', 14)

        return Data(
            x=torch.zeros((n, 13), dtype=torch.float32),
            edge_index=torch.zeros((2, 0), dtype=torch.long),
            edge_attr=torch.zeros((0, 2), dtype=torch.float32),
            global_attr=torch.zeros((1, 5), dtype=torch.float32)
        )

    def run_full_episode(self, training_mode=True):
        """
        è¿è¡Œå®Œæ•´çš„Episode

        Returns:
            dict: Episodeç»“æœç»Ÿè®¡
        """
        logger.info("=" * 60)
        logger.info("ğŸ¬ å¼€å§‹è¿è¡Œå®Œæ•´Episode")
        logger.info("=" * 60)

        # é‡ç½®ç¯å¢ƒ
        initial_state = self.env.reset()
        self.reset()

        episode_done = False
        total_high_steps = 0
        total_low_steps = 0

        # Episodeä¸»å¾ªç¯
        while not episode_done and total_high_steps < self.max_high_steps:
            # æ‰§è¡Œé«˜å±‚-ä½å±‚å¾ªç¯
            result = self.run_high_low_cycle(training_mode)

            high_state, high_action, high_reward, next_high_state, high_done, info = result

            # æ›´æ–°ç»Ÿè®¡
            total_high_steps += 1
            total_low_steps += info.get('low_steps', 0)

            # æ£€æŸ¥æ˜¯å¦å®Œæˆ
            if high_done:
                episode_done = True
                self.episode_stats['success'] = True
                logger.info("ğŸ‰ Episodeå®Œæˆï¼šæ‰€æœ‰ä»»åŠ¡æˆåŠŸå®Œæˆ")
                break

            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§æ­¥æ•°
            if total_high_steps >= self.max_high_steps:
                logger.warning(f"âš ï¸ Episodeè¾¾åˆ°æœ€å¤§é«˜å±‚æ­¥æ•°é™åˆ¶ ({self.max_high_steps})")
                episode_done = True
                break

        # æœ€ç»ˆç»Ÿè®¡
        self.episode_stats['high_steps'] = total_high_steps
        self.episode_stats['low_steps'] = total_low_steps

        # æ‰“å°æ€»ç»“
        self._print_episode_summary()

        return self.episode_stats.copy()

    def _print_episode_summary(self):
        """æ‰“å°Episodeæ€»ç»“"""
        stats = self.episode_stats

        logger.info("=" * 60)
        logger.info("ğŸ“Š Episodeæ€»ç»“æŠ¥å‘Š")
        logger.info("=" * 60)
        logger.info(f"   æ€»å¥–åŠ±: {stats['total_reward']:.2f}")
        logger.info(f"   é«˜å±‚å†³ç­–æ­¥æ•°: {stats['high_steps']}")
        logger.info(f"   ä½å±‚æ‰§è¡Œæ­¥æ•°: {stats['low_steps']}")
        logger.info(f"   VNFéƒ¨ç½²æ¬¡æ•°: {stats['vnf_deployments']}")
        logger.info(f"   ç›®çš„åœ°è¿æ¥æ•°: {stats['dest_connections']}")
        logger.info(f"   å¤±è´¥æ¬¡æ•°: {stats['failures']}")
        logger.info(f"   æ˜¯å¦æˆåŠŸ: {'âœ…' if stats['success'] else 'âŒ'}")
        logger.info("=" * 60)

    def get_training_data(self):
        """
        è·å–è®­ç»ƒæ•°æ®

        Returns:
            dict: åŒ…å«é«˜å±‚å’Œä½å±‚è®­ç»ƒæ•°æ®
        """
        return {
            'high_agent': {
                'states': self.history.get('high_states', []),
                'actions': self.history['high_actions'],
                'rewards': self.history['rewards'],
                'next_states': self.history.get('next_high_states', [])
            },
            'low_agent': {
                'states': self.history.get('low_states', []),
                'actions': self.history['low_actions'],
                'rewards': self.history.get('low_rewards', [])
            },
            'stats': dict(self.stats),
            'episode_stats': self.episode_stats
        }

    def update_agents(self, high_data=None, low_data=None):
        """
        æ›´æ–°Agent

        Args:
            high_data: é«˜å±‚è®­ç»ƒæ•°æ®
            low_data: ä½å±‚è®­ç»ƒæ•°æ®
        """
        if high_data and hasattr(self.high_agent, 'update'):
            try:
                self.high_agent.update(high_data)
                logger.debug("é«˜å±‚Agentå·²æ›´æ–°")
            except Exception as e:
                logger.error(f"é«˜å±‚Agentæ›´æ–°å¤±è´¥: {e}")

        if low_data and hasattr(self.low_agent, 'update'):
            try:
                self.low_agent.update(low_data)
                logger.debug("ä½å±‚Agentå·²æ›´æ–°")
            except Exception as e:
                logger.error(f"ä½å±‚Agentæ›´æ–°å¤±è´¥: {e}")

    # ============================================================
    # ğŸ”¥ [æ–°å¢] æ ¸å¿ƒæ‰§è¡Œæ–¹æ³•
    # ============================================================
    def step(self, force_goal=None):
        """
        ğŸ¯ [æ­»å¾ªç¯ç»ˆæä¿®å¤ç‰ˆ] æ‰§è¡Œ HRL å†³ç­–æ­¥

        Args:
            force_goal: å¦‚æœé Noneï¼Œè¡¨ç¤ºåˆšä»é«˜å±‚åˆ‡æ¢ä¸‹æ¥ï¼Œå¼ºåˆ¶æ‰§è¡Œä½å±‚é€»è¾‘ï¼Œè·³è¿‡æ£€æŸ¥ã€‚
        """

        # ============================================================
        # Phase 1: é«˜å±‚å†³ç­– (High-Level Decision)
        # ============================================================
        # åªæœ‰åœ¨æ²¡æœ‰å¼ºåˆ¶ç›®æ ‡ï¼Œä¸” (å½“å‰æ— ç›®æ ‡ æˆ– éœ€è¦é‡è§„åˆ’) æ—¶æ‰è¿›å…¥
        if force_goal is None and (self.current_goal is None or self._should_replan()):
            logger.info("ğŸ¯ [Coordinator] è§¦å‘é«˜å±‚å†³ç­–")
            self.low_step_count = 0
            # ğŸ”¥ æ£€æŸ¥placementçŠ¶æ€
            if hasattr(self.env, 'current_tree') and 'placement' in self.env.current_tree:
                placement_count = len(self.env.current_tree['placement'])
                logger.warning(f"ğŸ” [è°ƒè¯•] å½“å‰placementæ•°é‡: {placement_count}")
                if placement_count > 0:
                    logger.warning(f"   å‰3ä¸ªplacement: {list(self.env.current_tree['placement'].items())[:3]}")

            # 1. å‡†å¤‡çŠ¶æ€
            high_obs = self.env.get_high_level_state_graph()
            high_mask = None
            unconnected_dests = []
            try:
                if hasattr(self.env, 'get_high_level_action_mask'):
                    high_mask = self.env.get_high_level_action_mask()
                if self.env.current_request:
                    dests = self.env.current_request.get('dest', [])
                    connected = self.env.current_tree.get('connected_dests', set())
                    unconnected_dests = [d for d in dests if d not in connected]
            except Exception:
                pass

            # 2. Agent é€‰æ‹©
            high_action, _, high_info = self.high_agent.select_action(
                high_obs,
                action_mask=high_mask,
                unconnected_dests=unconnected_dests
            )
            logger.warning(
                f"ğŸ” [è°ƒè¯•] high_maskç±»å‹: {type(high_mask)}, å½¢çŠ¶: {high_mask.shape if hasattr(high_mask, 'shape') else len(high_mask)}")
            logger.warning(
                f"ğŸ” [è°ƒè¯•] high_mask[24] = {high_mask[24] if high_mask is not None and len(high_mask) > 24 else 'N/A'}")
            logger.warning(f"ğŸ” [è°ƒè¯•] Agenté€‰æ‹©: {high_action}")
            logger.warning(f"ğŸ” [è°ƒè¯•] DCèŠ‚ç‚¹åˆ—è¡¨: {getattr(self.env, 'dc_nodes', 'N/A')}")

            # ğŸ”¥ å¼ºåˆ¶éªŒè¯å¹¶ä¿®æ­£
            # ğŸ”¥ å¼ºåˆ¶éªŒè¯å¹¶ä¿®æ­£
            if high_mask is not None and high_action < len(high_mask):
                # ğŸ”¥ å…³é”®ï¼šåªåœ¨VNFéƒ¨ç½²é˜¶æ®µä¸”æ²¡æœ‰subgoalæ—¶æ‰éªŒè¯mask
                phase = getattr(self.env, 'current_phase', 'unknown')
                real_target = high_info.get('subgoal')  # ä¼˜å…ˆç”¨subgoal

                # å¦‚æœæ²¡æœ‰subgoalï¼Œæ‰æ£€æŸ¥high_action
                if real_target is None:
                    if high_mask[high_action] == 0 or high_mask[high_action] == False:
                        logger.error(f"âŒ Agenté€‰æ‹©äº†è¢«å±è”½çš„èŠ‚ç‚¹{high_action}ï¼å¼ºåˆ¶ä¿®æ­£...")

                        # æ‰¾åˆ°æ‰€æœ‰å¯ç”¨èŠ‚ç‚¹
                        valid_nodes = np.where(high_mask > 0)[0] if hasattr(high_mask, '__len__') else []

                        if len(valid_nodes) > 0:
                            # éšæœºé€‰ä¸€ä¸ªåˆæ³•èŠ‚ç‚¹
                            high_action = int(np.random.choice(valid_nodes))
                            logger.warning(f"âœ… ä¿®æ­£ä¸º: èŠ‚ç‚¹{high_action}")
                        else:
                            logger.error(f"âŒ æ²¡æœ‰å¯ç”¨èŠ‚ç‚¹ï¼Maskå…¨ä¸º0")
                else:
                    # æœ‰subgoalï¼Œç›´æ¥ä½¿ç”¨å®ƒï¼Œä¸éªŒè¯high_action
                    logger.debug(f"âœ… ä½¿ç”¨info['subgoal']={real_target}ï¼Œè·³è¿‡high_actionéªŒè¯")
                if high_mask[high_action] == 0 or high_mask[high_action] == False:
                    logger.error(f"âŒ Agenté€‰æ‹©äº†è¢«å±è”½çš„èŠ‚ç‚¹{high_action}ï¼å¼ºåˆ¶ä¿®æ­£...")

                    # æ‰¾åˆ°æ‰€æœ‰å¯ç”¨èŠ‚ç‚¹
                    valid_nodes = np.where(high_mask > 0)[0] if hasattr(high_mask, '__len__') else []

                    if len(valid_nodes) > 0:
                        # éšæœºé€‰ä¸€ä¸ªåˆæ³•èŠ‚ç‚¹
                        high_action = int(np.random.choice(valid_nodes))
                        logger.warning(f"âœ… ä¿®æ­£ä¸º: èŠ‚ç‚¹{high_action}")
                    else:
                        logger.error(f"âŒ æ²¡æœ‰å¯ç”¨èŠ‚ç‚¹ï¼Maskå…¨ä¸º0")
            # ğŸ”¥ğŸ”¥ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ­£ç¡®è§£æç›®æ ‡èŠ‚ç‚¹ ğŸ”¥ğŸ”¥ğŸ”¥
            # 3. ç›®æ ‡è§£æï¼ˆä¼˜å…ˆçº§ï¼šsubgoal > unconnected_destsæ˜ å°„ > ç›´æ¥ä½¿ç”¨ï¼‰
            # ğŸ” [è°ƒè¯•] åœ¨è§£æreal_target_nodeä¹‹å‰
            logger.warning(f"ğŸ” [è°ƒè¯•] high_infoå†…å®¹: {high_info}")
            logger.warning(f"ğŸ” [è°ƒè¯•] high_info.get('subgoal'): {high_info.get('subgoal')}")
            logger.warning(f"ğŸ” [è°ƒè¯•] unconnected_dests: {unconnected_dests}")
            logger.warning(f"ğŸ” [è°ƒè¯•] current_phase: {getattr(self.env, 'current_phase', 'unknown')}")
            logger.warning(f"ğŸ” [è°ƒè¯•] high_action: {high_action} (type: {type(high_action)})")
            real_target_node = high_info.get('subgoal')  # ä¼˜å…ˆä»infoè·å–

            if real_target_node is None:
                # å¦‚æœåœ¨ç›®çš„åœ°è¿æ¥é˜¶æ®µï¼Œæ˜ å°„ç´¢å¼•åˆ°ç›®çš„åœ°
                if unconnected_dests and 0 <= high_action < len(unconnected_dests):
                    real_target_node = unconnected_dests[high_action]
                    logger.info(f"ğŸ¯ [é«˜å±‚] ç›®çš„åœ°è¿æ¥é˜¶æ®µ: ç´¢å¼•{high_action} -> ç›®çš„åœ°{real_target_node}")
                else:
                    # VNFéƒ¨ç½²é˜¶æ®µï¼Œhigh_actionå°±æ˜¯èŠ‚ç‚¹ID
                    real_target_node = high_action
                    logger.info(f"ğŸ¯ [é«˜å±‚] VNFéƒ¨ç½²é˜¶æ®µ: é€‰æ‹©èŠ‚ç‚¹{real_target_node}")

            logger.info(f"ğŸ¯ [é«˜å±‚] æœ€ç»ˆç›®æ ‡: {real_target_node}")

            # ğŸ”¥ è®°å½•å®é™…èŠ‚ç‚¹ï¼ˆä¸æ˜¯ç´¢å¼•ï¼‰
            self.last_high_action = real_target_node

            # ğŸ”¥ è®¾ç½®ç›®æ ‡ï¼ˆä¼ å…¥å®é™…èŠ‚ç‚¹ï¼‰
            self._set_goal(real_target_node)

            # 4. æ‰§è¡Œé«˜å±‚ Stepï¼ˆä¼ å…¥å®é™…èŠ‚ç‚¹ï¼‰
            try:
                _, high_reward, high_done, high_truncated, step_info = self.env.step_high_level(real_target_node)

                # åŒæ­¥ Phase é˜²æ­¢è¯¯åˆ¤
                self._last_phase = getattr(self.env, 'current_phase', None)

                # æ›´æ–°info
                if isinstance(step_info, dict):
                    step_info.update(high_info)
                    step_info['high_action'] = real_target_node  # ğŸ”¥ è®°å½•å®é™…èŠ‚ç‚¹

                # å¦‚æœä»»åŠ¡å®Œæˆï¼Œç›´æ¥è¿”å›
                if high_done:
                    logger.info("âœ… [Coordinator] é«˜å±‚ä»»åŠ¡å…¨éƒ¨å®Œæˆ")
                    self.current_goal = None
                    return self.env.get_state(), high_reward, high_done, False, step_info

                # ğŸ”¥ [å…³é”®ä¿®å¤] é«˜å±‚è®¾å®šå®Œæˆï¼Œå‡†å¤‡ä½å±‚æ‰§è¡Œ
                logger.info("â†˜ï¸ [Coordinator] é«˜å±‚è®¾å®šå®Œæˆï¼Œå¼ºåˆ¶è¿›å…¥ä½å±‚")
                self.current_goal = real_target_node
                self.stats['total_high_decisions'] += 1

                # ğŸ”¥ğŸ”¥ğŸ”¥ ä¸è¦é€’å½’ï¼ç›´æ¥ç»§ç»­åˆ°Phase 2 ğŸ”¥ğŸ”¥ğŸ”¥
                # è®©ä»£ç è‡ªç„¶æµåˆ°ä¸‹é¢çš„ä½å±‚æ‰§è¡Œéƒ¨åˆ†

            except Exception as e:
                logger.error(f"âŒ [Coordinator] é«˜å±‚æ‰§è¡Œå´©æºƒ: {e}")
                import traceback
                traceback.print_exc()
                self.current_goal = None
                return self.env.get_state(), -10.0, False, True, {
                    'error': 'high_crash',
                    'high_action': real_target_node
                }

        # ============================================================
        # Phase 2: ä½å±‚æ‰§è¡Œ (Low-Level Execution)
        # ============================================================
        # å¦‚æœæœ‰force_goalï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™ä½¿ç”¨current_goal
        target_goal = force_goal if force_goal is not None else self.current_goal

        if target_goal is None:
            logger.warning("âš ï¸ [Coordinator] æ²¡æœ‰ç›®æ ‡ï¼Œè·³è¿‡ä½å±‚æ‰§è¡Œ")
            return self.env.get_state(), 0.0, False, False, {
                'error': 'no_goal',
                'high_action': getattr(self, 'last_high_action', None)
            }

        # è·å–ä½å±‚çŠ¶æ€å’Œmask
        low_obs = self.env.get_state()
        low_mask = self.env.get_low_level_action_mask()

        # ä½å±‚é€‰æ‹©åŠ¨ä½œ
        try:
            _, low_action, low_info = self.low_agent.select_action(
                low_obs,
                action_mask=low_mask
            )
            logger.debug(f"ğŸš¶ [ä½å±‚] æ‰§è¡ŒåŠ¨ä½œ: {low_action}, ç›®æ ‡: {target_goal}")

        except Exception as e:
            logger.error(f"âŒ [Coordinator] ä½å±‚åŠ¨ä½œé€‰æ‹©å¤±è´¥: {e}")
            return low_obs, -5.0, False, True, {
                'error': 'low_action_selection_failed',
                'high_action': getattr(self, 'last_high_action', None)
            }

        # æ‰§è¡Œä½å±‚åŠ¨ä½œ
        try:
            next_obs, reward, done, truncated, info = self.env.step_low_level(low_action)

            # ğŸ”¥ å¢åŠ ä½å±‚æ­¥æ•°
            self.low_step_count += 1
            logger.debug(f"ğŸ“Š [Coordinator] ä½å±‚æ­¥æ•°: {self.low_step_count}/{self.max_low_steps}")

        except Exception as e:
            logger.error(f"âŒ [Coordinator] ä½å±‚æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return low_obs, -10.0, False, True, {
                'error': 'low_level_execution_failed',
                'high_action': getattr(self, 'last_high_action', None)
            }

        # ========================================
        # Phase 3: çŠ¶æ€æ›´æ–°
        # ========================================
        # å¦‚æœå­ç›®æ ‡å®Œæˆï¼ˆtruncated=Trueï¼‰ï¼Œæ¸…ç©ºç›®æ ‡
        if truncated:
            logger.info("âœ… [Coordinator] å­ç›®æ ‡å®Œæˆï¼Œæ¸…ç©ºç›®æ ‡")
            self.current_goal = None
            self.stats['subgoals_completed'] += 1
            self.low_step_count = 0  # ğŸ”¥ é‡ç½®ä½å±‚æ­¥æ•°

        # å­˜å‚¨æœ€è¿‘çš„ transition
        self.last_transition = (low_obs, low_action, reward, next_obs, done)

        # æ›´æ–°ç»Ÿè®¡
        self.stats['total_low_actions'] += 1

        # ğŸ”¥ åœ¨ info ä¸­åŠ å…¥ high_action
        if not isinstance(info, dict):
            info = {}
        info['high_action'] = getattr(self, 'last_high_action', None)

        return next_obs, reward, done, truncated, info


    def _should_replan(self):
        """
        ğŸ”§ åˆ¤æ–­æ˜¯å¦éœ€è¦é‡æ–°è§„åˆ’

        é‡æ–°è§„åˆ’çš„æ¡ä»¶:
        1. ä½å±‚è¶…æ—¶ï¼ˆæ‰§è¡Œå¤ªå¤šæ­¥ä»æœªå®Œæˆï¼‰
        2. å½“å‰å­ç›®æ ‡å·²å®Œæˆï¼ˆVNFå·²éƒ¨ç½²æˆ–ç›®çš„åœ°å·²è¿æ¥ï¼‰
        3. å‘ç”Ÿé”™è¯¯
        """
        # 1. ä½å±‚è¶…æ—¶
        if self.low_step_count >= self.max_low_steps:
            logger.warning(f"âš ï¸ [Coordinator] ä½å±‚è¶…æ—¶({self.low_step_count}æ­¥)ï¼Œéœ€è¦é‡æ–°è§„åˆ’")
            return True

        # 2. æ£€æŸ¥å­ç›®æ ‡æ˜¯å¦å®Œæˆ
        if self.current_goal is not None:
            # æ£€æŸ¥VNFéƒ¨ç½²è¿›åº¦
            if hasattr(self.env, '_get_total_vnf_progress'):
                current_progress = self.env._get_total_vnf_progress()
                if current_progress > self.last_vnf_progress:
                    logger.info(f"âœ… [Coordinator] VNFè¿›åº¦æå‡({self.last_vnf_progress}->{current_progress})ï¼Œéœ€è¦æ–°ç›®æ ‡")
                    self.last_vnf_progress = current_progress
                    return True

            # æ£€æŸ¥ç›®çš„åœ°è¿æ¥è¿›åº¦
            if hasattr(self.env, 'current_tree'):
                connected = len(self.env.current_tree.get('connected_dests', set()))
                if connected > self.last_connected_count:
                    logger.info(f"âœ… [Coordinator] ç›®çš„åœ°è¿æ¥æå‡({self.last_connected_count}->{connected})ï¼Œéœ€è¦æ–°ç›®æ ‡")
                    self.last_connected_count = connected
                    return True

        # 3. é»˜è®¤ä¸é‡æ–°è§„åˆ’ï¼ˆè®©ä½å±‚ç»§ç»­æ‰§è¡Œå½“å‰ç›®æ ‡ï¼‰
        return False

    def _set_goal(self, high_action):
        """è®¾ç½®å½“å‰é«˜å±‚ç›®æ ‡"""
        self.current_goal = high_action
        self._low_step_count = 0
        self.stats['total_high_decisions'] += 1
        logger.debug(f"ğŸ“Œ [Coordinator] è®¾ç½®ç›®æ ‡: {high_action}")